
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Custom operations for GPUs with C++ and CUDA &#8212; JAX  documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
    <link rel="stylesheet" type="text/css" href="_static/style.css?v=02ed413a" />
    <link rel="stylesheet" href="_static/style.css" type="text/css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Custom_Operation_for_GPUs';</script>
    <link rel="icon" href="_static/favicon.png"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Generalized Convolutions in JAX" href="notebooks/convolutions.html" />
    <link rel="prev" title="Writing custom Jaxpr interpreters in JAX" href="notebooks/Writing_custom_interpreters_in_Jax.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/jax_logo_250px.png" class="logo__image only-light" alt="JAX  documentation - Home"/>
    <script>document.write(`<img src="_static/jax_logo_250px.png" class="logo__image only-dark" alt="JAX  documentation - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installing JAX</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/quickstart.html">JAX Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/thinking_in_jax.html">How to Think in JAX</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/Common_Gotchas_in_JAX.html">ðŸ”ª JAX - The Sharp Bits ðŸ”ª</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">JAX Frequently Asked Questions (FAQ)</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="jax-101/index.html">Tutorial: JAX 101</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="jax-101/01-jax-basics.html">JAX As Accelerated NumPy</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax-101/02-jitting.html">Just In Time Compilation with JAX</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax-101/03-vectorization.html">Automatic Vectorization in JAX</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax-101/04-advanced-autodiff.html">Advanced Automatic Differentiation in JAX</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax-101/05-random-numbers.html">Pseudo Random Numbers in JAX</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax-101/05.1-pytrees.html">Working with Pytrees</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax-101/06-parallelism.html">Parallel Evaluation in JAX</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax-101/07-state.html">Stateful Computations in JAX</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Further Resources</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="user_guides.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="profiling.html">Profiling JAX programs</a></li>
<li class="toctree-l2"><a class="reference internal" href="device_memory_profiling.html">Device Memory Profiling</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="debugging/index.html">Runtime value debugging in JAX</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="debugging/print_breakpoint.html"><code class="docutils literal notranslate"><span class="pre">jax.debug.print</span></code> and <code class="docutils literal notranslate"><span class="pre">jax.debug.breakpoint</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="debugging/checkify_guide.html">The <code class="docutils literal notranslate"><span class="pre">checkify</span></code> transformation</a></li>
<li class="toctree-l3"><a class="reference internal" href="debugging/flags.html">JAX debugging flags</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="gpu_performance_tips.html">GPU performance tips</a></li>
<li class="toctree-l2"><a class="reference internal" href="persistent_compilation_cache.html">Persistent Compilation Cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="jaxpr.html">Understanding Jaxprs</a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/external_callbacks.html">External Callbacks in JAX</a></li>
<li class="toctree-l2"><a class="reference internal" href="type_promotion.html">Type promotion semantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytrees.html">Pytrees</a></li>
<li class="toctree-l2"><a class="reference internal" href="aot.html">Ahead-of-time lowering and compilation</a></li>
<li class="toctree-l2"><a class="reference internal" href="errors.html">JAX Errors</a></li>
<li class="toctree-l2"><a class="reference internal" href="transfer_guard.html">Transfer guard</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pallas/index.html">Pallas: a JAX kernel language</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="pallas/design.html">Pallas Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="pallas/quickstart.html">Pallas Quickstart</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pallas/tpu/index.html">Pallas TPU</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="pallas/tpu/details.html">Writing TPU kernels with Pallas</a></li>
<li class="toctree-l4"><a class="reference internal" href="pallas/tpu/pipelining.html">Pipelining and <code class="docutils literal notranslate"><span class="pre">BlockSpec</span></code>s</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="advanced_guide.html">Advanced Tutorials</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="notebooks/neural_network_with_tfds_data.html">Training a Simple Neural Network, with tensorflow/datasets Data Loading</a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/Neural_Network_and_Data_Loading.html">Training a Simple Neural Network, with PyTorch Data Loading</a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/vmapped_log_probs.html">Autobatching for Bayesian Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="multi_process.html">Using JAX in multi-host and multi-process environments</a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/Distributed_arrays_and_automatic_parallelization.html">Distributed arrays and automatic parallelization</a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/shard_map.html">SPMD multi-device parallelism with <code class="docutils literal notranslate"><span class="pre">shard_map</span></code></a></li>



<li class="toctree-l2"><a class="reference internal" href="notebooks/xmap_tutorial.html">Named axes and easy-to-revise parallelism with <code class="docutils literal notranslate"><span class="pre">xmap</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/autodiff_cookbook.html">The Autodiff Cookbook</a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/Custom_derivative_rules_for_Python_code.html">Custom derivative rules for JAX-transformable Python functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/autodiff_remat.html">Control autodiffâ€™s saved values with <code class="docutils literal notranslate"><span class="pre">jax.checkpoint</span></code> (aka <code class="docutils literal notranslate"><span class="pre">jax.remat</span></code>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/How_JAX_primitives_work.html">How JAX primitives work</a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/Writing_custom_interpreters_in_Jax.html">Writing custom Jaxpr interpreters in JAX</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Custom operations for GPUs with C++ and CUDA</a></li>

<li class="toctree-l2"><a class="reference internal" href="notebooks/convolutions.html">Generalized Convolutions in JAX</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="contributor_guide.html">Developer Documentation</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="contributing.html">Contributing to JAX</a></li>
<li class="toctree-l2"><a class="reference internal" href="developer.html">Building from source</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax_internal_api.html">Internal APIs</a></li>
<li class="toctree-l2"><a class="reference internal" href="autodidax.html">Autodidax: JAX core from scratch</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="jep/index.html">JAX Enhancement Proposals (JEPs)</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="jep/263-prng.html">263: JAX PRNG Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/2026-custom-derivatives.html">2026: Custom JVP/VJP rules for JAX-transformable functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/4008-custom-vjp-update.html">4008: Custom VJP and `nondiff_argnums` update</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/4410-omnistaging.html">4410: Omnistaging</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/9263-typed-keys.html">9263: Typed keys &amp; pluggable RNGs</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/9407-type-promotion.html">9407: Design of Type Promotion Semantics for JAX</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/9419-jax-versioning.html">9419: Jax and Jaxlib versioning</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/10657-sequencing-effects.html">10657: Sequencing side-effects in JAX</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/11830-new-remat-checkpoint.html">11830: `jax.remat` / `jax.checkpoint` new implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/12049-type-annotations.html">12049: Type Annotation Roadmap for JAX</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/14273-shard-map.html">14273: `shard_map` (`shmap`) for simple per-device code</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/15856-jex.html">15856: `jax.extend`, an extensions module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/17111-shmap-transpose.html">17111: Efficient transposition of `shard_map` (and other maps)</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/18137-numpy-scipy-scope.html">18137: Scope of JAX NumPy &amp; SciPy Wrappers</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="investigating_a_regression.html">Investigating a regression</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="building_on_jax.html">Building on JAX</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="notes.html">Notes</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="api_compatibility.html">API compatibility</a></li>
<li class="toctree-l2"><a class="reference internal" href="deprecation.html">Python and NumPy version support policy</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax_array_migration.html">jax.Array migration</a></li>
<li class="toctree-l2"><a class="reference internal" href="async_dispatch.html">Asynchronous dispatch</a></li>
<li class="toctree-l2"><a class="reference internal" href="concurrency.html">Concurrency</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpu_memory_allocation.html">GPU memory allocation</a></li>
<li class="toctree-l2"><a class="reference internal" href="rank_promotion_warning.html">Rank promotion warning</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="jax.html">Public API: jax package</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="jax.numpy.html"><code class="docutils literal notranslate"><span class="pre">jax.numpy</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.fft.html">jax.numpy.fft.fft</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.fft2.html">jax.numpy.fft.fft2</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.fftfreq.html">jax.numpy.fft.fftfreq</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.fftn.html">jax.numpy.fft.fftn</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.fftshift.html">jax.numpy.fft.fftshift</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.hfft.html">jax.numpy.fft.hfft</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.ifft.html">jax.numpy.fft.ifft</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.ifft2.html">jax.numpy.fft.ifft2</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.ifftn.html">jax.numpy.fft.ifftn</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.ifftshift.html">jax.numpy.fft.ifftshift</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.ihfft.html">jax.numpy.fft.ihfft</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.irfft.html">jax.numpy.fft.irfft</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.irfft2.html">jax.numpy.fft.irfft2</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.irfftn.html">jax.numpy.fft.irfftn</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.rfft.html">jax.numpy.fft.rfft</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.rfft2.html">jax.numpy.fft.rfft2</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.rfftfreq.html">jax.numpy.fft.rfftfreq</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.rfftn.html">jax.numpy.fft.rfftn</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="jax.scipy.html"><code class="docutils literal notranslate"><span class="pre">jax.scipy</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.scipy.stats.bernoulli.logpmf.html">jax.scipy.stats.bernoulli.logpmf</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.scipy.stats.bernoulli.pmf.html">jax.scipy.stats.bernoulli.pmf</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.scipy.stats.bernoulli.cdf.html">jax.scipy.stats.bernoulli.cdf</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.scipy.stats.bernoulli.ppf.html">jax.scipy.stats.bernoulli.ppf</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="jax.lax.html"><code class="docutils literal notranslate"><span class="pre">jax.lax</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.random.html"><code class="docutils literal notranslate"><span class="pre">jax.random</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.sharding.html"><code class="docutils literal notranslate"><span class="pre">jax.sharding</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.debug.html"><code class="docutils literal notranslate"><span class="pre">jax.debug</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.dlpack.html"><code class="docutils literal notranslate"><span class="pre">jax.dlpack</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.distributed.html"><code class="docutils literal notranslate"><span class="pre">jax.distributed</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.dtypes.html"><code class="docutils literal notranslate"><span class="pre">jax.dtypes</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.flatten_util.html"><code class="docutils literal notranslate"><span class="pre">jax.flatten_util</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.image.html"><code class="docutils literal notranslate"><span class="pre">jax.image</span></code> module</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="jax.nn.html"><code class="docutils literal notranslate"><span class="pre">jax.nn</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="jax.nn.initializers.html"><code class="docutils literal notranslate"><span class="pre">jax.nn.initializers</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="jax.ops.html"><code class="docutils literal notranslate"><span class="pre">jax.ops</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.profiler.html"><code class="docutils literal notranslate"><span class="pre">jax.profiler</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.stages.html"><code class="docutils literal notranslate"><span class="pre">jax.stages</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.tree.html"><code class="docutils literal notranslate"><span class="pre">jax.tree</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.tree_util.html"><code class="docutils literal notranslate"><span class="pre">jax.tree_util</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.typing.html"><code class="docutils literal notranslate"><span class="pre">jax.typing</span></code> module</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="jax.extend.html"><code class="docutils literal notranslate"><span class="pre">jax.extend</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="jax.extend.linear_util.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.linear_util</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.extend.mlir.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.mlir</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.extend.random.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.random</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="jax.example_libraries.html"><code class="docutils literal notranslate"><span class="pre">jax.example_libraries</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="jax.example_libraries.optimizers.html"><code class="docutils literal notranslate"><span class="pre">jax.example_libraries.optimizers</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.example_libraries.stax.html"><code class="docutils literal notranslate"><span class="pre">jax.example_libraries.stax</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="jax.experimental.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="jax.experimental.array_api.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.array_api</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.experimental.checkify.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.checkify</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.experimental.host_callback.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.host_callback</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.experimental.maps.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.maps</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.experimental.pjit.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.pjit</span></code> module</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="jax.experimental.sparse.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.sparse</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.BCOO.html">jax.experimental.sparse.BCOO</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_broadcast_in_dim.html">jax.experimental.sparse.bcoo_broadcast_in_dim</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_concatenate.html">jax.experimental.sparse.bcoo_concatenate</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_dot_general.html">jax.experimental.sparse.bcoo_dot_general</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_dot_general_sampled.html">jax.experimental.sparse.bcoo_dot_general_sampled</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_dynamic_slice.html">jax.experimental.sparse.bcoo_dynamic_slice</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_extract.html">jax.experimental.sparse.bcoo_extract</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_fromdense.html">jax.experimental.sparse.bcoo_fromdense</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_gather.html">jax.experimental.sparse.bcoo_gather</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_multiply_dense.html">jax.experimental.sparse.bcoo_multiply_dense</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_multiply_sparse.html">jax.experimental.sparse.bcoo_multiply_sparse</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_update_layout.html">jax.experimental.sparse.bcoo_update_layout</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_reduce_sum.html">jax.experimental.sparse.bcoo_reduce_sum</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_reshape.html">jax.experimental.sparse.bcoo_reshape</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_slice.html">jax.experimental.sparse.bcoo_slice</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_sort_indices.html">jax.experimental.sparse.bcoo_sort_indices</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_squeeze.html">jax.experimental.sparse.bcoo_squeeze</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_sum_duplicates.html">jax.experimental.sparse.bcoo_sum_duplicates</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_todense.html">jax.experimental.sparse.bcoo_todense</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_transpose.html">jax.experimental.sparse.bcoo_transpose</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="jax.experimental.jet.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.jet</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.experimental.custom_partitioning.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.custom_partitioning</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.experimental.multihost_utils.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.multihost_utils</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.experimental.compilation_cache.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.compilation_cache</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.experimental.key_reuse.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.key_reuse</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.experimental.mesh_utils.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.mesh_utils</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="jax.lib.html"><code class="docutils literal notranslate"><span class="pre">jax.lib</span></code> module</a></li>
</ul>
</li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="changelog.html">Change log</a></li>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">JAX Glossary of Terms</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/google/jax" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/Custom_Operation_for_GPUs.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Custom operations for GPUs with C++ and CUDA</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Custom operations for GPUs with C++ and CUDA</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rms-normalization">RMS normalization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#high-level-steps">High-level steps</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#c-code">C code</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#build-gpu-ops-extension-module">Build <code class="docutils literal notranslate"><span class="pre">gpu_ops</span></code> extension module</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#add-rms-normalization-to-jax-as-custom-call">Add RMS normalization to JAX as custom call</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-primitives">Create primitives</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lowering-to-mlir-custom-call">Lowering to MLIR custom call</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#let-s-test-it">Letâ€™s test it</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#test-forward-function">Test forward function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#abstract-evaluation">Abstract evaluation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#let-s-test-it-again">Letâ€™s test it again</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#test-the-forward-function">Test the forward function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#test-the-backward-function">Test the backward function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#differentiation-rule">Differentiation rule</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#let-s-test-it-on-multiple-devices">Letâ€™s test it on multiple devices</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Test the forward function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shard-the-forward-function-with-custom-partitioning">Shard the forward function with custom_partitioning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shard-the-backward-function-with-custom-partitioning">Shard the backward function with custom_partitioning</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#check-for-correctness">Check for correctness</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#let-s-put-it-together">Letâ€™s put it together</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpu-ops-code-listing"><code class="docutils literal notranslate"><span class="pre">gpu_ops</span></code> code listing</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="custom-operations-for-gpus-with-c-and-cuda">
<h1>Custom operations for GPUs with C++ and CUDA<a class="headerlink" href="#custom-operations-for-gpus-with-c-and-cuda" title="Link to this heading">#</a></h1>
<p>JAX ships with a large number of built-in operations, but users occasionally run into a situation where they need a new operation that is not supported by JAX.</p>
<p>To accommodate such scenarios, JAX allows users to define custom operations and this tutorial is to explain how we can define one for GPUs and use it in single-GPU and multi-GPU environments.</p>
<p>This tutorial contains information from <a class="reference external" href="https://github.com/dfm/extending-jax">Extending JAX with custom C++ and CUDA code</a> and
supposes that you are familiar with <a class="reference external" href="https://jax.readthedocs.io/en/latest/notebooks/How_JAX_primitives_work.html">JAX primitive</a>.</p>
<section id="rms-normalization">
<h2>RMS normalization<a class="headerlink" href="#rms-normalization" title="Link to this heading">#</a></h2>
<p>For this tutorial, we are going to add the RMS normalization as a custom operation in JAX.
Note that the RMS normalization can be expressed with <a class="reference external" href="https://jax.readthedocs.io/en/latest/jax.numpy.html"><code class="docutils literal notranslate"><span class="pre">jax.numpy</span></code></a> directly. However, we are using it as an example to show the process of creating a custom operation for GPUs.
The CUDA code in <code class="docutils literal notranslate"><span class="pre">gpu_ops/rms_norm_kernels.cu</span></code> for this operation has been borrowed from <a class="reference external" href="https://github.com/NVIDIA/apex/blob/master/csrc/layer_norm_cuda_kernel.cu">Apex</a> and adapted to eliminate any dependency on PyTorch.</p>
</section>
<section id="high-level-steps">
<h2>High-level steps<a class="headerlink" href="#high-level-steps" title="Link to this heading">#</a></h2>
<p>This tutorial shows how to write both a custom operation and its gradient.</p>
<p>In C:
You need to follow these steps in C for each new JAX primitive:</p>
<ul class="simple">
<li><p>Have CUDA kernel(s).</p></li>
<li><p>Create a C function that dispatches the CUDA kernel that will be called by XLA.</p></li>
<li><p>Create a descriptor to convey information needed for the computation.</p>
<ul>
<li><p>The types, the shapes and other attributes.</p></li>
</ul>
</li>
<li><p>Bind C functions to Python</p>
<ul>
<li><p>To create the descriptor and to call the primitive during execution.</p></li>
</ul>
</li>
</ul>
<p>In Python:
You need to follow these steps in Python:</p>
<ul class="simple">
<li><p>Define a new JAX primitive (instruction/operation)</p></li>
<li><p>Write Python functions to build the graph nodes with the primitive.</p></li>
<li><p>Define its abstract evaluation.</p></li>
<li><p>Define its lowering to MLIR.</p></li>
<li><p>[Optional] Define the gradient.</p></li>
<li><p>[Optional] Use <a class="reference external" href="https://jax.readthedocs.io/en/latest/jax.experimental.custom_partitioning.html">custom_partitioning</a> or <a class="reference external" href="https://jax.readthedocs.io/en/latest/jep/14273-shard-map.html">shard_map</a> functions for fast multi-GPU.</p></li>
</ul>
</section>
<section id="c-code">
<h2>C code<a class="headerlink" href="#c-code" title="Link to this heading">#</a></h2>
<p>See <a class="reference internal" href="#gpu-ops-code-listing"><code class="docutils literal notranslate"><span class="pre">gpu_ops</span></code> code listing</a> for a complete code listing of C++ and CUDA files.
<code class="docutils literal notranslate"><span class="pre">gpu_ops/rms_norm_kernels.cu</span></code> defines the following functions, which are declared with the XLA custom function signature.
These functions are responsible for launching RMS normalization kernels with the given <code class="docutils literal notranslate"><span class="pre">buffers</span></code> on the specified <code class="docutils literal notranslate"><span class="pre">stream</span></code>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">namespace</span><span class="w"> </span><span class="nn">gpu_ops</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span>
<span class="kt">void</span><span class="w"> </span><span class="nf">rms_forward_affine_mixed_dtypes</span><span class="p">(</span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">stream</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">**</span><span class="n">buffers</span><span class="p">,</span>
<span class="w">                                     </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">opaque</span><span class="p">,</span>
<span class="w">                                     </span><span class="n">std</span><span class="o">::</span><span class="kt">size_t</span><span class="w"> </span><span class="n">opaque_len</span><span class="p">);</span>

<span class="kt">void</span><span class="w"> </span><span class="nf">rms_backward_affine</span><span class="p">(</span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">stream</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">**</span><span class="n">buffers</span><span class="p">,</span>
<span class="w">                         </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">opaque</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="kt">size_t</span><span class="w"> </span><span class="n">opaque_len</span><span class="p">);</span>

<span class="p">}</span><span class="w"> </span><span class="c1">// namespace gpu_ops</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">stream</span></code> is the CUDA stream to be used to execute any kernel on the GPU.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">buffers</span></code> has all pointers to input buffers followed by all pointers to output buffers.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">opaque</span></code> is a buffer for any extra information that is being passed to the custom functions and <code class="docutils literal notranslate"><span class="pre">opaque_len</span></code> is the length of <code class="docutils literal notranslate"><span class="pre">opaque</span></code>.</p></li>
</ul>
<p>For this tutorial, an <code class="docutils literal notranslate"><span class="pre">RMSNormDescriptor</span></code> object will be passed to these functions as <code class="docutils literal notranslate"><span class="pre">opaque</span></code>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">namespace</span><span class="w"> </span><span class="nn">gpu_ops</span><span class="w"> </span><span class="p">{</span>

<span class="k">enum</span><span class="w"> </span><span class="nc">ElementType</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">BF16</span><span class="p">,</span><span class="w"> </span><span class="n">F16</span><span class="p">,</span><span class="w"> </span><span class="n">F32</span><span class="p">,</span><span class="w"> </span><span class="n">F64</span><span class="w"> </span><span class="p">};</span>

<span class="k">struct</span><span class="w"> </span><span class="nc">RMSNormDescriptor</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">n1</span><span class="p">;</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">n2</span><span class="p">;</span>
<span class="w">  </span><span class="kt">double</span><span class="w"> </span><span class="n">eps</span><span class="p">;</span>
<span class="w">  </span><span class="n">ElementType</span><span class="w"> </span><span class="n">x_type</span><span class="p">;</span>
<span class="w">  </span><span class="n">ElementType</span><span class="w"> </span><span class="n">w_type</span><span class="p">;</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">part_grad_size</span><span class="p">;</span>
<span class="p">};</span>

<span class="p">}</span><span class="w"> </span><span class="c1">// namespace gpu_ops</span>
</pre></div>
</div>
<p>Now, we need to expose these functions as well as <code class="docutils literal notranslate"><span class="pre">ElementType</span></code> and <code class="docutils literal notranslate"><span class="pre">RMSNormDescriptor</span></code> as a Python module, <code class="docutils literal notranslate"><span class="pre">gpu_ops</span></code>, through <code class="docutils literal notranslate"><span class="pre">pybind11</span></code>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">pybind11</span><span class="o">::</span><span class="n">dict</span><span class="w"> </span><span class="nf">RMSNormRegistrations</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">pybind11</span><span class="o">::</span><span class="n">dict</span><span class="w"> </span><span class="n">dict</span><span class="p">;</span>
<span class="w">  </span><span class="n">dict</span><span class="p">[</span><span class="s">&quot;rms_forward_affine_mixed_dtype&quot;</span><span class="p">]</span><span class="w"> </span><span class="o">=</span>
<span class="w">      </span><span class="n">gpu_ops</span><span class="o">::</span><span class="n">EncapsulateFunction</span><span class="p">(</span><span class="n">gpu_ops</span><span class="o">::</span><span class="n">rms_forward_affine_mixed_dtypes</span><span class="p">);</span>
<span class="w">  </span><span class="n">dict</span><span class="p">[</span><span class="s">&quot;rms_backward_affine&quot;</span><span class="p">]</span><span class="w"> </span><span class="o">=</span>
<span class="w">      </span><span class="n">gpu_ops</span><span class="o">::</span><span class="n">EncapsulateFunction</span><span class="p">(</span><span class="n">gpu_ops</span><span class="o">::</span><span class="n">rms_backward_affine</span><span class="p">);</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">dict</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">PYBIND11_MODULE</span><span class="p">(</span><span class="n">gpu_ops</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">&quot;get_rms_norm_registrations&quot;</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">RMSNormRegistrations</span><span class="p">);</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">&quot;create_rms_norm_descriptor&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="p">[](</span><span class="kt">int</span><span class="w"> </span><span class="n">n1</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">n2</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="n">eps</span><span class="p">,</span><span class="w"> </span><span class="n">gpu_ops</span><span class="o">::</span><span class="n">ElementType</span><span class="w"> </span><span class="n">x_type</span><span class="p">,</span>
<span class="w">           </span><span class="n">gpu_ops</span><span class="o">::</span><span class="n">ElementType</span><span class="w"> </span><span class="n">w_type</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">part_grad_size</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">          </span><span class="k">return</span><span class="w"> </span><span class="n">gpu_ops</span><span class="o">::</span><span class="n">PackDescriptor</span><span class="p">(</span><span class="n">gpu_ops</span><span class="o">::</span><span class="n">RMSNormDescriptor</span><span class="p">{</span>
<span class="w">              </span><span class="n">n1</span><span class="p">,</span><span class="w"> </span><span class="n">n2</span><span class="p">,</span><span class="w"> </span><span class="n">eps</span><span class="p">,</span><span class="w"> </span><span class="n">x_type</span><span class="p">,</span><span class="w"> </span><span class="n">w_type</span><span class="p">,</span><span class="w"> </span><span class="n">part_grad_size</span><span class="p">});</span>
<span class="w">        </span><span class="p">});</span>

<span class="w">  </span><span class="n">pybind11</span><span class="o">::</span><span class="n">enum_</span><span class="o">&lt;</span><span class="n">gpu_ops</span><span class="o">::</span><span class="n">ElementType</span><span class="o">&gt;</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;ElementType&quot;</span><span class="p">)</span>
<span class="w">      </span><span class="p">.</span><span class="n">value</span><span class="p">(</span><span class="s">&quot;BF16&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">gpu_ops</span><span class="o">::</span><span class="n">ElementType</span><span class="o">::</span><span class="n">BF16</span><span class="p">)</span>
<span class="w">      </span><span class="p">.</span><span class="n">value</span><span class="p">(</span><span class="s">&quot;F16&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">gpu_ops</span><span class="o">::</span><span class="n">ElementType</span><span class="o">::</span><span class="n">F16</span><span class="p">)</span>
<span class="w">      </span><span class="p">.</span><span class="n">value</span><span class="p">(</span><span class="s">&quot;F32&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">gpu_ops</span><span class="o">::</span><span class="n">ElementType</span><span class="o">::</span><span class="n">F32</span><span class="p">)</span>
<span class="w">      </span><span class="p">.</span><span class="n">value</span><span class="p">(</span><span class="s">&quot;F64&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">gpu_ops</span><span class="o">::</span><span class="n">ElementType</span><span class="o">::</span><span class="n">F64</span><span class="p">);</span>

<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="build-gpu-ops-extension-module">
<h2>Build <code class="docutils literal notranslate"><span class="pre">gpu_ops</span></code> extension module<a class="headerlink" href="#build-gpu-ops-extension-module" title="Link to this heading">#</a></h2>
<p>We build the <code class="docutils literal notranslate"><span class="pre">gpu_ops</span></code> Python extension module with the aforementioned code.
(See <a class="reference internal" href="#gpu-ops-code-listing"><code class="docutils literal notranslate"><span class="pre">gpu_ops</span></code> code listing</a> for a complete code listing of C++ and CUDA files.)</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span><span class="nv">pybind11</span><span class="o">==</span><span class="m">2</span>.10.1
mkdir<span class="w"> </span>-p<span class="w"> </span>build
<span class="nv">pybind_include_path</span><span class="o">=</span><span class="k">$(</span>python<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;import pybind11; print(pybind11.get_include())&quot;</span><span class="k">)</span>
<span class="nv">python_executable</span><span class="o">=</span><span class="k">$(</span>python<span class="w"> </span>-c<span class="w"> </span><span class="s1">&#39;import sys; print(sys.executable)&#39;</span><span class="k">)</span>


nvcc<span class="w"> </span>--threads<span class="w"> </span><span class="m">4</span><span class="w"> </span>-Xcompiler<span class="w"> </span>-Wall<span class="w"> </span>-ldl<span class="w"> </span>--expt-relaxed-constexpr<span class="w"> </span>-O3<span class="w"> </span>-DNDEBUG<span class="w"> </span>-Xcompiler<span class="w"> </span>-O3<span class="w"> </span>--generate-code<span class="o">=</span><span class="nv">arch</span><span class="o">=</span>compute_70,code<span class="o">=[</span>compute_70,sm_70<span class="o">]</span><span class="w"> </span>--generate-code<span class="o">=</span><span class="nv">arch</span><span class="o">=</span>compute_75,code<span class="o">=[</span>compute_75,sm_75<span class="o">]</span><span class="w"> </span>--generate-code<span class="o">=</span><span class="nv">arch</span><span class="o">=</span>compute_80,code<span class="o">=[</span>compute_80,sm_80<span class="o">]</span><span class="w"> </span>--generate-code<span class="o">=</span><span class="nv">arch</span><span class="o">=</span>compute_86,code<span class="o">=[</span>compute_86,sm_86<span class="o">]</span><span class="w"> </span>-Xcompiler<span class="o">=</span>-fPIC<span class="w"> </span>-Xcompiler<span class="o">=</span>-fvisibility<span class="o">=</span>hidden<span class="w"> </span>-x<span class="w"> </span>cu<span class="w"> </span>-c<span class="w"> </span>gpu_ops/rms_norm_kernels.cu<span class="w"> </span>-o<span class="w"> </span>build/rms_norm_kernels.cu.o
c++<span class="w"> </span>-I/usr/local/cuda/include<span class="w"> </span>-I<span class="nv">$pybind_include_path</span><span class="w"> </span><span class="k">$(</span><span class="si">${</span><span class="nv">python_executable</span><span class="si">}</span>-config<span class="w"> </span>--cflags<span class="k">)</span><span class="w"> </span>-O3<span class="w"> </span>-DNDEBUG<span class="w"> </span>-O3<span class="w"> </span>-fPIC<span class="w"> </span>-fvisibility<span class="o">=</span>hidden<span class="w"> </span>-flto<span class="w"> </span>-fno-fat-lto-objects<span class="w"> </span>-o<span class="w"> </span>build/gpu_ops.cpp.o<span class="w"> </span>-c<span class="w"> </span>gpu_ops/gpu_ops.cpp
c++<span class="w"> </span>-fPIC<span class="w"> </span>-O3<span class="w"> </span>-DNDEBUG<span class="w"> </span>-O3<span class="w"> </span>-flto<span class="w"> </span>-shared<span class="w">  </span>-o<span class="w"> </span>build/gpu_ops<span class="k">$(</span><span class="si">${</span><span class="nv">python_executable</span><span class="si">}</span>-config<span class="w"> </span>--extension-suffix<span class="k">)</span><span class="w"> </span>build/gpu_ops.cpp.o<span class="w"> </span>build/rms_norm_kernels.cu.o<span class="w"> </span>-L/usr/local/cuda/lib64<span class="w">  </span>-lcudadevrt<span class="w"> </span>-lcudart_static<span class="w"> </span>-lrt<span class="w"> </span>-lpthread<span class="w"> </span>-ldl
strip<span class="w"> </span>build/gpu_ops<span class="k">$(</span><span class="si">${</span><span class="nv">python_executable</span><span class="si">}</span>-config<span class="w"> </span>--extension-suffix<span class="k">)</span>
</pre></div>
</div>
</section>
<section id="add-rms-normalization-to-jax-as-custom-call">
<h2>Add RMS normalization to JAX as custom call<a class="headerlink" href="#add-rms-normalization-to-jax-as-custom-call" title="Link to this heading">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">gpu_ops</span></code> is just a Python extension module and we need more work to plug it into JAX.</p>
<section id="create-primitives">
<h3>Create primitives<a class="headerlink" href="#create-primitives" title="Link to this heading">#</a></h3>
<p>We first create primitives, <code class="docutils literal notranslate"><span class="pre">_rms_norm_fwd_p</span></code> and <code class="docutils literal notranslate"><span class="pre">_rms_norm_bwd_p</span></code>, which the custom functions can be mapped to.
We set the <code class="docutils literal notranslate"><span class="pre">multiple_results</span></code> attribute to <code class="docutils literal notranslate"><span class="pre">True</span></code> for these operations, which means that the operation produces multiple outputs as a tuple.
When it is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the operation produces a single output without a tuple.
For more details, see <a class="reference external" href="https://jax.readthedocs.io/en/latest/notebooks/How_JAX_primitives_work.html">How JAX primitives work</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">import</span> <span class="nn">jax._src.test_util</span> <span class="k">as</span> <span class="nn">jtu</span>
<span class="kn">from</span> <span class="nn">build</span> <span class="kn">import</span> <span class="n">gpu_ops</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">core</span><span class="p">,</span> <span class="n">dtypes</span>
<span class="kn">from</span> <span class="nn">jax.interpreters</span> <span class="kn">import</span> <span class="n">xla</span>
<span class="kn">from</span> <span class="nn">jax.lib</span> <span class="kn">import</span> <span class="n">xla_client</span>


<span class="c1"># Create _rms_norm_fwd_p for forward operation.</span>
<span class="n">_rms_norm_fwd_p</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">Primitive</span><span class="p">(</span><span class="s2">&quot;rms_norm_fwd&quot;</span><span class="p">)</span>
<span class="n">_rms_norm_fwd_p</span><span class="o">.</span><span class="n">multiple_results</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">_rms_norm_fwd_p</span><span class="o">.</span><span class="n">def_impl</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">xla</span><span class="o">.</span><span class="n">apply_primitive</span><span class="p">,</span> <span class="n">_rms_norm_fwd_p</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">rms_norm_fwd</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">):</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">invvar</span> <span class="o">=</span> <span class="n">_rms_norm_fwd_p</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>


<span class="c1"># Create _rms_norm_bwd_p for backward operation.</span>
<span class="n">_rms_norm_bwd_p</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">Primitive</span><span class="p">(</span><span class="s2">&quot;rms_norm_bwd&quot;</span><span class="p">)</span>
<span class="n">_rms_norm_bwd_p</span><span class="o">.</span><span class="n">multiple_results</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">_rms_norm_bwd_p</span><span class="o">.</span><span class="n">def_impl</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">xla</span><span class="o">.</span><span class="n">apply_primitive</span><span class="p">,</span> <span class="n">_rms_norm_bwd_p</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">rms_norm_bwd</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">invvar</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">eps</span><span class="p">):</span>
    <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_weight</span><span class="p">,</span> <span class="n">part_grad</span> <span class="o">=</span> <span class="n">_rms_norm_bwd_p</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span>
        <span class="n">g</span><span class="p">,</span> <span class="n">invvar</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_weight</span>
</pre></div>
</div>
</section>
<section id="lowering-to-mlir-custom-call">
<h3>Lowering to MLIR custom call<a class="headerlink" href="#lowering-to-mlir-custom-call" title="Link to this heading">#</a></h3>
<p>To map the custom functions to the new primitives, <code class="docutils literal notranslate"><span class="pre">_rms_norm_fwd_p</span></code> and <code class="docutils literal notranslate"><span class="pre">_rms_norm_bwd_p</span></code>, we need to:</p>
<ul class="simple">
<li><p>Register custom functions as custom call targets with <code class="docutils literal notranslate"><span class="pre">xla_client.register_custom_call_target</span></code>, and</p></li>
<li><p>Register lowering functions that lower the primitives to MLIR custom calls with the registered custom call targets.</p></li>
</ul>
<p>The functions <code class="docutils literal notranslate"><span class="pre">_rms_norm_fwd_cuda_lowering</span></code> and <code class="docutils literal notranslate"><span class="pre">_rms_norm_bwd_cuda_lowering</span></code> below lower the primitives to MLIR custom call operations with the custom targets from <code class="docutils literal notranslate"><span class="pre">gpu_ops</span></code>.  These functions are registered with <code class="docutils literal notranslate"><span class="pre">jax.interpreters.mlir.register_lowering</span></code>.</p>
<p>Note that an <code class="docutils literal notranslate"><span class="pre">RMSNormDescriptor</span></code> object is created in the lowering function, and passed to the custom call as <code class="docutils literal notranslate"><span class="pre">opaque</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">reduce</span>

<span class="kn">from</span> <span class="nn">jax.interpreters</span> <span class="kn">import</span> <span class="n">mlir</span>
<span class="kn">from</span> <span class="nn">jax.interpreters.mlir</span> <span class="kn">import</span> <span class="n">ir</span>
<span class="kn">from</span> <span class="nn">jaxlib.hlo_helpers</span> <span class="kn">import</span> <span class="n">custom_call</span>


<span class="c1"># Register functions defined in gpu_ops as custom call target for GPUs</span>
<span class="k">for</span> <span class="n">_name</span><span class="p">,</span> <span class="n">_value</span> <span class="ow">in</span> <span class="n">gpu_ops</span><span class="o">.</span><span class="n">get_rms_norm_registrations</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">xla_client</span><span class="o">.</span><span class="n">register_custom_call_target</span><span class="p">(</span><span class="n">_name</span><span class="p">,</span> <span class="n">_value</span><span class="p">,</span> <span class="n">platform</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">element_type_to_descriptor_type_mapping</span><span class="p">(</span><span class="n">element_type</span><span class="p">):</span>
    <span class="n">_element_type_to_descriptor_type_mapping</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">ir</span><span class="o">.</span><span class="n">BF16Type</span><span class="o">.</span><span class="n">get</span><span class="p">():</span> <span class="n">gpu_ops</span><span class="o">.</span><span class="n">ElementType</span><span class="o">.</span><span class="n">BF16</span><span class="p">,</span>
        <span class="n">ir</span><span class="o">.</span><span class="n">F16Type</span><span class="o">.</span><span class="n">get</span><span class="p">():</span> <span class="n">gpu_ops</span><span class="o">.</span><span class="n">ElementType</span><span class="o">.</span><span class="n">F16</span><span class="p">,</span>
        <span class="n">ir</span><span class="o">.</span><span class="n">F32Type</span><span class="o">.</span><span class="n">get</span><span class="p">():</span> <span class="n">gpu_ops</span><span class="o">.</span><span class="n">ElementType</span><span class="o">.</span><span class="n">F32</span><span class="p">,</span>
        <span class="n">ir</span><span class="o">.</span><span class="n">F64Type</span><span class="o">.</span><span class="n">get</span><span class="p">():</span> <span class="n">gpu_ops</span><span class="o">.</span><span class="n">ElementType</span><span class="o">.</span><span class="n">F64</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">_element_type_to_descriptor_type_mapping</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">element_type</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">default_layouts</span><span class="p">(</span><span class="o">*</span><span class="n">shapes</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">shape</span> <span class="ow">in</span> <span class="n">shapes</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">_rms_norm_fwd_cuda_lowering</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">eps</span><span class="p">):</span>
    <span class="n">x_type</span> <span class="o">=</span> <span class="n">ir</span><span class="o">.</span><span class="n">RankedTensorType</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">type</span><span class="p">)</span>
    <span class="n">x_shape</span> <span class="o">=</span> <span class="n">x_type</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">w_type</span> <span class="o">=</span> <span class="n">ir</span><span class="o">.</span><span class="n">RankedTensorType</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">type</span><span class="p">)</span>
    <span class="n">w_shape</span> <span class="o">=</span> <span class="n">w_type</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">iv_element_type</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">ir</span><span class="o">.</span><span class="n">F32Type</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">x_type</span><span class="o">.</span><span class="n">element_type</span> <span class="ow">in</span> <span class="p">[</span><span class="n">ir</span><span class="o">.</span><span class="n">F16Type</span><span class="o">.</span><span class="n">get</span><span class="p">(),</span> <span class="n">ir</span><span class="o">.</span><span class="n">BF16Type</span><span class="o">.</span><span class="n">get</span><span class="p">()]</span>
        <span class="k">else</span> <span class="n">x_type</span><span class="o">.</span><span class="n">element_type</span>
    <span class="p">)</span>

    <span class="n">n2</span> <span class="o">=</span> <span class="n">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">)</span>
    <span class="n">n1</span> <span class="o">=</span> <span class="n">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">)</span> <span class="o">//</span> <span class="n">n2</span>

    <span class="n">opaque</span> <span class="o">=</span> <span class="n">gpu_ops</span><span class="o">.</span><span class="n">create_rms_norm_descriptor</span><span class="p">(</span>
        <span class="n">n1</span><span class="p">,</span>
        <span class="n">n2</span><span class="p">,</span>
        <span class="n">eps</span><span class="p">,</span>
        <span class="n">element_type_to_descriptor_type_mapping</span><span class="p">(</span><span class="n">x_type</span><span class="o">.</span><span class="n">element_type</span><span class="p">),</span>
        <span class="n">element_type_to_descriptor_type_mapping</span><span class="p">(</span><span class="n">w_type</span><span class="o">.</span><span class="n">element_type</span><span class="p">),</span>
        <span class="mi">0</span><span class="p">,</span>  <span class="c1"># unused</span>
    <span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">custom_call</span><span class="p">(</span>
        <span class="sa">b</span><span class="s2">&quot;rms_forward_affine_mixed_dtype&quot;</span><span class="p">,</span>
        <span class="n">result_types</span><span class="o">=</span><span class="p">[</span>
            <span class="n">ir</span><span class="o">.</span><span class="n">RankedTensorType</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">x_shape</span><span class="p">,</span> <span class="n">w_type</span><span class="o">.</span><span class="n">element_type</span><span class="p">),</span>
            <span class="n">ir</span><span class="o">.</span><span class="n">RankedTensorType</span><span class="o">.</span><span class="n">get</span><span class="p">((</span><span class="n">n1</span><span class="p">,),</span> <span class="n">iv_element_type</span><span class="p">),</span>
        <span class="p">],</span>
        <span class="n">operands</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">],</span>
        <span class="n">backend_config</span><span class="o">=</span><span class="n">opaque</span><span class="p">,</span>
        <span class="n">operand_layouts</span><span class="o">=</span><span class="n">default_layouts</span><span class="p">(</span><span class="n">x_shape</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">),</span>
        <span class="n">result_layouts</span><span class="o">=</span><span class="n">default_layouts</span><span class="p">(</span><span class="n">x_shape</span><span class="p">,</span> <span class="p">(</span><span class="n">n1</span><span class="p">,)),</span>
    <span class="p">)</span><span class="o">.</span><span class="n">results</span>
    <span class="k">return</span> <span class="n">out</span>


<span class="n">mlir</span><span class="o">.</span><span class="n">register_lowering</span><span class="p">(</span>
    <span class="n">_rms_norm_fwd_p</span><span class="p">,</span>
    <span class="n">_rms_norm_fwd_cuda_lowering</span><span class="p">,</span>
    <span class="n">platform</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span>
<span class="p">)</span>


<span class="k">def</span> <span class="nf">_rms_norm_bwd_cuda_lowering</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">,</span> <span class="n">invvar</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">eps</span><span class="p">):</span>
    <span class="n">x_type</span> <span class="o">=</span> <span class="n">ir</span><span class="o">.</span><span class="n">RankedTensorType</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">type</span><span class="p">)</span>
    <span class="n">x_shape</span> <span class="o">=</span> <span class="n">x_type</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">w_type</span> <span class="o">=</span> <span class="n">ir</span><span class="o">.</span><span class="n">RankedTensorType</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">type</span><span class="p">)</span>
    <span class="n">w_shape</span> <span class="o">=</span> <span class="n">w_type</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">iv_type</span> <span class="o">=</span> <span class="n">ir</span><span class="o">.</span><span class="n">RankedTensorType</span><span class="p">(</span><span class="n">invvar</span><span class="o">.</span><span class="n">type</span><span class="p">)</span>

    <span class="n">n2</span> <span class="o">=</span> <span class="n">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">)</span>
    <span class="n">n1</span> <span class="o">=</span> <span class="n">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">)</span> <span class="o">//</span> <span class="n">n2</span>

    <span class="n">part_grad_shape</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">avals_out</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>

    <span class="n">opaque</span> <span class="o">=</span> <span class="n">gpu_ops</span><span class="o">.</span><span class="n">create_rms_norm_descriptor</span><span class="p">(</span>
        <span class="n">n1</span><span class="p">,</span>
        <span class="n">n2</span><span class="p">,</span>
        <span class="n">eps</span><span class="p">,</span>
        <span class="n">element_type_to_descriptor_type_mapping</span><span class="p">(</span><span class="n">x_type</span><span class="o">.</span><span class="n">element_type</span><span class="p">),</span>
        <span class="n">element_type_to_descriptor_type_mapping</span><span class="p">(</span><span class="n">w_type</span><span class="o">.</span><span class="n">element_type</span><span class="p">),</span>
        <span class="n">part_grad_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">custom_call</span><span class="p">(</span>
        <span class="sa">b</span><span class="s2">&quot;rms_backward_affine&quot;</span><span class="p">,</span>
        <span class="n">result_types</span><span class="o">=</span><span class="p">[</span>
            <span class="n">ir</span><span class="o">.</span><span class="n">RankedTensorType</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">x_shape</span><span class="p">,</span> <span class="n">x_type</span><span class="o">.</span><span class="n">element_type</span><span class="p">),</span>
            <span class="n">ir</span><span class="o">.</span><span class="n">RankedTensorType</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">w_shape</span><span class="p">,</span> <span class="n">w_type</span><span class="o">.</span><span class="n">element_type</span><span class="p">),</span>
            <span class="n">ir</span><span class="o">.</span><span class="n">RankedTensorType</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">part_grad_shape</span><span class="p">,</span> <span class="n">iv_type</span><span class="o">.</span><span class="n">element_type</span><span class="p">),</span>
        <span class="p">],</span>
        <span class="n">operands</span><span class="o">=</span><span class="p">[</span><span class="n">grad_output</span><span class="p">,</span> <span class="n">invvar</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">],</span>
        <span class="n">backend_config</span><span class="o">=</span><span class="n">opaque</span><span class="p">,</span>
        <span class="n">operand_layouts</span><span class="o">=</span><span class="n">default_layouts</span><span class="p">(</span><span class="n">x_shape</span><span class="p">,</span> <span class="p">(</span><span class="n">n1</span><span class="p">,),</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">),</span>
        <span class="n">result_layouts</span><span class="o">=</span><span class="n">default_layouts</span><span class="p">(</span><span class="n">x_shape</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">,</span> <span class="n">part_grad_shape</span><span class="p">),</span>
    <span class="p">)</span><span class="o">.</span><span class="n">results</span>
    <span class="k">return</span> <span class="n">out</span>


<span class="n">mlir</span><span class="o">.</span><span class="n">register_lowering</span><span class="p">(</span>
    <span class="n">_rms_norm_bwd_p</span><span class="p">,</span>
    <span class="n">_rms_norm_bwd_cuda_lowering</span><span class="p">,</span>
    <span class="n">platform</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="let-s-test-it">
<h2>Letâ€™s test it<a class="headerlink" href="#let-s-test-it" title="Link to this heading">#</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">per_core_batch_size</span><span class="o">=</span><span class="mi">4</span>
<span class="n">seq_len</span><span class="o">=</span><span class="mi">512</span>
<span class="n">emb_dim</span><span class="o">=</span><span class="mi">512</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span>
    <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
    <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">local_device_count</span><span class="p">()</span> <span class="o">*</span> <span class="n">per_core_batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">),</span>
    <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">norm_shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>
<span class="n">weight</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">norm_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
</pre></div>
</div>
<section id="test-forward-function">
<h3>Test forward function<a class="headerlink" href="#test-forward-function" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">out</span> <span class="o">=</span> <span class="n">rms_norm_fwd</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">---------------------------------------------------------------------------</span>
<span class="ne">NotImplementedError</span>                       <span class="n">Traceback</span> <span class="p">(</span><span class="n">most</span> <span class="n">recent</span> <span class="n">call</span> <span class="n">last</span><span class="p">)</span>
<span class="n">Cell</span> <span class="n">In</span> <span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="n">line</span> <span class="mi">1</span>
<span class="o">----&gt;</span> <span class="mi">1</span> <span class="n">out</span> <span class="o">=</span> <span class="n">rms_norm_fwd</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>

<span class="o">...</span>

<span class="ne">NotImplementedError</span><span class="p">:</span> <span class="n">Abstract</span> <span class="n">evaluation</span> <span class="k">for</span> <span class="s1">&#39;rms_norm_fwd&#39;</span> <span class="ow">not</span> <span class="n">implemented</span>
</pre></div>
</div>
</section>
</section>
<section id="abstract-evaluation">
<h2>Abstract evaluation<a class="headerlink" href="#abstract-evaluation" title="Link to this heading">#</a></h2>
<p>The test above failed with <code class="docutils literal notranslate"><span class="pre">NotImplementedError:</span> <span class="pre">Abstract</span> <span class="pre">evaluation</span> <span class="pre">for</span> <span class="pre">'rms_norm_fwd'</span> <span class="pre">not</span> <span class="pre">implemented</span></code>.  Why did the test fail?  What does it mean?</p>
<p>As part of the execution, JAX performs abstract evaluation.  As JAX has no knowledge about the new primitives, it doesnâ€™t know how to compute the output shapes and output data types, thus canâ€™t evaluate these operations abstractly.</p>
<p>We need to provide a function for abstract evaluation of each primitive.
These abstract evaluation functions compute the shape and the data type of the outputs, but donâ€™t compute actual values for the operations.</p>
<p>These functions are passed to <code class="docutils literal notranslate"><span class="pre">.def_abstract_eval</span></code> method to be registered with the corresponding primitives.</p>
<p>See <a class="reference external" href="https://jax.readthedocs.io/en/latest/notebooks/How_JAX_primitives_work.html#abstract-evaluation-rules">How JAX primitives work</a> for more information on abstract evaluation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">reduce</span>
<span class="kn">from</span> <span class="nn">operator</span> <span class="kn">import</span> <span class="n">mul</span>

<span class="kn">from</span> <span class="nn">jax.core</span> <span class="kn">import</span> <span class="n">ShapedArray</span>


<span class="k">def</span> <span class="nf">_rms_norm_fwd_abstract</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">eps</span><span class="p">):</span>
    <span class="n">w_dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">canonicalize_dtype</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">iv_dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">canonicalize_dtype</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">iv_dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="n">jnp</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">]:</span>
        <span class="n">iv_dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float32</span>
    <span class="n">n2</span> <span class="o">=</span> <span class="n">reduce</span><span class="p">(</span><span class="n">mul</span><span class="p">,</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">n1</span> <span class="o">=</span> <span class="n">reduce</span><span class="p">(</span><span class="n">mul</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">//</span> <span class="n">n2</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">ShapedArray</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">w_dtype</span><span class="p">,</span> <span class="n">named_shape</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">named_shape</span><span class="p">),</span>  <span class="c1"># output</span>
        <span class="n">ShapedArray</span><span class="p">((</span><span class="n">n1</span><span class="p">,),</span> <span class="n">iv_dtype</span><span class="p">,</span> <span class="n">named_shape</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">named_shape</span><span class="p">),</span>  <span class="c1"># invvar</span>
    <span class="p">)</span>


<span class="n">_rms_norm_fwd_p</span><span class="o">.</span><span class="n">def_abstract_eval</span><span class="p">(</span><span class="n">_rms_norm_fwd_abstract</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_rms_norm_bwd_abstract</span><span class="p">(</span><span class="n">grad_output</span><span class="p">,</span> <span class="n">invvar</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">eps</span><span class="p">):</span>
    <span class="n">iv_dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">canonicalize_dtype</span><span class="p">(</span><span class="n">invvar</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">w_dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">canonicalize_dtype</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">x_dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">canonicalize_dtype</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">n2</span> <span class="o">=</span> <span class="n">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">n1</span> <span class="o">=</span> <span class="n">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">//</span> <span class="n">n2</span>
    <span class="n">part_grad_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">n2</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">canonicalize_dtype</span><span class="p">(</span><span class="n">grad_output</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">==</span> <span class="n">w_dtype</span>
    <span class="k">assert</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">assert</span> <span class="n">invvar</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">n1</span><span class="p">,)</span>
    <span class="k">assert</span> <span class="p">(</span>
        <span class="n">iv_dtype</span> <span class="o">==</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float32</span> <span class="k">if</span> <span class="n">x_dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="n">jnp</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">]</span> <span class="k">else</span> <span class="n">x_dtype</span>
    <span class="p">)</span>
    <span class="k">assert</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">named_shape</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">named_shape</span>
    <span class="n">weight_named_shape</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">weight_named_shape</span> <span class="k">if</span> <span class="n">weight</span><span class="o">.</span><span class="n">named_shape</span> <span class="k">else</span> <span class="n">x</span><span class="o">.</span><span class="n">named_shape</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">ShapedArray</span><span class="p">(</span>
            <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">named_shape</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">named_shape</span>
        <span class="p">),</span>  <span class="c1"># grad input</span>
        <span class="n">ShapedArray</span><span class="p">(</span>
            <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">w_dtype</span><span class="p">,</span> <span class="n">named_shape</span><span class="o">=</span><span class="n">weight_named_shape</span>
        <span class="p">),</span>  <span class="c1"># grad weight</span>
        <span class="n">ShapedArray</span><span class="p">(</span>
            <span class="n">part_grad_shape</span><span class="p">,</span> <span class="n">iv_dtype</span><span class="p">,</span> <span class="n">named_shape</span><span class="o">=</span><span class="n">weight_named_shape</span>
        <span class="p">),</span>  <span class="c1"># part grad</span>
    <span class="p">)</span>


<span class="n">_rms_norm_bwd_p</span><span class="o">.</span><span class="n">def_abstract_eval</span><span class="p">(</span><span class="n">_rms_norm_bwd_abstract</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="let-s-test-it-again">
<h2>Letâ€™s test it again<a class="headerlink" href="#let-s-test-it-again" title="Link to this heading">#</a></h2>
<section id="test-the-forward-function">
<h3>Test the forward function<a class="headerlink" href="#test-the-forward-function" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">out</span> <span class="o">=</span> <span class="n">rms_norm_fwd</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="test-the-backward-function">
<h3>Test the backward function<a class="headerlink" href="#test-the-backward-function" title="Link to this heading">#</a></h3>
<p>Now letâ€™s test the backward operation using <code class="docutils literal notranslate"><span class="pre">jax.grad</span></code> and <code class="docutils literal notranslate"><span class="pre">jtu.check_grads</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">rms_norm_fwd</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">predictions</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>


<span class="n">loss_grad</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">loss_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
<span class="n">jtu</span><span class="o">.</span><span class="n">check_grads</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">),</span> <span class="n">modes</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;rev&quot;</span><span class="p">],</span> <span class="n">order</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">---------------------------------------------------------------------------</span>
<span class="ne">NotImplementedError</span>                       <span class="n">Traceback</span> <span class="p">(</span><span class="n">most</span> <span class="n">recent</span> <span class="n">call</span> <span class="n">last</span><span class="p">)</span>
<span class="n">Cell</span> <span class="n">In</span> <span class="p">[</span><span class="mi">8</span><span class="p">],</span> <span class="n">line</span> <span class="mi">7</span>
      <span class="mi">3</span>     <span class="k">return</span> <span class="o">-</span><span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">predictions</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
      <span class="mi">6</span> <span class="n">loss_grad</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<span class="o">----&gt;</span> <span class="mi">7</span> <span class="n">out</span> <span class="o">=</span> <span class="n">loss_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>

<span class="o">...</span>

<span class="ne">NotImplementedError</span><span class="p">:</span> <span class="n">Differentiation</span> <span class="n">rule</span> <span class="k">for</span> <span class="s1">&#39;rms_norm_fwd&#39;</span> <span class="ow">not</span> <span class="n">implemented</span>
</pre></div>
</div>
</section>
</section>
<section id="differentiation-rule">
<h2>Differentiation rule<a class="headerlink" href="#differentiation-rule" title="Link to this heading">#</a></h2>
<p>The backward operation failed with the error <code class="docutils literal notranslate"><span class="pre">NotImplementedError:</span> <span class="pre">Differentiation</span> <span class="pre">rule</span> <span class="pre">for</span> <span class="pre">'rms_norm_fwd'</span> <span class="pre">not</span> <span class="pre">implemented</span></code>.  It means that, although we have defined <code class="docutils literal notranslate"><span class="pre">rms_norm_fwd</span></code> and <code class="docutils literal notranslate"><span class="pre">rms_norm_bwd</span></code>, JAX doesnâ€™t know the relationship between them.</p>
<p>We can teach JAX that <code class="docutils literal notranslate"><span class="pre">rms_norm_bwd</span></code> is the backward operation for <code class="docutils literal notranslate"><span class="pre">rms_norm_fwd</span></code>, using <code class="docutils literal notranslate"><span class="pre">jax.custom_vjp</span></code> and its convention.  As the first step, we need to refine the definition of <code class="docutils literal notranslate"><span class="pre">rms_norm_fwd</span></code> and <code class="docutils literal notranslate"><span class="pre">rms_norm_bwd</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># rms_norm_fwd was previously defined as</span>
<span class="c1">#</span>
<span class="c1"># def rms_norm_fwd(x, weight, eps=1e-05):</span>
<span class="c1">#     output, invvar = _rms_norm_fwd_p.bind(x, weight, eps=eps)</span>
<span class="c1">#     return output</span>
<span class="c1">#</span>
<span class="k">def</span> <span class="nf">rms_norm_fwd</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">):</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">invvar</span> <span class="o">=</span> <span class="n">_rms_norm_fwd_p</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="p">(</span><span class="n">invvar</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>


<span class="c1"># rms_norm_bwd was previously defined as</span>
<span class="c1">#</span>
<span class="c1"># def rms_norm_bwd(g, invvar, x, weight, eps):</span>
<span class="c1">#     grad_input, grad_weight, part_grad = _rms_norm_bwd_p.bind(</span>
<span class="c1">#         g, invvar, x, weight, eps=eps</span>
<span class="c1">#     )</span>
<span class="c1">#     return grad_input, grad_weight</span>
<span class="c1">#</span>
<span class="k">def</span> <span class="nf">rms_norm_bwd</span><span class="p">(</span><span class="n">eps</span><span class="p">,</span> <span class="n">res</span><span class="p">,</span> <span class="n">g</span><span class="p">):</span>
    <span class="n">invvar</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span> <span class="o">=</span> <span class="n">res</span>
    <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_weight</span><span class="p">,</span> <span class="n">part_grad</span> <span class="o">=</span> <span class="n">_rms_norm_bwd_p</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span>
        <span class="n">g</span><span class="p">,</span> <span class="n">invvar</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_weight</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">rms_norm_fwd</span></code> now returns an extra output <code class="docutils literal notranslate"><span class="pre">(invvar,</span> <span class="pre">x,</span> <span class="pre">weight)</span></code> for the residual data and <code class="docutils literal notranslate"><span class="pre">rms_norm_bwd</span></code> takes <code class="docutils literal notranslate"><span class="pre">eps</span></code>, <code class="docutils literal notranslate"><span class="pre">res</span></code>, and <code class="docutils literal notranslate"><span class="pre">g</span></code> as the parameters.</p>
<p>Once the relationship between <code class="docutils literal notranslate"><span class="pre">rms_norm_fwd</span></code> and <code class="docutils literal notranslate"><span class="pre">rms_norm_bwd</span></code> is established through <code class="docutils literal notranslate"><span class="pre">jax.custom_vjp</span></code>, JAX will ensure that the residual data from <code class="docutils literal notranslate"><span class="pre">rms_norm_fwd</span></code> is passed to <code class="docutils literal notranslate"><span class="pre">rms_norm_bwd</span></code> as <code class="docutils literal notranslate"><span class="pre">res</span></code> for backward operation.
For non-differentiable parameters such as <code class="docutils literal notranslate"><span class="pre">eps</span></code>, JAX ensures that they are passed to the backward operation before the residual data.  Thatâ€™s why <code class="docutils literal notranslate"><span class="pre">eps</span></code> precedes <code class="docutils literal notranslate"><span class="pre">res</span></code> in the parameter list of <code class="docutils literal notranslate"><span class="pre">rms_norm_bwd</span></code>.</p>
<p>Now that <code class="docutils literal notranslate"><span class="pre">rms_norm_fwd</span></code> returns the residual data, which is not needed for simple RMS normalization operation, we define a wrapper around it, <code class="docutils literal notranslate"><span class="pre">rms_norm</span></code>.  It simply calls <code class="docutils literal notranslate"><span class="pre">rms_norm_fwd</span></code> and returns only <code class="docutils literal notranslate"><span class="pre">output</span></code>.  Note that <code class="docutils literal notranslate"><span class="pre">rms_norm</span></code> is annotated with <code class="docutils literal notranslate"><span class="pre">&#64;partial(jax.custom_vjp,</span> <span class="pre">nondiff_argnums=(2,))</span></code> and we are passing <code class="docutils literal notranslate"><span class="pre">rms_norm_fwd</span></code> and <code class="docutils literal notranslate"><span class="pre">rms_norm_bwd</span></code> to <code class="docutils literal notranslate"><span class="pre">rms_norm.defvjp</span></code>.  It teaches JAX that, when <code class="docutils literal notranslate"><span class="pre">rms_norm</span></code> is differentiated, <code class="docutils literal notranslate"><span class="pre">rms_norm_fwd</span></code> is to be used for forward operation, and <code class="docutils literal notranslate"><span class="pre">rms_norm_bwd</span></code> is to be used for backward operation.</p>
<p>See <a class="reference external" href="https://jax.readthedocs.io/en/latest/notebooks/Custom_derivative_rules_for_Python_code.html#use-jax-custom-vjp-to-define-custom-reverse-mode-only-rules">Custom derivative rules for JAX-transformable Python functions</a> for more information on <code class="docutils literal notranslate"><span class="pre">jax.custom_vjp</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">custom_vjp</span><span class="p">,</span> <span class="n">nondiff_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,))</span>
<span class="k">def</span> <span class="nf">rms_norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">):</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">rms_norm_fwd</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>


<span class="n">rms_norm</span><span class="o">.</span><span class="n">defvjp</span><span class="p">(</span><span class="n">rms_norm_fwd</span><span class="p">,</span> <span class="n">rms_norm_bwd</span><span class="p">)</span>
</pre></div>
</div>
<p>With the refinement we have made, the backward operation test works with a modification: <code class="docutils literal notranslate"><span class="pre">loss</span></code> now calls <code class="docutils literal notranslate"><span class="pre">rms_norm</span></code> instead of <code class="docutils literal notranslate"><span class="pre">rms_norm_fwd</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">rms_norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">predictions</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>


<span class="n">loss_grad</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">loss_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
<span class="n">jtu</span><span class="o">.</span><span class="n">check_grads</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">),</span> <span class="n">modes</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;rev&quot;</span><span class="p">],</span> <span class="n">order</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="let-s-test-it-on-multiple-devices">
<h2>Letâ€™s test it on multiple devices<a class="headerlink" href="#let-s-test-it-on-multiple-devices" title="Link to this heading">#</a></h2>
<p>We are using <code class="docutils literal notranslate"><span class="pre">jax.experimental.pjit.pjit</span></code> for parallel execution on multiple devices, and we produce reference values with sequential execution on a single device.</p>
<section id="id1">
<h3>Test the forward function<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<p>Letâ€™s first test the forward operation on multiple devices.  We are creating a simple 1D mesh and sharding <code class="docutils literal notranslate"><span class="pre">x</span></code> on all devices.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">jax.sharding</span> <span class="kn">import</span> <span class="n">Mesh</span><span class="p">,</span> <span class="n">PartitionSpec</span>
<span class="kn">from</span> <span class="nn">jax.experimental.pjit</span> <span class="kn">import</span> <span class="n">pjit</span>


<span class="n">mesh</span> <span class="o">=</span> <span class="n">Mesh</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">local_devices</span><span class="p">(),</span> <span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,))</span>
<span class="n">ref</span> <span class="o">=</span> <span class="n">rms_norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
<span class="n">pjitted</span> <span class="o">=</span> <span class="n">pjit</span><span class="p">(</span>
    <span class="n">rms_norm</span><span class="p">,</span>
    <span class="c1"># Shard x by batch dimension and replicate weight on all devices.</span>
    <span class="n">in_shardings</span><span class="o">=</span><span class="p">(</span><span class="n">PartitionSpec</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)),</span>
    <span class="c1"># Shard the output by batch dimension.</span>
    <span class="n">out_shardings</span><span class="o">=</span><span class="n">PartitionSpec</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
<span class="p">)</span>

<span class="k">with</span> <span class="n">mesh</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">pjitted</span><span class="o">.</span><span class="n">lower</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span><span class="o">.</span><span class="n">runtime_executable</span><span class="p">()</span><span class="o">.</span><span class="n">hlo_modules</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to_string</span><span class="p">())</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">pjitted</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>

<span class="n">jnp</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">ref</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">HloModule</span> <span class="n">pjit_rms_norm</span><span class="p">,</span> <span class="n">entry_computation_layout</span><span class="o">=</span><span class="p">{(</span><span class="n">bf16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">},</span><span class="n">bf16</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">})</span><span class="o">-&gt;</span><span class="n">bf16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}}</span>

<span class="o">%</span><span class="n">fused_computation</span> <span class="p">(</span><span class="n">param_1</span><span class="p">:</span> <span class="n">bf16</span><span class="p">[</span><span class="mi">32</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">],</span> <span class="n">param_1</span><span class="mf">.3</span><span class="p">:</span> <span class="n">u32</span><span class="p">[])</span> <span class="o">-&gt;</span> <span class="n">bf16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]</span> <span class="p">{</span>
  <span class="o">%</span><span class="n">param_1</span> <span class="o">=</span> <span class="n">bf16</span><span class="p">[</span><span class="mi">32</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="n">parameter</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
  <span class="o">%</span><span class="n">param_1</span><span class="mf">.3</span> <span class="o">=</span> <span class="n">u32</span><span class="p">[]</span> <span class="n">parameter</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
  <span class="o">%</span><span class="n">convert</span><span class="mf">.2</span> <span class="o">=</span> <span class="n">s32</span><span class="p">[]</span> <span class="n">convert</span><span class="p">(</span><span class="n">u32</span><span class="p">[]</span> <span class="o">%</span><span class="n">param_1</span><span class="mf">.3</span><span class="p">),</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="n">op_name</span><span class="o">=</span><span class="s2">&quot;pjit(rms_norm)/jit(main)/rms_norm_fwd[eps=1e-05]&quot;</span> <span class="n">source_file</span><span class="o">=</span><span class="s2">&quot;/tmp/ipykernel_25235/3343076723.py&quot;</span> <span class="n">source_line</span><span class="o">=</span><span class="mi">8</span><span class="p">}</span>
  <span class="o">%</span><span class="n">constant_9</span> <span class="o">=</span> <span class="n">s32</span><span class="p">[]</span> <span class="n">constant</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="n">op_name</span><span class="o">=</span><span class="s2">&quot;pjit(rms_norm)/jit(main)/rms_norm_fwd[eps=1e-05]&quot;</span> <span class="n">source_file</span><span class="o">=</span><span class="s2">&quot;/tmp/ipykernel_25235/3343076723.py&quot;</span> <span class="n">source_line</span><span class="o">=</span><span class="mi">8</span><span class="p">}</span>
  <span class="o">%</span><span class="n">multiply</span><span class="mf">.3</span> <span class="o">=</span> <span class="n">s32</span><span class="p">[]</span> <span class="n">multiply</span><span class="p">(</span><span class="n">s32</span><span class="p">[]</span> <span class="o">%</span><span class="n">convert</span><span class="mf">.2</span><span class="p">,</span> <span class="n">s32</span><span class="p">[]</span> <span class="o">%</span><span class="n">constant_9</span><span class="p">),</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="n">op_name</span><span class="o">=</span><span class="s2">&quot;pjit(rms_norm)/jit(main)/rms_norm_fwd[eps=1e-05]&quot;</span> <span class="n">source_file</span><span class="o">=</span><span class="s2">&quot;/tmp/ipykernel_25235/3343076723.py&quot;</span> <span class="n">source_line</span><span class="o">=</span><span class="mi">8</span><span class="p">}</span>
  <span class="o">%</span><span class="n">constant_8</span> <span class="o">=</span> <span class="n">s32</span><span class="p">[]</span> <span class="n">constant</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="n">op_name</span><span class="o">=</span><span class="s2">&quot;pjit(rms_norm)/jit(main)/rms_norm_fwd[eps=1e-05]&quot;</span> <span class="n">source_file</span><span class="o">=</span><span class="s2">&quot;/tmp/ipykernel_25235/3343076723.py&quot;</span> <span class="n">source_line</span><span class="o">=</span><span class="mi">8</span><span class="p">}</span>
  <span class="n">ROOT</span> <span class="o">%</span><span class="n">dynamic</span><span class="o">-</span><span class="nb">slice</span><span class="mf">.2</span> <span class="o">=</span> <span class="n">bf16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="n">dynamic</span><span class="o">-</span><span class="nb">slice</span><span class="p">(</span><span class="n">bf16</span><span class="p">[</span><span class="mi">32</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="o">%</span><span class="n">param_1</span><span class="p">,</span> <span class="n">s32</span><span class="p">[]</span> <span class="o">%</span><span class="n">multiply</span><span class="mf">.3</span><span class="p">,</span> <span class="n">s32</span><span class="p">[]</span> <span class="o">%</span><span class="n">constant_8</span><span class="p">,</span> <span class="n">s32</span><span class="p">[]</span> <span class="o">%</span><span class="n">constant_8</span><span class="p">),</span> <span class="n">dynamic_slice_sizes</span><span class="o">=</span><span class="p">{</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">},</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="n">op_name</span><span class="o">=</span><span class="s2">&quot;pjit(rms_norm)/jit(main)/rms_norm_fwd[eps=1e-05]&quot;</span> <span class="n">source_file</span><span class="o">=</span><span class="s2">&quot;/tmp/ipykernel_25235/3343076723.py&quot;</span> <span class="n">source_line</span><span class="o">=</span><span class="mi">8</span><span class="p">}</span>
<span class="p">}</span>

<span class="n">ENTRY</span> <span class="o">%</span><span class="n">main</span><span class="mf">.7</span><span class="n">_spmd</span> <span class="p">(</span><span class="n">param</span><span class="p">:</span> <span class="n">bf16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">],</span> <span class="n">param</span><span class="mf">.1</span><span class="p">:</span> <span class="n">bf16</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">bf16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]</span> <span class="p">{</span>
  <span class="o">%</span><span class="n">param</span> <span class="o">=</span> <span class="n">bf16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="n">parameter</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">sharding</span><span class="o">=</span><span class="p">{</span><span class="n">devices</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">}</span>
  <span class="o">%</span><span class="nb">all</span><span class="o">-</span><span class="n">gather</span> <span class="o">=</span> <span class="n">bf16</span><span class="p">[</span><span class="mi">32</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="nb">all</span><span class="o">-</span><span class="n">gather</span><span class="p">(</span><span class="n">bf16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="o">%</span><span class="n">param</span><span class="p">),</span> <span class="n">channel_id</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">replica_groups</span><span class="o">=</span><span class="p">{{</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">}},</span> <span class="n">dimensions</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">},</span> <span class="n">use_global_device_ids</span><span class="o">=</span><span class="n">true</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="n">op_name</span><span class="o">=</span><span class="s2">&quot;pjit(rms_norm)/jit(main)/rms_norm_fwd[eps=1e-05]&quot;</span> <span class="n">source_file</span><span class="o">=</span><span class="s2">&quot;/tmp/ipykernel_25235/3343076723.py&quot;</span> <span class="n">source_line</span><span class="o">=</span><span class="mi">8</span><span class="p">}</span>
  <span class="o">%</span><span class="n">param</span><span class="mf">.1</span> <span class="o">=</span> <span class="n">bf16</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="n">parameter</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">sharding</span><span class="o">=</span><span class="p">{</span><span class="n">replicated</span><span class="p">}</span>
  <span class="o">%</span><span class="n">custom</span><span class="o">-</span><span class="n">call</span><span class="mf">.0</span> <span class="o">=</span> <span class="p">(</span><span class="n">bf16</span><span class="p">[</span><span class="mi">32</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">},</span> <span class="n">f32</span><span class="p">[</span><span class="mi">32</span><span class="p">]{</span><span class="mi">0</span><span class="p">})</span> <span class="n">custom</span><span class="o">-</span><span class="n">call</span><span class="p">(</span><span class="n">bf16</span><span class="p">[</span><span class="mi">32</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="o">%</span><span class="nb">all</span><span class="o">-</span><span class="n">gather</span><span class="p">,</span> <span class="n">bf16</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="o">%</span><span class="n">param</span><span class="mf">.1</span><span class="p">),</span> <span class="n">custom_call_target</span><span class="o">=</span><span class="s2">&quot;rms_forward_affine_mixed_dtype&quot;</span><span class="p">,</span> <span class="n">operand_layout_constraints</span><span class="o">=</span><span class="p">{</span><span class="n">bf16</span><span class="p">[</span><span class="mi">32</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">},</span> <span class="n">bf16</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}},</span> <span class="n">api_version</span><span class="o">=</span><span class="n">API_VERSION_STATUS_RETURNING</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="n">op_name</span><span class="o">=</span><span class="s2">&quot;pjit(rms_norm)/jit(main)/rms_norm_fwd[eps=1e-05]&quot;</span> <span class="n">source_file</span><span class="o">=</span><span class="s2">&quot;/tmp/ipykernel_25235/3343076723.py&quot;</span> <span class="n">source_line</span><span class="o">=</span><span class="mi">8</span><span class="p">},</span> <span class="n">backend_config</span><span class="o">=</span><span class="s2">&quot; </span><span class="se">\000\000\000\000\000\004\000\361</span><span class="s2">h</span><span class="se">\343\210\265\370\344</span><span class="s2">&gt;</span><span class="se">\000\000\000\000\000\000\000\000\000\000\000\000\255\177\000\000</span><span class="s2">&quot;</span>
  <span class="o">%</span><span class="n">get</span><span class="o">-</span><span class="nb">tuple</span><span class="o">-</span><span class="n">element</span> <span class="o">=</span> <span class="n">bf16</span><span class="p">[</span><span class="mi">32</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="n">get</span><span class="o">-</span><span class="nb">tuple</span><span class="o">-</span><span class="n">element</span><span class="p">((</span><span class="n">bf16</span><span class="p">[</span><span class="mi">32</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">},</span> <span class="n">f32</span><span class="p">[</span><span class="mi">32</span><span class="p">]{</span><span class="mi">0</span><span class="p">})</span> <span class="o">%</span><span class="n">custom</span><span class="o">-</span><span class="n">call</span><span class="mf">.0</span><span class="p">),</span> <span class="n">index</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="n">op_name</span><span class="o">=</span><span class="s2">&quot;pjit(rms_norm)/jit(main)/rms_norm_fwd[eps=1e-05]&quot;</span> <span class="n">source_file</span><span class="o">=</span><span class="s2">&quot;/tmp/ipykernel_25235/3343076723.py&quot;</span> <span class="n">source_line</span><span class="o">=</span><span class="mi">8</span><span class="p">}</span>
  <span class="o">%</span><span class="n">partition</span><span class="o">-</span><span class="nb">id</span> <span class="o">=</span> <span class="n">u32</span><span class="p">[]</span> <span class="n">partition</span><span class="o">-</span><span class="nb">id</span><span class="p">(),</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="n">op_name</span><span class="o">=</span><span class="s2">&quot;pjit(rms_norm)/jit(main)/rms_norm_fwd[eps=1e-05]&quot;</span> <span class="n">source_file</span><span class="o">=</span><span class="s2">&quot;/tmp/ipykernel_25235/3343076723.py&quot;</span> <span class="n">source_line</span><span class="o">=</span><span class="mi">8</span><span class="p">}</span>
  <span class="n">ROOT</span> <span class="o">%</span><span class="n">fusion</span> <span class="o">=</span> <span class="n">bf16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="n">fusion</span><span class="p">(</span><span class="n">bf16</span><span class="p">[</span><span class="mi">32</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="o">%</span><span class="n">get</span><span class="o">-</span><span class="nb">tuple</span><span class="o">-</span><span class="n">element</span><span class="p">,</span> <span class="n">u32</span><span class="p">[]</span> <span class="o">%</span><span class="n">partition</span><span class="o">-</span><span class="nb">id</span><span class="p">),</span> <span class="n">kind</span><span class="o">=</span><span class="n">kLoop</span><span class="p">,</span> <span class="n">calls</span><span class="o">=%</span><span class="n">fused_computation</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="n">op_name</span><span class="o">=</span><span class="s2">&quot;pjit(rms_norm)/jit(main)/rms_norm_fwd[eps=1e-05]&quot;</span> <span class="n">source_file</span><span class="o">=</span><span class="s2">&quot;/tmp/ipykernel_25235/3343076723.py&quot;</span> <span class="n">source_line</span><span class="o">=</span><span class="mi">8</span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kc">True</span>
</pre></div>
</div>
<p>The values have been computed correctly for forward operation, however, the generated HLO modules show an <code class="docutils literal notranslate"><span class="pre">all-gather</span></code> operation to replicate <code class="docutils literal notranslate"><span class="pre">x</span></code> on all devices, incurring large communication overhead.</p>
<p>As XLA does not have enough knowledge about the custom functions to shard input tensors, it decides to replicate them to produce correct values before making the custom call.</p>
<p>To avoid this duplication, we can:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://jax.readthedocs.io/en/latest/jax.experimental.custom_partitioning.html">custom_partitioning</a>: to make it behave like all native JAX operations (but more complicated)</p></li>
<li><p>Use manual sharding</p>
<ul>
<li><p><a class="reference external" href="https://jax.readthedocs.io/en/latest/jep/14273-shard-map.html">shard_map</a>: the new replacement for xmap</p></li>
<li><p><a class="reference external" href="https://jax.readthedocs.io/en/latest/notebooks/xmap_tutorial.html">xmap</a> (now deprecated)</p></li>
</ul>
</li>
</ul>
<p>This example demonstrates the use of custom_partitioning.</p>
</section>
<section id="shard-the-forward-function-with-custom-partitioning">
<h3>Shard the forward function with custom_partitioning<a class="headerlink" href="#shard-the-forward-function-with-custom-partitioning" title="Link to this heading">#</a></h3>
<p>We first create a helper function to help with all the JAX/XLA callback registration required.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">register_primitive</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    register jax primitive</span>

<span class="sd">    The order of calls. Each operation is composed of two primitives: Inner and Outer.</span>

<span class="sd">    Inner, only the basic to wrap the custom_call itself.</span>
<span class="sd">    - impl to XLA custom_call in C.</span>
<span class="sd">    - abstract to know the static shapes</span>
<span class="sd">    - lower to StableHLO XLA custom_call.</span>
<span class="sd">    Outer, mostly all the rest:</span>
<span class="sd">    - impl: Bind to the inner primitive. Not used for real computation, but only for tracing. So we only need to bind.</span>
<span class="sd">    - abstract: same</span>
<span class="sd">    - lower to StableHLO custom_p. (XLA will call the python callback from it)</span>
<span class="sd">    - custom_p</span>
<span class="sd">    - vmap: could be added here.</span>
<span class="sd">    VJP is based on Outer, but not handled in this function.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">name_of_wrapper_p</span><span class="p">():</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;_wrapper&quot;</span>

    <span class="n">inner_p</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">Primitive</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="n">dispatch</span><span class="o">.</span><span class="n">prim_requires_devices_during_lowering</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">inner_p</span><span class="p">)</span>
    <span class="n">inner_p</span><span class="o">.</span><span class="n">multiple_results</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">multiple_results</span>
    <span class="n">inner_p</span><span class="o">.</span><span class="n">def_impl</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">xla</span><span class="o">.</span><span class="n">apply_primitive</span><span class="p">,</span> <span class="n">inner_p</span><span class="p">))</span>
    <span class="n">inner_p</span><span class="o">.</span><span class="n">def_abstract_eval</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">abstract</span><span class="p">)</span>
    <span class="n">mlir</span><span class="o">.</span><span class="n">register_lowering</span><span class="p">(</span><span class="n">inner_p</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">lowering</span><span class="p">,</span> <span class="n">platform</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
    <span class="bp">cls</span><span class="o">.</span><span class="n">inner_primitive</span> <span class="o">=</span> <span class="n">inner_p</span>

    <span class="n">outer_p</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">Primitive</span><span class="p">(</span><span class="n">name_of_wrapper_p</span><span class="p">())</span>
    <span class="n">dispatch</span><span class="o">.</span><span class="n">prim_requires_devices_during_lowering</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">outer_p</span><span class="p">)</span>
    <span class="n">outer_p</span><span class="o">.</span><span class="n">multiple_results</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">multiple_results</span>
    <span class="n">outer_p</span><span class="o">.</span><span class="n">def_impl</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">impl</span><span class="p">)</span>
    <span class="n">outer_p</span><span class="o">.</span><span class="n">def_abstract_eval</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">abstract</span><span class="p">)</span>
    <span class="n">batching</span><span class="o">.</span><span class="n">primitive_batchers</span><span class="p">[</span><span class="n">outer_p</span><span class="p">]</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">batcher</span>
    <span class="n">outer_p_lower</span> <span class="o">=</span> <span class="n">custom_partitioning</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">impl</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">impl_static_args</span><span class="p">)</span>
    <span class="n">outer_p_lower</span><span class="o">.</span><span class="n">def_partition</span><span class="p">(</span><span class="n">infer_sharding_from_operands</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">infer_sharding_from_operands</span><span class="p">,</span>
                                <span class="n">partition</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">partition</span><span class="p">)</span>
    <span class="n">mlir</span><span class="o">.</span><span class="n">register_lowering</span><span class="p">(</span><span class="n">outer_p</span><span class="p">,</span>
                           <span class="n">mlir</span><span class="o">.</span><span class="n">lower_fun</span><span class="p">(</span><span class="n">outer_p_lower</span><span class="p">,</span> <span class="n">multiple_results</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">multiple_results</span><span class="p">))</span>
    <span class="bp">cls</span><span class="o">.</span><span class="n">outer_primitive</span> <span class="o">=</span> <span class="n">outer_p</span>
<span class="o">...</span>
</pre></div>
</div>
<p>We define 2 JAX primitives, one inner primitive that map to the
real kernel we want to warp in JAX. And an outer primitive that will
be used with the custom_partitioning registration and for the
gradient. (And if you implement the interface to support vmat, it will
also be on the outer primitive).</p>
<p>JAX custom_partitioning implementation are callbacks from XLA to Python during XLA sharding logic.
XLA sharding goes in two phases: a sharding propagation phase and a partition phase.
The propagation phase is when XLA plan the sharding to be created. It is the partition phase that create the sharded graph.
For XLA to be able to shard our custom operations, it needs us to define 2 extra functions:
infer_sharding_from_operands() and partition(). They are used in the first and second phase respectively.</p>
<p>The infer_sharding_from_operands() function must do what its name say: infer the output sharding from the input sharding.</p>
<p>The partition() function will do a few things:</p>
<ul class="simple">
<li><p>tell which input sharding will be expected. XLA will reshad if needed.</p></li>
<li><p>tell the final version of the output sharding.</p></li>
<li><p>give a function that will create the new instruction from the sharded inputs.</p></li>
</ul>
<p>See the code comments for more explanation:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">RmsNormFwdClass</span><span class="p">:</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;rms_forward_affine_mixed_dtype&quot;</span>
    <span class="n">multiple_results</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">impl_static_args</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,)</span>    <span class="c1"># eps</span>
    <span class="n">inner_primitive</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">outer_primitive</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">infer_sharding_from_operands</span><span class="p">(</span><span class="n">eps</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">mesh</span> <span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">Mesh</span><span class="p">,</span>
                                     <span class="n">arg_infos</span> <span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">jax</span><span class="o">.</span><span class="n">_src</span><span class="o">.</span><span class="n">api</span><span class="o">.</span><span class="n">ShapeDtypeStruct</span><span class="p">],</span>
                                     <span class="n">result_infos</span> <span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">jax</span><span class="o">.</span><span class="n">_src</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">ShapedArray</span><span class="p">]):</span>
        <span class="k">del</span> <span class="n">eps</span><span class="p">,</span> <span class="n">result_infos</span>  <span class="c1"># Not needed for this example.</span>
        <span class="n">x_info</span><span class="p">,</span> <span class="n">weight_info</span> <span class="o">=</span> <span class="n">arg_infos</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_info</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">weight_info</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
        <span class="c1"># partition() will force all dims of all inputs to be replicated except the</span>
        <span class="c1"># first dim of x that will be kept as is.</span>
        <span class="c1"># This is because the implementaion can only be sharded on the batch dimensions.</span>

        <span class="n">x_spec</span> <span class="o">=</span> <span class="n">arg_infos</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">spec</span>
        <span class="c1"># None mean that we replicate on that dimension.</span>
        <span class="n">output_sharding</span> <span class="o">=</span> <span class="n">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="n">x_spec</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
        <span class="n">invvar_sharding</span> <span class="o">=</span> <span class="n">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="n">x_spec</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">output_sharding</span><span class="p">,</span> <span class="n">invvar_sharding</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">partition</span><span class="p">(</span><span class="n">eps</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">mesh</span> <span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">Mesh</span><span class="p">,</span>
                  <span class="n">arg_infos</span> <span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">jax</span><span class="o">.</span><span class="n">_src</span><span class="o">.</span><span class="n">api</span><span class="o">.</span><span class="n">ShapeDtypeStruct</span><span class="p">],</span>
                  <span class="n">result_infos</span> <span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">jax</span><span class="o">.</span><span class="n">_src</span><span class="o">.</span><span class="n">api</span><span class="o">.</span><span class="n">ShapeDtypeStruct</span><span class="p">]):</span>
        <span class="k">del</span> <span class="n">result_infos</span>  <span class="c1"># Not needed for this example.</span>
        <span class="n">x_info</span><span class="p">,</span> <span class="n">weight_info</span> <span class="o">=</span> <span class="n">arg_infos</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_info</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">weight_info</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
        <span class="n">x_spec</span> <span class="o">=</span> <span class="n">arg_infos</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">spec</span>
        <span class="c1"># We only support sharding on the batch dimensions.</span>
        <span class="c1"># Force sharding on all others dimensions with None.</span>
        <span class="n">arg_shardings</span> <span class="o">=</span> <span class="p">(</span><span class="n">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="n">x_spec</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)),</span>
                         <span class="n">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)))</span>
        <span class="n">invvar_sharding</span> <span class="o">=</span> <span class="n">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="n">x_spec</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">output_shardings</span> <span class="o">=</span> <span class="p">(</span><span class="n">arg_shardings</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">invvar_sharding</span><span class="p">)</span>
        <span class="c1"># Sharded_impl only accepts positional arugments</span>
        <span class="c1"># And they should be Jax traceable variables</span>
        <span class="n">impl</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">RmsNormFwdClass</span><span class="o">.</span><span class="n">impl</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">mesh</span><span class="p">,</span> <span class="n">impl</span><span class="p">,</span> <span class="n">output_shardings</span><span class="p">,</span> <span class="n">arg_shardings</span>
<span class="n">register_primitive</span><span class="p">(</span><span class="n">RmsNormFwdClass</span><span class="p">)</span>
</pre></div>
</div>
<p>Next we define the primitive for the backward pass of RMSNorm</p>
</section>
<section id="shard-the-backward-function-with-custom-partitioning">
<h3>Shard the backward function with custom_partitioning<a class="headerlink" href="#shard-the-backward-function-with-custom-partitioning" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">RmsNormBwdClass</span><span class="p">:</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;rms_norm_bwd&quot;</span>
    <span class="n">multiple_results</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">impl_static_args</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,)</span>    <span class="c1"># eps</span>
    <span class="n">inner_primitive</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">outer_primitive</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">infer_sharding_from_operands</span><span class="p">(</span><span class="n">eps</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">mesh</span> <span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">Mesh</span><span class="p">,</span>
                                     <span class="n">arg_infos</span> <span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">jax</span><span class="o">.</span><span class="n">_src</span><span class="o">.</span><span class="n">api</span><span class="o">.</span><span class="n">ShapeDtypeStruct</span><span class="p">],</span>
                                     <span class="n">result_infos</span> <span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">jax</span><span class="o">.</span><span class="n">_src</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">ShapedArray</span><span class="p">]):</span>
        <span class="k">del</span> <span class="n">eps</span><span class="p">,</span> <span class="n">result_infos</span>  <span class="c1"># Not needed for this example.</span>
        <span class="n">g_info</span><span class="p">,</span> <span class="n">invvar_info</span><span class="p">,</span> <span class="n">x_info</span><span class="p">,</span> <span class="n">weight_info</span> <span class="o">=</span> <span class="n">arg_infos</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">g_info</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">invvar_info</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_info</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">weight_info</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
        <span class="c1"># partition() will force all dims to be replicated except the batch dimension.</span>
        <span class="n">x_spec</span> <span class="o">=</span> <span class="n">x_info</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">spec</span>
        <span class="n">output_sharding</span> <span class="o">=</span> <span class="n">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="n">x_spec</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
        <span class="n">invvar_sharding</span> <span class="o">=</span> <span class="n">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">output_sharding</span><span class="p">,</span> <span class="n">invvar_sharding</span><span class="p">,</span> <span class="n">output_sharding</span><span class="p">,</span> <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">partition</span><span class="p">(</span><span class="n">eps</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">mesh</span> <span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">Mesh</span><span class="p">,</span>
                  <span class="n">arg_infos</span> <span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">jax</span><span class="o">.</span><span class="n">_src</span><span class="o">.</span><span class="n">api</span><span class="o">.</span><span class="n">ShapeDtypeStruct</span><span class="p">],</span>
                  <span class="n">result_infos</span> <span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">jax</span><span class="o">.</span><span class="n">_src</span><span class="o">.</span><span class="n">api</span><span class="o">.</span><span class="n">ShapeDtypeStruct</span><span class="p">]):</span>
        <span class="k">del</span> <span class="n">result_infos</span>  <span class="c1"># Not needed for this example.</span>
        <span class="n">g_info</span><span class="p">,</span> <span class="n">invvar_info</span><span class="p">,</span> <span class="n">x_info</span><span class="p">,</span> <span class="n">weight_info</span> <span class="o">=</span> <span class="n">arg_infos</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">g_info</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">invvar_info</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_info</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">weight_info</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>

        <span class="c1"># We only support sharding on the batch dimensions.</span>
        <span class="c1"># Force sharding on all others dimensions with None.</span>
        <span class="c1"># Also force gx, x and invvar to have the same batch sharding/replication.</span>
        <span class="n">x_spec</span> <span class="o">=</span> <span class="n">x_info</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">spec</span>
        <span class="n">arg_shardings</span> <span class="o">=</span> <span class="p">(</span><span class="n">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="n">x_spec</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)),</span>
                         <span class="n">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="n">x_spec</span><span class="p">[</span><span class="mi">0</span><span class="p">],)),</span>
                         <span class="n">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="n">x_spec</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)),</span>
                         <span class="n">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)))</span>

        <span class="n">output_sharding</span> <span class="o">=</span> <span class="n">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="n">x_spec</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
        <span class="n">invvar_sharding</span> <span class="o">=</span> <span class="n">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
        <span class="n">output_shardings</span> <span class="o">=</span> <span class="p">(</span><span class="n">output_sharding</span><span class="p">,</span> <span class="n">invvar_sharding</span><span class="p">,</span> <span class="n">invvar_sharding</span><span class="p">)</span>


        <span class="c1"># Sharded_impl only accepts positional arugments</span>
        <span class="c1"># And they should be Jax traceable variables</span>
        <span class="k">def</span> <span class="nf">impl</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">invvar</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
            <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_weight</span><span class="p">,</span> <span class="n">part_grad</span> <span class="o">=</span> <span class="n">_rms_norm_bwd_p</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span>
                <span class="n">g</span><span class="p">,</span> <span class="n">invvar</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span>
            <span class="p">)</span>
            <span class="c1"># We need to sum the weight gradient from all partition.</span>
            <span class="n">global_weight</span> <span class="o">=</span> <span class="n">grad_weight</span>
            <span class="k">if</span> <span class="n">x_spec</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                <span class="n">global_weight</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">psum</span><span class="p">(</span><span class="n">grad_weight</span><span class="p">,</span> <span class="n">x_spec</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">grad_input</span><span class="p">,</span> <span class="n">global_weight</span><span class="p">,</span> <span class="n">part_grad</span>
        <span class="k">return</span> <span class="n">mesh</span><span class="p">,</span> <span class="n">impl</span><span class="p">,</span> <span class="n">output_shardings</span><span class="p">,</span> <span class="n">arg_shardings</span>
<span class="n">register_primitive</span><span class="p">(</span><span class="n">RmsNormBwdClass</span><span class="p">)</span>
</pre></div>
</div>
<p>Plumbing to establish the forward and backward primtives with a custom_vjp rule as before:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">custom_vjp</span><span class="p">,</span> <span class="n">nondiff_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,))</span>
<span class="k">def</span> <span class="nf">custom_p_rms_norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">):</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">custom_p_rms_norm_fwd</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>
  
<span class="k">def</span> <span class="nf">custom_p_rms_norm_fwd</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">):</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">invvar</span> <span class="o">=</span> <span class="n">RmsNormFwdClass</span><span class="o">.</span><span class="n">outer_primitive</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="p">(</span><span class="n">invvar</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">custom_p_rms_norm_bwd</span><span class="p">(</span><span class="n">eps</span><span class="p">,</span> <span class="n">res</span><span class="p">,</span> <span class="n">g</span><span class="p">):</span>
    <span class="n">invvar</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span> <span class="o">=</span> <span class="n">res</span>
    <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_weight</span><span class="p">,</span> <span class="n">part_grad</span> <span class="o">=</span> <span class="n">RmsNormBwdClass</span><span class="o">.</span><span class="n">outer_primitive</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span>
        <span class="n">g</span><span class="p">,</span> <span class="n">invvar</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_weight</span>

<span class="n">custom_p_rms_norm</span><span class="o">.</span><span class="n">defvjp</span><span class="p">(</span><span class="n">custom_p_rms_norm_fwd</span><span class="p">,</span> <span class="n">custom_p_rms_norm_bwd</span><span class="p">)</span>
</pre></div>
</div>
<p>With that we have completely defined our custom RMS norm primitive with custom_partitioning. To check for correctness we define the following loss functions: ref_loss is the reference value to compare against, while custom_p_loss uses our new primitive that implements custom_partitioning.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ref_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">rms_norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">predictions</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>


<span class="n">ref</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">ref_loss</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">custom_p_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">custom_p_rms_norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">predictions</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="check-for-correctness">
<h1>Check for correctness<a class="headerlink" href="#check-for-correctness" title="Link to this heading">#</a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">Mesh</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">local_devices</span><span class="p">(),</span> <span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,)):</span>
    <span class="k">def</span> <span class="nf">run_and_verify</span><span class="p">(</span><span class="n">loss</span><span class="p">):</span>
        <span class="n">pjitted</span> <span class="o">=</span> <span class="n">pjit</span><span class="p">(</span>
            <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
            <span class="c1"># Shard x by batch dimension and replicate weight on all devices.</span>
            <span class="n">in_shardings</span><span class="o">=</span><span class="p">(</span>
                <span class="n">PartitionSpec</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
                <span class="n">PartitionSpec</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
            <span class="p">),</span>
            <span class="c1"># Shard the output by batch dimension and replicate weight grad on all devices.</span>
            <span class="n">out_shardings</span><span class="o">=</span><span class="p">(</span>
                <span class="n">PartitionSpec</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
                <span class="n">PartitionSpec</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
            <span class="p">),</span>
        <span class="p">)</span>
        <span class="n">hlo</span> <span class="o">=</span> <span class="n">pjitted</span><span class="o">.</span><span class="n">lower</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span><span class="o">.</span><span class="n">as_text</span><span class="p">()</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">pjitted</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">hlo</span><span class="p">)</span>
        <span class="k">assert</span> <span class="s2">&quot;all-reduce-done&quot;</span> <span class="ow">in</span> <span class="n">hlo</span><span class="p">,</span> <span class="s2">&quot;The gradient will produce wrong value!&quot;</span>
        <span class="k">if</span> <span class="s2">&quot;all-gather-start&quot;</span> <span class="ow">in</span> <span class="n">hlo</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;NOT OPTIMIZED, ALL_GATHER in the graph!&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="n">custom_p_out</span> <span class="o">=</span> <span class="n">run_and_verify</span><span class="p">(</span><span class="n">custom_p_loss</span><span class="p">)</span>


<span class="k">for</span> <span class="n">r</span><span class="p">,</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">ref_out</span><span class="p">,</span> <span class="n">custom_p_out</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">o</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">HloModule</span> <span class="n">pjit_custom_p_loss</span><span class="p">,</span> <span class="n">is_scheduled</span><span class="o">=</span><span class="n">true</span><span class="p">,</span> <span class="n">entry_computation_layout</span><span class="o">=</span><span class="p">{(</span><span class="n">f16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">},</span> <span class="n">f16</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">})</span><span class="o">-&gt;</span><span class="p">(</span><span class="n">f16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">},</span> <span class="n">f16</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">})},</span> <span class="n">allow_spmd_sharding_propagation_to_parameters</span><span class="o">=</span><span class="p">{</span><span class="n">false</span><span class="p">,</span><span class="n">false</span><span class="p">},</span> <span class="n">allow_spmd_sharding_propagation_to_output</span><span class="o">=</span><span class="p">{</span><span class="n">false</span><span class="p">,</span><span class="n">false</span><span class="p">},</span> <span class="n">num_partitions</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">frontend_attributes</span><span class="o">=</span><span class="p">{</span><span class="n">fingerprint_before_lhs</span><span class="o">=</span><span class="s2">&quot;d7b9bc40de002332dd665ff2ab537b76&quot;</span><span class="p">}</span>

<span class="o">%</span><span class="n">fused_multiply</span> <span class="p">(</span><span class="n">param_0</span><span class="p">:</span> <span class="n">f16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">f16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]</span> <span class="p">{</span>
  <span class="o">%</span><span class="n">param_0</span> <span class="o">=</span> <span class="n">f16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="n">parameter</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
  <span class="o">%</span><span class="n">constant_4_1</span> <span class="o">=</span> <span class="n">f16</span><span class="p">[]</span> <span class="n">constant</span><span class="p">(</span><span class="o">-</span><span class="mf">4.7684e-07</span><span class="p">)</span>
  <span class="o">%</span><span class="n">broadcast</span><span class="mf">.8.1</span> <span class="o">=</span> <span class="n">f16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="n">broadcast</span><span class="p">(</span><span class="n">f16</span><span class="p">[]</span> <span class="o">%</span><span class="n">constant_4_1</span><span class="p">),</span> <span class="n">dimensions</span><span class="o">=</span><span class="p">{},</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="n">op_name</span><span class="o">=</span><span class="s2">&quot;pjit(custom_p_loss)/jit(main)/mul&quot;</span> <span class="n">source_file</span><span class="o">=</span><span class="s2">&quot;/opt/jax/docs/Custom_Operation_for_GPUs.py&quot;</span> <span class="n">source_line</span><span class="o">=</span><span class="mi">484</span><span class="p">}</span>
  <span class="n">ROOT</span> <span class="o">%</span><span class="n">multiply</span><span class="mf">.5.1</span> <span class="o">=</span> <span class="n">f16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="n">multiply</span><span class="p">(</span><span class="n">f16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="o">%</span><span class="n">param_0</span><span class="p">,</span> <span class="n">f16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="o">%</span><span class="n">broadcast</span><span class="mf">.8.1</span><span class="p">),</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="n">op_name</span><span class="o">=</span><span class="s2">&quot;pjit(custom_p_loss)/jit(main)/mul&quot;</span> <span class="n">source_file</span><span class="o">=</span><span class="s2">&quot;/opt/jax/docs/Custom_Operation_for_GPUs.py&quot;</span> <span class="n">source_line</span><span class="o">=</span><span class="mi">484</span><span class="p">}</span>
<span class="p">}</span>

<span class="o">%</span><span class="n">region_0</span><span class="mf">.9</span><span class="o">.</span><span class="n">_custom_call_lowering_rule</span> <span class="p">(</span><span class="n">Arg_0</span><span class="mf">.10.0</span><span class="p">:</span> <span class="n">f16</span><span class="p">[],</span> <span class="n">Arg_1</span><span class="mf">.11.0</span><span class="p">:</span> <span class="n">f16</span><span class="p">[])</span> <span class="o">-&gt;</span> <span class="n">f16</span><span class="p">[]</span> <span class="p">{</span>
  <span class="o">%</span><span class="n">Arg_1</span><span class="mf">.11.0</span> <span class="o">=</span> <span class="n">f16</span><span class="p">[]</span> <span class="n">parameter</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
  <span class="o">%</span><span class="n">Arg_0</span><span class="mf">.10.0</span> <span class="o">=</span> <span class="n">f16</span><span class="p">[]</span> <span class="n">parameter</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">ROOT</span> <span class="o">%</span><span class="n">add</span><span class="mf">.2.0</span> <span class="o">=</span> <span class="n">f16</span><span class="p">[]</span> <span class="n">add</span><span class="p">(</span><span class="n">f16</span><span class="p">[]</span> <span class="o">%</span><span class="n">Arg_0</span><span class="mf">.10.0</span><span class="p">,</span> <span class="n">f16</span><span class="p">[]</span> <span class="o">%</span><span class="n">Arg_1</span><span class="mf">.11.0</span><span class="p">),</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="n">op_name</span><span class="o">=</span><span class="s2">&quot;jit(main)/add&quot;</span> <span class="n">source_file</span><span class="o">=</span><span class="s2">&quot;/opt/jax/docs/Custom_Operation_for_GPUs.py&quot;</span> <span class="n">source_line</span><span class="o">=</span><span class="mi">433</span><span class="p">}</span>
<span class="p">}</span>

<span class="n">ENTRY</span> <span class="o">%</span><span class="n">main</span><span class="mf">.23</span><span class="n">_spmd</span> <span class="p">(</span><span class="n">param</span><span class="mf">.2</span><span class="p">:</span> <span class="n">f16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">],</span> <span class="n">param</span><span class="mf">.1.0</span><span class="p">:</span> <span class="n">f16</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="p">(</span><span class="n">f16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">],</span> <span class="n">f16</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">])</span> <span class="p">{</span>
  <span class="o">%</span><span class="n">param</span><span class="mf">.1.0</span> <span class="o">=</span> <span class="n">f16</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="n">parameter</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">sharding</span><span class="o">=</span><span class="p">{</span><span class="n">replicated</span><span class="p">}</span>
  <span class="o">%</span><span class="n">param</span><span class="mf">.2</span> <span class="o">=</span> <span class="n">f16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="n">parameter</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">sharding</span><span class="o">=</span><span class="p">{</span><span class="n">devices</span><span class="o">=</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">&lt;=</span><span class="p">[</span><span class="mi">4</span><span class="p">]}</span>
  <span class="o">%</span><span class="n">custom</span><span class="o">-</span><span class="n">call</span><span class="mf">.3.0</span> <span class="o">=</span> <span class="p">(</span><span class="n">f16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">},</span> <span class="n">f32</span><span class="p">[</span><span class="mi">4</span><span class="p">]{</span><span class="mi">0</span><span class="p">})</span> <span class="n">custom</span><span class="o">-</span><span class="n">call</span><span class="p">(</span><span class="n">f16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="o">%</span><span class="n">param</span><span class="mf">.2</span><span class="p">,</span> <span class="n">f16</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="o">%</span><span class="n">param</span><span class="mf">.1.0</span><span class="p">),</span> <span class="n">custom_call_target</span><span class="o">=</span><span class="s2">&quot;rms_forward_affine_mixed_dtype&quot;</span><span class="p">,</span> <span class="n">operand_layout_constraints</span><span class="o">=</span><span class="p">{</span><span class="n">f16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">},</span> <span class="n">f16</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}},</span> <span class="n">api_version</span><span class="o">=</span><span class="n">API_VERSION_STATUS_RETURNING</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="n">op_name</span><span class="o">=</span><span class="s2">&quot;pjit(custom_p_loss)/jit(main)/custom_partitioning[partition=&lt;function RmsNormFwdClass.partition at 0x7ff99e3980d0&gt; propagate_user_sharding=None infer_sharding_from_operands=&lt;function RmsNormFwdClass.infer_sharding_from_operands at 0x7ff99e398040&gt; decode_shardings=True in_tree=PyTreeDef((*, *)) out_tree=PyTreeDef((*, *)) static_args=[1e-05]]&quot;</span> <span class="n">source_file</span><span class="o">=</span><span class="s2">&quot;/opt/jax/docs/Custom_Operation_for_GPUs.py&quot;</span> <span class="n">source_line</span><span class="o">=</span><span class="mi">440</span><span class="p">},</span> <span class="n">backend_config</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\004\000\000\000\000\000\004\000\361</span><span class="s2">h</span><span class="se">\343\210\265\370\344</span><span class="s2">&gt;</span><span class="se">\001\000\000\000\001\000\000\000\000\000\000\000</span><span class="s2">$V</span><span class="se">\000\000</span><span class="s2">&quot;</span>
  <span class="o">%</span><span class="n">get</span><span class="o">-</span><span class="nb">tuple</span><span class="o">-</span><span class="n">element</span><span class="mf">.14</span> <span class="o">=</span> <span class="n">f16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="n">get</span><span class="o">-</span><span class="nb">tuple</span><span class="o">-</span><span class="n">element</span><span class="p">((</span><span class="n">f16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">},</span> <span class="n">f32</span><span class="p">[</span><span class="mi">4</span><span class="p">]{</span><span class="mi">0</span><span class="p">})</span> <span class="o">%</span><span class="n">custom</span><span class="o">-</span><span class="n">call</span><span class="mf">.3.0</span><span class="p">),</span> <span class="n">index</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="n">op_name</span><span class="o">=</span><span class="s2">&quot;pjit(custom_p_loss)/jit(main)/custom_partitioning[partition=&lt;function RmsNormFwdClass.partition at 0x7ff99e3980d0&gt; propagate_user_sharding=None infer_sharding_from_operands=&lt;function RmsNormFwdClass.infer_sharding_from_operands at 0x7ff99e398040&gt; decode_shardings=True in_tree=PyTreeDef((*, *)) out_tree=PyTreeDef((*, *)) static_args=[1e-05]]&quot;</span> <span class="n">source_file</span><span class="o">=</span><span class="s2">&quot;/opt/jax/docs/Custom_Operation_for_GPUs.py&quot;</span> <span class="n">source_line</span><span class="o">=</span><span class="mi">440</span><span class="p">}</span>
  <span class="o">%</span><span class="n">loop_multiply_fusion</span> <span class="o">=</span> <span class="n">f16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="n">fusion</span><span class="p">(</span><span class="n">f16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="o">%</span><span class="n">get</span><span class="o">-</span><span class="nb">tuple</span><span class="o">-</span><span class="n">element</span><span class="mf">.14</span><span class="p">),</span> <span class="n">kind</span><span class="o">=</span><span class="n">kLoop</span><span class="p">,</span> <span class="n">calls</span><span class="o">=%</span><span class="n">fused_multiply</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="n">op_name</span><span class="o">=</span><span class="s2">&quot;pjit(custom_p_loss)/jit(main)/mul&quot;</span> <span class="n">source_file</span><span class="o">=</span><span class="s2">&quot;/opt/jax/docs/Custom_Operation_for_GPUs.py&quot;</span> <span class="n">source_line</span><span class="o">=</span><span class="mi">484</span><span class="p">}</span>
  <span class="o">%</span><span class="n">get</span><span class="o">-</span><span class="nb">tuple</span><span class="o">-</span><span class="n">element</span><span class="mf">.1.0</span> <span class="o">=</span> <span class="n">f32</span><span class="p">[</span><span class="mi">4</span><span class="p">]{</span><span class="mi">0</span><span class="p">}</span> <span class="n">get</span><span class="o">-</span><span class="nb">tuple</span><span class="o">-</span><span class="n">element</span><span class="p">((</span><span class="n">f16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">},</span> <span class="n">f32</span><span class="p">[</span><span class="mi">4</span><span class="p">]{</span><span class="mi">0</span><span class="p">})</span> <span class="o">%</span><span class="n">custom</span><span class="o">-</span><span class="n">call</span><span class="mf">.3.0</span><span class="p">),</span> <span class="n">index</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="n">op_name</span><span class="o">=</span><span class="s2">&quot;pjit(custom_p_loss)/jit(main)/custom_partitioning[partition=&lt;function RmsNormFwdClass.partition at 0x7ff99e3980d0&gt; propagate_user_sharding=None infer_sharding_from_operands=&lt;function RmsNormFwdClass.infer_sharding_from_operands at 0x7ff99e398040&gt; decode_shardings=True in_tree=PyTreeDef((*, *)) out_tree=PyTreeDef((*, *)) static_args=[1e-05]]&quot;</span> <span class="n">source_file</span><span class="o">=</span><span class="s2">&quot;/opt/jax/docs/Custom_Operation_for_GPUs.py&quot;</span> <span class="n">source_line</span><span class="o">=</span><span class="mi">440</span><span class="p">}</span>
  <span class="o">%</span><span class="n">custom</span><span class="o">-</span><span class="n">call</span><span class="mf">.5.0</span> <span class="o">=</span> <span class="p">(</span><span class="n">f16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">},</span> <span class="n">f16</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">},</span> <span class="n">f32</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span><span class="mi">262144</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">})</span> <span class="n">custom</span><span class="o">-</span><span class="n">call</span><span class="p">(</span><span class="n">f16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="o">%</span><span class="n">loop_multiply_fusion</span><span class="p">,</span> <span class="n">f32</span><span class="p">[</span><span class="mi">4</span><span class="p">]{</span><span class="mi">0</span><span class="p">}</span> <span class="o">%</span><span class="n">get</span><span class="o">-</span><span class="nb">tuple</span><span class="o">-</span><span class="n">element</span><span class="mf">.1.0</span><span class="p">,</span> <span class="n">f16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="o">%</span><span class="n">param</span><span class="mf">.2</span><span class="p">,</span> <span class="n">f16</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="o">%</span><span class="n">param</span><span class="mf">.1.0</span><span class="p">),</span> <span class="n">custom_call_target</span><span class="o">=</span><span class="s2">&quot;rms_backward_affine&quot;</span><span class="p">,</span> <span class="n">operand_layout_constraints</span><span class="o">=</span><span class="p">{</span><span class="n">f16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">},</span> <span class="n">f32</span><span class="p">[</span><span class="mi">4</span><span class="p">]{</span><span class="mi">0</span><span class="p">},</span> <span class="n">f16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">},</span> <span class="n">f16</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}},</span> <span class="n">api_version</span><span class="o">=</span><span class="n">API_VERSION_STATUS_RETURNING</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="n">op_name</span><span class="o">=</span><span class="s2">&quot;pjit(custom_p_loss)/jit(main)/custom_partitioning[partition=&lt;function RmsNormBwdClass.partition at 0x7ff99e3985e0&gt; propagate_user_sharding=None infer_sharding_from_operands=&lt;function RmsNormBwdClass.infer_sharding_from_operands at 0x7ff99e398550&gt; decode_shardings=True in_tree=PyTreeDef((*, *, *, *)) out_tree=PyTreeDef((*, *, *)) static_args=[1e-05]]&quot;</span> <span class="n">source_file</span><span class="o">=</span><span class="s2">&quot;/opt/jax/docs/Custom_Operation_for_GPUs.py&quot;</span> <span class="n">source_line</span><span class="o">=</span><span class="mi">483</span><span class="p">},</span> <span class="n">backend_config</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\004\000\000\000\000\000\004\000\361</span><span class="s2">h</span><span class="se">\343\210\265\370\344</span><span class="s2">&gt;</span><span class="se">\001\000\000\000\001\000\000\000\020\000\000\000</span><span class="s2">$V</span><span class="se">\000\000</span><span class="s2">&quot;</span>
  <span class="o">%</span><span class="n">get</span><span class="o">-</span><span class="nb">tuple</span><span class="o">-</span><span class="n">element</span><span class="mf">.7.0</span> <span class="o">=</span> <span class="n">f16</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="n">get</span><span class="o">-</span><span class="nb">tuple</span><span class="o">-</span><span class="n">element</span><span class="p">((</span><span class="n">f16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">},</span> <span class="n">f16</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">},</span> <span class="n">f32</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span><span class="mi">262144</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">})</span> <span class="o">%</span><span class="n">custom</span><span class="o">-</span><span class="n">call</span><span class="mf">.5.0</span><span class="p">),</span> <span class="n">index</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="n">op_name</span><span class="o">=</span><span class="s2">&quot;pjit(custom_p_loss)/jit(main)/custom_partitioning[partition=&lt;function RmsNormBwdClass.partition at 0x7ff99e3985e0&gt; propagate_user_sharding=None infer_sharding_from_operands=&lt;function RmsNormBwdClass.infer_sharding_from_operands at 0x7ff99e398550&gt; decode_shardings=True in_tree=PyTreeDef((*, *, *, *)) out_tree=PyTreeDef((*, *, *)) static_args=[1e-05]]&quot;</span> <span class="n">source_file</span><span class="o">=</span><span class="s2">&quot;/opt/jax/docs/Custom_Operation_for_GPUs.py&quot;</span> <span class="n">source_line</span><span class="o">=</span><span class="mi">483</span><span class="p">}</span>
  <span class="o">%</span><span class="nb">all</span><span class="o">-</span><span class="n">reduce</span><span class="o">-</span><span class="n">start</span> <span class="o">=</span> <span class="n">f16</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="nb">all</span><span class="o">-</span><span class="n">reduce</span><span class="o">-</span><span class="n">start</span><span class="p">(</span><span class="n">f16</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="o">%</span><span class="n">get</span><span class="o">-</span><span class="nb">tuple</span><span class="o">-</span><span class="n">element</span><span class="mf">.7.0</span><span class="p">),</span> <span class="n">channel_id</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">replica_groups</span><span class="o">=</span><span class="p">{{</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">}},</span> <span class="n">use_global_device_ids</span><span class="o">=</span><span class="n">true</span><span class="p">,</span> <span class="n">to_apply</span><span class="o">=%</span><span class="n">region_0</span><span class="mf">.9</span><span class="o">.</span><span class="n">_custom_call_lowering_rule</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="n">op_name</span><span class="o">=</span><span class="s2">&quot;pjit(custom_p_loss)/jit(main)/custom_partitioning[partition=&lt;function RmsNormBwdClass.partition at 0x7ff99e3985e0&gt; propagate_user_sharding=None infer_sharding_from_operands=&lt;function RmsNormBwdClass.infer_sharding_from_operands at 0x7ff99e398550&gt; decode_shardings=True in_tree=PyTreeDef((*, *, *, *)) out_tree=PyTreeDef((*, *, *)) static_args=[1e-05]]&quot;</span> <span class="n">source_file</span><span class="o">=</span><span class="s2">&quot;/opt/jax/docs/Custom_Operation_for_GPUs.py&quot;</span> <span class="n">source_line</span><span class="o">=</span><span class="mi">483</span><span class="p">},</span> <span class="n">backend_config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;operation_queue_id&quot;</span><span class="p">:</span><span class="s2">&quot;0&quot;</span><span class="p">,</span><span class="s2">&quot;wait_on_operation_queues&quot;</span><span class="p">:[],</span><span class="s2">&quot;collective_backend_config&quot;</span><span class="p">:{</span><span class="s2">&quot;is_sync&quot;</span><span class="p">:</span><span class="n">true</span><span class="p">,</span><span class="s2">&quot;no_parallel_custom_call&quot;</span><span class="p">:</span><span class="n">false</span><span class="p">}}</span>
  <span class="o">%</span><span class="nb">all</span><span class="o">-</span><span class="n">reduce</span><span class="o">-</span><span class="n">done</span> <span class="o">=</span> <span class="n">f16</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="nb">all</span><span class="o">-</span><span class="n">reduce</span><span class="o">-</span><span class="n">done</span><span class="p">(</span><span class="n">f16</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="o">%</span><span class="nb">all</span><span class="o">-</span><span class="n">reduce</span><span class="o">-</span><span class="n">start</span><span class="p">),</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="n">op_name</span><span class="o">=</span><span class="s2">&quot;pjit(custom_p_loss)/jit(main)/custom_partitioning[partition=&lt;function RmsNormBwdClass.partition at 0x7ff99e3985e0&gt; propagate_user_sharding=None infer_sharding_from_operands=&lt;function RmsNormBwdClass.infer_sharding_from_operands at 0x7ff99e398550&gt; decode_shardings=True in_tree=PyTreeDef((*, *, *, *)) out_tree=PyTreeDef((*, *, *)) static_args=[1e-05]]&quot;</span> <span class="n">source_file</span><span class="o">=</span><span class="s2">&quot;/opt/jax/docs/Custom_Operation_for_GPUs.py&quot;</span> <span class="n">source_line</span><span class="o">=</span><span class="mi">483</span><span class="p">}</span>
  <span class="o">%</span><span class="n">get</span><span class="o">-</span><span class="nb">tuple</span><span class="o">-</span><span class="n">element</span><span class="mf">.12.0</span> <span class="o">=</span> <span class="n">f16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="n">get</span><span class="o">-</span><span class="nb">tuple</span><span class="o">-</span><span class="n">element</span><span class="p">((</span><span class="n">f16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">},</span> <span class="n">f16</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">},</span> <span class="n">f32</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span><span class="mi">262144</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">})</span> <span class="o">%</span><span class="n">custom</span><span class="o">-</span><span class="n">call</span><span class="mf">.5.0</span><span class="p">),</span> <span class="n">index</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="n">op_name</span><span class="o">=</span><span class="s2">&quot;pjit(custom_p_loss)/jit(main)/custom_partitioning[partition=&lt;function RmsNormBwdClass.partition at 0x7ff99e3985e0&gt; propagate_user_sharding=None infer_sharding_from_operands=&lt;function RmsNormBwdClass.infer_sharding_from_operands at 0x7ff99e398550&gt; decode_shardings=True in_tree=PyTreeDef((*, *, *, *)) out_tree=PyTreeDef((*, *, *)) static_args=[1e-05]]&quot;</span> <span class="n">source_file</span><span class="o">=</span><span class="s2">&quot;/opt/jax/docs/Custom_Operation_for_GPUs.py&quot;</span> <span class="n">source_line</span><span class="o">=</span><span class="mi">483</span><span class="p">}</span>
  <span class="n">ROOT</span> <span class="o">%</span><span class="nb">tuple</span><span class="mf">.1.0</span> <span class="o">=</span> <span class="p">(</span><span class="n">f16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">},</span> <span class="n">f16</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">})</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">f16</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="o">%</span><span class="n">get</span><span class="o">-</span><span class="nb">tuple</span><span class="o">-</span><span class="n">element</span><span class="mf">.12.0</span><span class="p">,</span> <span class="n">f16</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span> <span class="o">%</span><span class="nb">all</span><span class="o">-</span><span class="n">reduce</span><span class="o">-</span><span class="n">done</span><span class="p">)</span>
<span class="p">}</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kc">True</span>
<span class="kc">True</span>
</pre></div>
</div>
<p>Now there are no all-gathers in the HLO, sharding is respected and only gradients are accumulated via an all-reduce.</p>
<section id="let-s-put-it-together">
<h2>Letâ€™s put it together<a class="headerlink" href="#let-s-put-it-together" title="Link to this heading">#</a></h2>
<p>The complete definition of the primitives using custom_partitioning can be found in <a class="reference download internal" download="" href="_downloads/e70836516c331a83dfcd185dd73435fe/Custom_Operation_for_GPUs.py"><span class="xref download myst">Custom_Operation_for_GPUs.py</span></a> and the corresponding C++ code the defines python bindings in addition to the kernel implementations can be found below:</p>
<section id="gpu-ops-code-listing">
<h3><code class="docutils literal notranslate"><span class="pre">gpu_ops</span></code> code listing<a class="headerlink" href="#gpu-ops-code-listing" title="Link to this heading">#</a></h3>
<p><a class="reference download internal" download="" href="_downloads/6887b43f6c37e251530df2326372488f/kernel_helpers.h"><span class="xref download myst">gpu_ops/kernel_helpers.h</span></a> <br />
<a class="reference download internal" download="" href="_downloads/4c733b237b6d815ca720717043d5ea90/kernels.h"><span class="xref download myst">gpu_ops/kernels.h</span></a> <br />
<a class="reference download internal" download="" href="_downloads/3593cade345df30d769ec24ec17d558a/pybind11_kernel_helpers.h"><span class="xref download myst">gpu_ops/pybind11_kernel_helpers.h</span></a> <br />
<a class="reference download internal" download="" href="_downloads/49efef05ecedef4ac051b10b37e63882/gpu_ops.cpp"><span class="xref download myst">gpu_ops/gpu_ops.cpp</span></a> <br />
<a class="reference download internal" download="" href="_downloads/dbc13c87d0b5bf20fc6bf05c26a42b1e/rms_norm_kernels.cu"><span class="xref download myst">gpu_ops/rms_norm_kernels.cu</span></a></p>
</section>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="notebooks/Writing_custom_interpreters_in_Jax.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Writing custom Jaxpr interpreters in JAX</p>
      </div>
    </a>
    <a class="right-next"
       href="notebooks/convolutions.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Generalized Convolutions in JAX</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Custom operations for GPUs with C++ and CUDA</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rms-normalization">RMS normalization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#high-level-steps">High-level steps</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#c-code">C code</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#build-gpu-ops-extension-module">Build <code class="docutils literal notranslate"><span class="pre">gpu_ops</span></code> extension module</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#add-rms-normalization-to-jax-as-custom-call">Add RMS normalization to JAX as custom call</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-primitives">Create primitives</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lowering-to-mlir-custom-call">Lowering to MLIR custom call</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#let-s-test-it">Letâ€™s test it</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#test-forward-function">Test forward function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#abstract-evaluation">Abstract evaluation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#let-s-test-it-again">Letâ€™s test it again</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#test-the-forward-function">Test the forward function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#test-the-backward-function">Test the backward function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#differentiation-rule">Differentiation rule</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#let-s-test-it-on-multiple-devices">Letâ€™s test it on multiple devices</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Test the forward function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shard-the-forward-function-with-custom-partitioning">Shard the forward function with custom_partitioning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shard-the-backward-function-with-custom-partitioning">Shard the backward function with custom_partitioning</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#check-for-correctness">Check for correctness</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#let-s-put-it-together">Letâ€™s put it together</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpu-ops-code-listing"><code class="docutils literal notranslate"><span class="pre">gpu_ops</span></code> code listing</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The JAX authors
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      Â© Copyright 2024, The JAX Authors. NumPy and SciPy documentation are copyright the respective authors..
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>