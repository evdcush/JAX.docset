
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>jax._src.api &#8212; JAX  documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../../../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
    <link rel="stylesheet" type="text/css" href="../../../_static/style.css?v=02ed413a" />
    <link rel="stylesheet" href="../../../_static/style.css" type="text/css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../../../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script src="../../../_static/design-tabs.js?v=36754332"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/jax/_src/api';</script>
    <link rel="icon" href="../../../_static/favicon.png"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../../_static/jax_logo_250px.png" class="logo__image only-light" alt="JAX  documentation - Home"/>
    <script>document.write(`<img src="../../../_static/jax_logo_250px.png" class="logo__image only-dark" alt="JAX  documentation - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../installation.html">Installing JAX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/quickstart.html">JAX Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/thinking_in_jax.html">How to Think in JAX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/Common_Gotchas_in_JAX.html">ðŸ”ª JAX - The Sharp Bits ðŸ”ª</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq.html">JAX Frequently Asked Questions (FAQ)</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../jax-101/index.html">Tutorial: JAX 101</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../jax-101/01-jax-basics.html">JAX As Accelerated NumPy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../jax-101/02-jitting.html">Just In Time Compilation with JAX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../jax-101/03-vectorization.html">Automatic Vectorization in JAX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../jax-101/04-advanced-autodiff.html">Advanced Automatic Differentiation in JAX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../jax-101/05-random-numbers.html">Pseudo Random Numbers in JAX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../jax-101/05.1-pytrees.html">Working with Pytrees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../jax-101/06-parallelism.html">Parallel Evaluation in JAX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../jax-101/07-state.html">Stateful Computations in JAX</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Further Resources</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../user_guides.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../profiling.html">Profiling JAX programs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../device_memory_profiling.html">Device Memory Profiling</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../debugging/index.html">Runtime value debugging in JAX</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../debugging/print_breakpoint.html"><code class="docutils literal notranslate"><span class="pre">jax.debug.print</span></code> and <code class="docutils literal notranslate"><span class="pre">jax.debug.breakpoint</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../debugging/checkify_guide.html">The <code class="docutils literal notranslate"><span class="pre">checkify</span></code> transformation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../debugging/flags.html">JAX debugging flags</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../gpu_performance_tips.html">GPU performance tips</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../persistent_compilation_cache.html">Persistent Compilation Cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../jaxpr.html">Understanding Jaxprs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notebooks/external_callbacks.html">External Callbacks in JAX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../type_promotion.html">Type promotion semantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pytrees.html">Pytrees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../aot.html">Ahead-of-time lowering and compilation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../errors.html">JAX Errors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../transfer_guard.html">Transfer guard</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../pallas/index.html">Pallas: a JAX kernel language</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../pallas/design.html">Pallas Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../pallas/quickstart.html">Pallas Quickstart</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../pallas/tpu/index.html">Pallas TPU</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../pallas/tpu/details.html">Writing TPU kernels with Pallas</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../pallas/tpu/pipelining.html">Pipelining and <code class="docutils literal notranslate"><span class="pre">BlockSpec</span></code>s</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../advanced_guide.html">Advanced Tutorials</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../notebooks/neural_network_with_tfds_data.html">Training a Simple Neural Network, with tensorflow/datasets Data Loading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notebooks/Neural_Network_and_Data_Loading.html">Training a Simple Neural Network, with PyTorch Data Loading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notebooks/vmapped_log_probs.html">Autobatching for Bayesian Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../multi_process.html">Using JAX in multi-host and multi-process environments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notebooks/Distributed_arrays_and_automatic_parallelization.html">Distributed arrays and automatic parallelization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notebooks/shard_map.html">SPMD multi-device parallelism with <code class="docutils literal notranslate"><span class="pre">shard_map</span></code></a></li>



<li class="toctree-l2"><a class="reference internal" href="../../../notebooks/xmap_tutorial.html">Named axes and easy-to-revise parallelism with <code class="docutils literal notranslate"><span class="pre">xmap</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notebooks/autodiff_cookbook.html">The Autodiff Cookbook</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notebooks/Custom_derivative_rules_for_Python_code.html">Custom derivative rules for JAX-transformable Python functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notebooks/autodiff_remat.html">Control autodiffâ€™s saved values with <code class="docutils literal notranslate"><span class="pre">jax.checkpoint</span></code> (aka <code class="docutils literal notranslate"><span class="pre">jax.remat</span></code>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notebooks/How_JAX_primitives_work.html">How JAX primitives work</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notebooks/Writing_custom_interpreters_in_Jax.html">Writing custom Jaxpr interpreters in JAX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../Custom_Operation_for_GPUs.html">Custom operations for GPUs with C++ and CUDA</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../../notebooks/convolutions.html">Generalized Convolutions in JAX</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../contributor_guide.html">Developer Documentation</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html">Contributing to JAX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../developer.html">Building from source</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../jax_internal_api.html">Internal APIs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../autodidax.html">Autodidax: JAX core from scratch</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../jep/index.html">JAX Enhancement Proposals (JEPs)</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../jep/263-prng.html">263: JAX PRNG Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../jep/2026-custom-derivatives.html">2026: Custom JVP/VJP rules for JAX-transformable functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../jep/4008-custom-vjp-update.html">4008: Custom VJP and `nondiff_argnums` update</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../jep/4410-omnistaging.html">4410: Omnistaging</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../jep/9263-typed-keys.html">9263: Typed keys &amp; pluggable RNGs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../jep/9407-type-promotion.html">9407: Design of Type Promotion Semantics for JAX</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../jep/9419-jax-versioning.html">9419: Jax and Jaxlib versioning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../jep/10657-sequencing-effects.html">10657: Sequencing side-effects in JAX</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../jep/11830-new-remat-checkpoint.html">11830: `jax.remat` / `jax.checkpoint` new implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../jep/12049-type-annotations.html">12049: Type Annotation Roadmap for JAX</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../jep/14273-shard-map.html">14273: `shard_map` (`shmap`) for simple per-device code</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../jep/15856-jex.html">15856: `jax.extend`, an extensions module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../jep/17111-shmap-transpose.html">17111: Efficient transposition of `shard_map` (and other maps)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../jep/18137-numpy-scipy-scope.html">18137: Scope of JAX NumPy &amp; SciPy Wrappers</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../investigating_a_regression.html">Investigating a regression</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../building_on_jax.html">Building on JAX</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../notes.html">Notes</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api_compatibility.html">API compatibility</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../deprecation.html">Python and NumPy version support policy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../jax_array_migration.html">jax.Array migration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../async_dispatch.html">Asynchronous dispatch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../concurrency.html">Concurrency</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../gpu_memory_allocation.html">GPU memory allocation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../rank_promotion_warning.html">Rank promotion warning</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../jax.html">Public API: jax package</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../jax.numpy.html"><code class="docutils literal notranslate"><span class="pre">jax.numpy</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../_autosummary/jax.numpy.fft.fft.html">jax.numpy.fft.fft</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../_autosummary/jax.numpy.fft.fft2.html">jax.numpy.fft.fft2</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../_autosummary/jax.numpy.fft.fftfreq.html">jax.numpy.fft.fftfreq</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../_autosummary/jax.numpy.fft.fftn.html">jax.numpy.fft.fftn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../_autosummary/jax.numpy.fft.fftshift.html">jax.numpy.fft.fftshift</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../_autosummary/jax.numpy.fft.hfft.html">jax.numpy.fft.hfft</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../_autosummary/jax.numpy.fft.ifft.html">jax.numpy.fft.ifft</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../_autosummary/jax.numpy.fft.ifft2.html">jax.numpy.fft.ifft2</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../_autosummary/jax.numpy.fft.ifftn.html">jax.numpy.fft.ifftn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../_autosummary/jax.numpy.fft.ifftshift.html">jax.numpy.fft.ifftshift</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../_autosummary/jax.numpy.fft.ihfft.html">jax.numpy.fft.ihfft</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../_autosummary/jax.numpy.fft.irfft.html">jax.numpy.fft.irfft</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../_autosummary/jax.numpy.fft.irfft2.html">jax.numpy.fft.irfft2</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../_autosummary/jax.numpy.fft.irfftn.html">jax.numpy.fft.irfftn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../_autosummary/jax.numpy.fft.rfft.html">jax.numpy.fft.rfft</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../_autosummary/jax.numpy.fft.rfft2.html">jax.numpy.fft.rfft2</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../_autosummary/jax.numpy.fft.rfftfreq.html">jax.numpy.fft.rfftfreq</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../_autosummary/jax.numpy.fft.rfftn.html">jax.numpy.fft.rfftn</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../jax.scipy.html"><code class="docutils literal notranslate"><span class="pre">jax.scipy</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../_autosummary/jax.scipy.stats.bernoulli.logpmf.html">jax.scipy.stats.bernoulli.logpmf</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../_autosummary/jax.scipy.stats.bernoulli.pmf.html">jax.scipy.stats.bernoulli.pmf</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../_autosummary/jax.scipy.stats.bernoulli.cdf.html">jax.scipy.stats.bernoulli.cdf</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../_autosummary/jax.scipy.stats.bernoulli.ppf.html">jax.scipy.stats.bernoulli.ppf</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../jax.lax.html"><code class="docutils literal notranslate"><span class="pre">jax.lax</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../jax.random.html"><code class="docutils literal notranslate"><span class="pre">jax.random</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../jax.sharding.html"><code class="docutils literal notranslate"><span class="pre">jax.sharding</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../jax.debug.html"><code class="docutils literal notranslate"><span class="pre">jax.debug</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../jax.dlpack.html"><code class="docutils literal notranslate"><span class="pre">jax.dlpack</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../jax.distributed.html"><code class="docutils literal notranslate"><span class="pre">jax.distributed</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../jax.dtypes.html"><code class="docutils literal notranslate"><span class="pre">jax.dtypes</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../jax.flatten_util.html"><code class="docutils literal notranslate"><span class="pre">jax.flatten_util</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../jax.image.html"><code class="docutils literal notranslate"><span class="pre">jax.image</span></code> module</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../jax.nn.html"><code class="docutils literal notranslate"><span class="pre">jax.nn</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../jax.nn.initializers.html"><code class="docutils literal notranslate"><span class="pre">jax.nn.initializers</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../jax.ops.html"><code class="docutils literal notranslate"><span class="pre">jax.ops</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../jax.profiler.html"><code class="docutils literal notranslate"><span class="pre">jax.profiler</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../jax.stages.html"><code class="docutils literal notranslate"><span class="pre">jax.stages</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../jax.tree.html"><code class="docutils literal notranslate"><span class="pre">jax.tree</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../jax.tree_util.html"><code class="docutils literal notranslate"><span class="pre">jax.tree_util</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../jax.typing.html"><code class="docutils literal notranslate"><span class="pre">jax.typing</span></code> module</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../jax.extend.html"><code class="docutils literal notranslate"><span class="pre">jax.extend</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../jax.extend.linear_util.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.linear_util</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../jax.extend.mlir.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.mlir</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../jax.extend.random.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.random</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../jax.example_libraries.html"><code class="docutils literal notranslate"><span class="pre">jax.example_libraries</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../jax.example_libraries.optimizers.html"><code class="docutils literal notranslate"><span class="pre">jax.example_libraries.optimizers</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../jax.example_libraries.stax.html"><code class="docutils literal notranslate"><span class="pre">jax.example_libraries.stax</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../jax.experimental.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../jax.experimental.array_api.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.array_api</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../jax.experimental.checkify.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.checkify</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../jax.experimental.host_callback.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.host_callback</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../jax.experimental.maps.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.maps</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../jax.experimental.pjit.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.pjit</span></code> module</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../jax.experimental.sparse.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.sparse</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/jax.experimental.sparse.BCOO.html">jax.experimental.sparse.BCOO</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/jax.experimental.sparse.bcoo_broadcast_in_dim.html">jax.experimental.sparse.bcoo_broadcast_in_dim</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/jax.experimental.sparse.bcoo_concatenate.html">jax.experimental.sparse.bcoo_concatenate</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/jax.experimental.sparse.bcoo_dot_general.html">jax.experimental.sparse.bcoo_dot_general</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/jax.experimental.sparse.bcoo_dot_general_sampled.html">jax.experimental.sparse.bcoo_dot_general_sampled</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/jax.experimental.sparse.bcoo_dynamic_slice.html">jax.experimental.sparse.bcoo_dynamic_slice</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/jax.experimental.sparse.bcoo_extract.html">jax.experimental.sparse.bcoo_extract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/jax.experimental.sparse.bcoo_fromdense.html">jax.experimental.sparse.bcoo_fromdense</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/jax.experimental.sparse.bcoo_gather.html">jax.experimental.sparse.bcoo_gather</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/jax.experimental.sparse.bcoo_multiply_dense.html">jax.experimental.sparse.bcoo_multiply_dense</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/jax.experimental.sparse.bcoo_multiply_sparse.html">jax.experimental.sparse.bcoo_multiply_sparse</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/jax.experimental.sparse.bcoo_update_layout.html">jax.experimental.sparse.bcoo_update_layout</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/jax.experimental.sparse.bcoo_reduce_sum.html">jax.experimental.sparse.bcoo_reduce_sum</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/jax.experimental.sparse.bcoo_reshape.html">jax.experimental.sparse.bcoo_reshape</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/jax.experimental.sparse.bcoo_slice.html">jax.experimental.sparse.bcoo_slice</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/jax.experimental.sparse.bcoo_sort_indices.html">jax.experimental.sparse.bcoo_sort_indices</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/jax.experimental.sparse.bcoo_squeeze.html">jax.experimental.sparse.bcoo_squeeze</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/jax.experimental.sparse.bcoo_sum_duplicates.html">jax.experimental.sparse.bcoo_sum_duplicates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/jax.experimental.sparse.bcoo_todense.html">jax.experimental.sparse.bcoo_todense</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/jax.experimental.sparse.bcoo_transpose.html">jax.experimental.sparse.bcoo_transpose</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../jax.experimental.jet.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.jet</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../jax.experimental.custom_partitioning.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.custom_partitioning</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../jax.experimental.multihost_utils.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.multihost_utils</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../jax.experimental.compilation_cache.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.compilation_cache</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../jax.experimental.key_reuse.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.key_reuse</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../jax.experimental.mesh_utils.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.mesh_utils</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../jax.lib.html"><code class="docutils literal notranslate"><span class="pre">jax.lib</span></code> module</a></li>
</ul>
</li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../changelog.html">Change log</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../glossary.html">JAX Glossary of Terms</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/google/jax" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>

</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1></h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for jax._src.api</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2018 The JAX Authors.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>

<span class="sd">&quot;&quot;&quot;JAX user-facing transformations and utilities.</span>

<span class="sd">The transformations here mostly wrap internal transformations, providing</span>
<span class="sd">convenience flags to control behavior and handling Python containers of</span>
<span class="sd">arguments and outputs. The Python containers handled are pytrees (see</span>
<span class="sd">tree_util.py), which include nested tuples/lists/dicts, where the leaves are</span>
<span class="sd">arrays.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">from</span> <span class="nn">collections.abc</span> <span class="kn">import</span> <span class="n">Generator</span><span class="p">,</span> <span class="n">Hashable</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">,</span> <span class="n">Sequence</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">import</span> <span class="nn">inspect</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">typing</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="p">(</span><span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Literal</span><span class="p">,</span> <span class="n">NamedTuple</span><span class="p">,</span> <span class="n">TypeVar</span><span class="p">,</span> <span class="n">overload</span><span class="p">,</span>
                    <span class="n">cast</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">weakref</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">contextmanager</span><span class="p">,</span> <span class="n">ExitStack</span>

<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">linear_util</span> <span class="k">as</span> <span class="n">lu</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">stages</span>
<span class="kn">from</span> <span class="nn">jax._src.tree_util</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">tree_map</span><span class="p">,</span> <span class="n">tree_flatten</span><span class="p">,</span> <span class="n">tree_unflatten</span><span class="p">,</span> <span class="n">tree_structure</span><span class="p">,</span> <span class="n">tree_transpose</span><span class="p">,</span>
    <span class="n">tree_leaves</span><span class="p">,</span> <span class="n">Partial</span><span class="p">,</span> <span class="n">PyTreeDef</span><span class="p">,</span> <span class="n">all_leaves</span><span class="p">,</span> <span class="n">keystr</span><span class="p">,</span> <span class="n">broadcast_prefix</span><span class="p">,</span>
    <span class="n">prefix_errors</span><span class="p">,</span> <span class="n">generate_key_paths</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">api_util</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">config</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">core</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">dispatch</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">effects</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">array</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">basearray</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">dtypes</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">sharding_impls</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">sharding_specs</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">source_info_util</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">traceback_util</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">pjit</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">xla_bridge</span> <span class="k">as</span> <span class="n">xb</span>
<span class="kn">from</span> <span class="nn">jax._src.core</span> <span class="kn">import</span> <span class="n">eval_jaxpr</span><span class="p">,</span> <span class="n">ShapedArray</span><span class="p">,</span> <span class="n">ConcreteArray</span>
<span class="kn">from</span> <span class="nn">jax._src.api_util</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">flatten_fun</span><span class="p">,</span> <span class="n">flatten_fun_nokwargs</span><span class="p">,</span> <span class="n">flatten_fun_nokwargs2</span><span class="p">,</span> <span class="n">argnums_partial</span><span class="p">,</span>
    <span class="n">argnums_partial_except</span><span class="p">,</span> <span class="n">flatten_axes</span><span class="p">,</span> <span class="n">donation_vector</span><span class="p">,</span>
    <span class="n">rebase_donate_argnums</span><span class="p">,</span> <span class="n">_ensure_index</span><span class="p">,</span> <span class="n">_ensure_index_tuple</span><span class="p">,</span>
    <span class="n">shaped_abstractify</span><span class="p">,</span> <span class="n">apply_flat_fun_nokwargs</span><span class="p">,</span> <span class="n">check_callable</span><span class="p">,</span> <span class="n">debug_info</span><span class="p">,</span>
    <span class="n">result_paths</span><span class="p">,</span> <span class="n">flat_out_axes</span><span class="p">,</span> <span class="n">debug_info_final</span><span class="p">,</span> <span class="n">fun_sourceinfo</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">jax._src.lax</span> <span class="kn">import</span> <span class="n">lax</span> <span class="k">as</span> <span class="n">lax_internal</span>
<span class="kn">from</span> <span class="nn">jax._src.lib</span> <span class="kn">import</span> <span class="n">jax_jit</span>
<span class="kn">from</span> <span class="nn">jax._src.lib</span> <span class="kn">import</span> <span class="n">xla_client</span> <span class="k">as</span> <span class="n">xc</span>
<span class="kn">from</span> <span class="nn">jax._src.lib</span> <span class="kn">import</span> <span class="n">xla_extension_version</span>
<span class="kn">from</span> <span class="nn">jax._src.lib</span> <span class="kn">import</span> <span class="n">pmap_lib</span>
<span class="kn">from</span> <span class="nn">jax._src.sharding</span> <span class="kn">import</span> <span class="n">Sharding</span>
<span class="kn">from</span> <span class="nn">jax._src.sharding_impls</span> <span class="kn">import</span> <span class="p">(</span><span class="n">PmapSharding</span><span class="p">,</span> <span class="n">TransferToMemoryKind</span><span class="p">,</span>
                                     <span class="n">XLACompatibleSharding</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">jax._src.layout</span> <span class="kn">import</span> <span class="n">Layout</span><span class="p">,</span> <span class="n">AutoLayout</span>
<span class="kn">from</span> <span class="nn">jax._src.traceback_util</span> <span class="kn">import</span> <span class="n">api_boundary</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">tree_util</span>
<span class="kn">from</span> <span class="nn">jax._src.util</span> <span class="kn">import</span> <span class="n">unzip2</span><span class="p">,</span> <span class="n">safe_map</span><span class="p">,</span> <span class="n">safe_zip</span><span class="p">,</span> <span class="n">wrap_name</span><span class="p">,</span> <span class="n">wraps</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">util</span>

<span class="kn">from</span> <span class="nn">jax._src.interpreters</span> <span class="kn">import</span> <span class="n">ad</span>
<span class="kn">from</span> <span class="nn">jax._src.interpreters</span> <span class="kn">import</span> <span class="n">batching</span>
<span class="kn">from</span> <span class="nn">jax._src.interpreters</span> <span class="kn">import</span> <span class="n">mlir</span>
<span class="kn">from</span> <span class="nn">jax._src.interpreters</span> <span class="kn">import</span> <span class="n">partial_eval</span> <span class="k">as</span> <span class="n">pe</span>
<span class="kn">from</span> <span class="nn">jax._src.interpreters</span> <span class="kn">import</span> <span class="n">pxla</span>
<span class="kn">from</span> <span class="nn">jax._src.interpreters</span> <span class="kn">import</span> <span class="n">xla</span>


<span class="n">traceback_util</span><span class="o">.</span><span class="n">register_exclusion</span><span class="p">(</span><span class="vm">__file__</span><span class="p">)</span>

<span class="n">_dtype</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">dtypes</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">canonicalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">AxisName</span> <span class="o">=</span> <span class="n">Hashable</span>

<span class="n">Device</span> <span class="o">=</span> <span class="n">xc</span><span class="o">.</span><span class="n">Device</span>

<span class="c1"># These TypeVars are used below to express the fact that function types</span>
<span class="c1"># (i.e. call signatures) are invariant under the vmap transformation.</span>
<span class="n">F</span> <span class="o">=</span> <span class="n">TypeVar</span><span class="p">(</span><span class="s2">&quot;F&quot;</span><span class="p">,</span> <span class="n">bound</span><span class="o">=</span><span class="n">Callable</span><span class="p">)</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">TypeVar</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">)</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">TypeVar</span><span class="p">(</span><span class="s2">&quot;U&quot;</span><span class="p">)</span>

<span class="nb">map</span><span class="p">,</span> <span class="n">unsafe_map</span> <span class="o">=</span> <span class="n">safe_map</span><span class="p">,</span> <span class="nb">map</span>
<span class="nb">zip</span><span class="p">,</span> <span class="n">unsafe_zip</span> <span class="o">=</span> <span class="n">safe_zip</span><span class="p">,</span> <span class="nb">zip</span>


<span class="k">def</span> <span class="nf">_nan_check_posthook</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Hook function called by the C++ jit/pmap to perform NaN checking.&quot;&quot;&quot;</span>
  <span class="n">buffers</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">leaf</span> <span class="ow">in</span> <span class="n">tree_leaves</span><span class="p">(</span><span class="n">output</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">leaf</span><span class="p">,</span> <span class="s2">&quot;addressable_shards&quot;</span><span class="p">):</span>
      <span class="n">buffers</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">shard</span><span class="o">.</span><span class="n">data</span> <span class="k">for</span> <span class="n">shard</span> <span class="ow">in</span> <span class="n">leaf</span><span class="o">.</span><span class="n">addressable_shards</span><span class="p">])</span>

  <span class="k">try</span><span class="p">:</span>
    <span class="n">dispatch</span><span class="o">.</span><span class="n">check_special</span><span class="p">(</span><span class="n">pjit</span><span class="o">.</span><span class="n">pjit_p</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">buffers</span><span class="p">)</span>
  <span class="k">except</span> <span class="ne">FloatingPointError</span><span class="p">:</span>
    <span class="c1"># compiled_fun can only raise in this case</span>
    <span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">debug_nans</span><span class="o">.</span><span class="n">value</span> <span class="ow">or</span> <span class="n">config</span><span class="o">.</span><span class="n">debug_infs</span><span class="o">.</span><span class="n">value</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Invalid nan value encountered in the output of a C++-jit/pmap &quot;</span>
          <span class="s2">&quot;function. Calling the de-optimized version.&quot;</span><span class="p">)</span>
    <span class="n">fun</span><span class="o">.</span><span class="n">_cache_miss</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># probably won&#39;t return</span>

<span class="k">def</span> <span class="nf">_update_debug_special_global</span><span class="p">(</span><span class="n">_</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">_read</span><span class="p">(</span><span class="s2">&quot;jax_debug_nans&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">config</span><span class="o">.</span><span class="n">_read</span><span class="p">(</span><span class="s2">&quot;jax_debug_infs&quot;</span><span class="p">):</span>
    <span class="n">jax_jit</span><span class="o">.</span><span class="n">global_state</span><span class="p">()</span><span class="o">.</span><span class="n">post_hook</span> <span class="o">=</span> <span class="n">_nan_check_posthook</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">jax_jit</span><span class="o">.</span><span class="n">global_state</span><span class="p">()</span><span class="o">.</span><span class="n">post_hook</span> <span class="o">=</span> <span class="kc">None</span>

<span class="k">def</span> <span class="nf">_update_debug_special_thread_local</span><span class="p">(</span><span class="n">_</span><span class="p">):</span>
  <span class="k">if</span> <span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">_thread_local_state</span><span class="p">,</span> <span class="s2">&quot;jax_debug_nans&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="ow">or</span>
      <span class="nb">getattr</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">_thread_local_state</span><span class="p">,</span> <span class="s2">&quot;jax_debug_infs&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)):</span>
    <span class="n">jax_jit</span><span class="o">.</span><span class="n">thread_local_state</span><span class="p">()</span><span class="o">.</span><span class="n">post_hook</span> <span class="o">=</span> <span class="n">_nan_check_posthook</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">jax_jit</span><span class="o">.</span><span class="n">thread_local_state</span><span class="p">()</span><span class="o">.</span><span class="n">post_hook</span> <span class="o">=</span> <span class="kc">None</span>

<span class="n">config</span><span class="o">.</span><span class="n">debug_nans</span><span class="o">.</span><span class="n">_add_hooks</span><span class="p">(</span><span class="n">_update_debug_special_global</span><span class="p">,</span>
                             <span class="n">_update_debug_special_thread_local</span><span class="p">)</span>
<span class="n">config</span><span class="o">.</span><span class="n">debug_infs</span><span class="o">.</span><span class="n">_add_hooks</span><span class="p">(</span><span class="n">_update_debug_special_global</span><span class="p">,</span>
                             <span class="n">_update_debug_special_thread_local</span><span class="p">)</span>


<span class="n">float0</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float0</span>


<div class="viewcode-block" id="jit">
<a class="viewcode-back" href="../../../_autosummary/jax.jit.html#jax.jit">[docs]</a>
<span class="k">def</span> <span class="nf">jit</span><span class="p">(</span>
  <span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
  <span class="n">in_shardings</span><span class="o">=</span><span class="n">sharding_impls</span><span class="o">.</span><span class="n">UNSPECIFIED</span><span class="p">,</span>
  <span class="n">out_shardings</span><span class="o">=</span><span class="n">sharding_impls</span><span class="o">.</span><span class="n">UNSPECIFIED</span><span class="p">,</span>
  <span class="n">static_argnums</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
  <span class="n">static_argnames</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
  <span class="n">donate_argnums</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
  <span class="n">donate_argnames</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
  <span class="n">keep_unused</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
  <span class="n">device</span><span class="p">:</span> <span class="n">xc</span><span class="o">.</span><span class="n">Device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
  <span class="n">backend</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
  <span class="n">inline</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
  <span class="n">abstracted_axes</span><span class="p">:</span> <span class="n">Any</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pjit</span><span class="o">.</span><span class="n">JitWrapped</span><span class="p">:</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Sets up ``fun`` for just-in-time compilation with XLA.</span>

<span class="sd">  Args:</span>
<span class="sd">    fun: Function to be jitted. ``fun`` should be a pure function, as</span>
<span class="sd">      side-effects may only be executed once.</span>

<span class="sd">      The arguments and return value of ``fun`` should be arrays,</span>
<span class="sd">      scalars, or (nested) standard Python containers (tuple/list/dict) thereof.</span>
<span class="sd">      Positional arguments indicated by ``static_argnums`` can be anything at</span>
<span class="sd">      all, provided they are hashable and have an equality operation defined.</span>
<span class="sd">      Static arguments are included as part of a compilation cache key, which is</span>
<span class="sd">      why hash and equality operators must be defined.</span>

<span class="sd">      JAX keeps a weak reference to ``fun`` for use as a compilation cache key,</span>
<span class="sd">      so the object ``fun`` must be weakly-referenceable. Most :class:`Callable`</span>
<span class="sd">      objects will already satisfy this requirement.</span>
<span class="sd">    in_shardings: Pytree of structure matching that of arguments to ``fun``,</span>
<span class="sd">      with all actual arguments replaced by resource assignment specifications.</span>
<span class="sd">      It is also valid to specify a pytree prefix (e.g. one value in place of a</span>
<span class="sd">      whole subtree), in which case the leaves get broadcast to all values in</span>
<span class="sd">      that subtree.</span>

<span class="sd">      The ``in_shardings`` argument is optional. JAX will infer the shardings</span>
<span class="sd">      from the input :py:class:`jax.Array`&#39;s and defaults to replicating the input</span>
<span class="sd">      if the sharding cannot be inferred.</span>

<span class="sd">      The valid resource assignment specifications are:</span>
<span class="sd">        - :py:class:`XLACompatibleSharding`, which will decide how the value</span>
<span class="sd">            will be partitioned. With this, using a mesh context manager is not</span>
<span class="sd">            required.</span>
<span class="sd">        - :py:obj:`None`, will give JAX the freedom to choose whatever sharding</span>
<span class="sd">          it wants.</span>
<span class="sd">          For in_shardings, JAX will mark is as replicated but this behavior</span>
<span class="sd">          can change in the future.</span>
<span class="sd">          For out_shardings, we will rely on the XLA GSPMD partitioner to</span>
<span class="sd">          determine the output shardings.</span>

<span class="sd">      The size of every dimension has to be a multiple of the total number of</span>
<span class="sd">      resources assigned to it. This is similar to pjit&#39;s in_shardings.</span>
<span class="sd">    out_shardings: Like ``in_shardings``, but specifies resource</span>
<span class="sd">      assignment for function outputs. This is similar to pjit&#39;s</span>
<span class="sd">      out_shardings.</span>

<span class="sd">      The ``out_shardings`` argument is optional. If not specified, :py:func:`jax.jit`</span>
<span class="sd">      will use GSPMD&#39;s sharding propagation to figure out what the sharding of the</span>
<span class="sd">      output(s) should be.</span>
<span class="sd">    static_argnums: An optional int or collection of ints that specify which</span>
<span class="sd">      positional arguments to treat as static (compile-time constant).</span>
<span class="sd">      Operations that only depend on static arguments will be constant-folded in</span>
<span class="sd">      Python (during tracing), and so the corresponding argument values can be</span>
<span class="sd">      any Python object.</span>

<span class="sd">      Static arguments should be hashable, meaning both ``__hash__`` and</span>
<span class="sd">      ``__eq__`` are implemented, and immutable. Calling the jitted function</span>
<span class="sd">      with different values for these constants will trigger recompilation.</span>
<span class="sd">      Arguments that are not arrays or containers thereof must be marked as</span>
<span class="sd">      static.</span>

<span class="sd">      If neither ``static_argnums`` nor ``static_argnames`` is provided, no</span>
<span class="sd">      arguments are treated as static. If ``static_argnums`` is not provided but</span>
<span class="sd">      ``static_argnames`` is, or vice versa, JAX uses</span>
<span class="sd">      :code:`inspect.signature(fun)` to find any positional arguments that</span>
<span class="sd">      correspond to ``static_argnames``</span>
<span class="sd">      (or vice versa). If both ``static_argnums`` and ``static_argnames`` are</span>
<span class="sd">      provided, ``inspect.signature`` is not used, and only actual</span>
<span class="sd">      parameters listed in either ``static_argnums`` or ``static_argnames`` will</span>
<span class="sd">      be treated as static.</span>
<span class="sd">    static_argnames: An optional string or collection of strings specifying</span>
<span class="sd">      which named arguments to treat as static (compile-time constant). See the</span>
<span class="sd">      comment on ``static_argnums`` for details. If not</span>
<span class="sd">      provided but ``static_argnums`` is set, the default is based on calling</span>
<span class="sd">      ``inspect.signature(fun)`` to find corresponding named arguments.</span>
<span class="sd">    donate_argnums: Specify which positional argument buffers are &quot;donated&quot; to</span>
<span class="sd">      the computation. It is safe to donate argument buffers if you no longer</span>
<span class="sd">      need them once the computation has finished. In some cases XLA can make</span>
<span class="sd">      use of donated buffers to reduce the amount of memory needed to perform a</span>
<span class="sd">      computation, for example recycling one of your input buffers to store a</span>
<span class="sd">      result. You should not reuse buffers that you donate to a computation, JAX</span>
<span class="sd">      will raise an error if you try to. By default, no argument buffers are</span>
<span class="sd">      donated.</span>

<span class="sd">      If neither ``donate_argnums`` nor ``donate_argnames`` is provided, no</span>
<span class="sd">      arguments are donated. If ``donate_argnums`` is not provided but</span>
<span class="sd">      ``donate_argnames`` is, or vice versa, JAX uses</span>
<span class="sd">      :code:`inspect.signature(fun)` to find any positional arguments that</span>
<span class="sd">      correspond to ``donate_argnames``</span>
<span class="sd">      (or vice versa). If both ``donate_argnums`` and ``donate_argnames`` are</span>
<span class="sd">      provided, ``inspect.signature`` is not used, and only actual</span>
<span class="sd">      parameters listed in either ``donate_argnums`` or ``donate_argnames`` will</span>
<span class="sd">      be donated.</span>

<span class="sd">      For more details on buffer donation see the</span>
<span class="sd">      `FAQ &lt;https://jax.readthedocs.io/en/latest/faq.html#buffer-donation&gt;`_.</span>
<span class="sd">    donate_argnames: An optional string or collection of strings specifying</span>
<span class="sd">      which named arguments are donated to the computation. See the</span>
<span class="sd">      comment on ``donate_argnums`` for details. If not</span>
<span class="sd">      provided but ``donate_argnums`` is set, the default is based on calling</span>
<span class="sd">      ``inspect.signature(fun)`` to find corresponding named arguments.</span>
<span class="sd">    keep_unused: If `False` (the default), arguments that JAX determines to be</span>
<span class="sd">      unused by `fun` *may* be dropped from resulting compiled XLA executables.</span>
<span class="sd">      Such arguments will not be transferred to the device nor provided to the</span>
<span class="sd">      underlying executable. If `True`, unused arguments will not be pruned.</span>
<span class="sd">    device: This is an experimental feature and the API is likely to change.</span>
<span class="sd">      Optional, the Device the jitted function will run on. (Available devices</span>
<span class="sd">      can be retrieved via :py:func:`jax.devices`.) The default is inherited</span>
<span class="sd">      from XLA&#39;s DeviceAssignment logic and is usually to use</span>
<span class="sd">      ``jax.devices()[0]``.</span>
<span class="sd">    backend: This is an experimental feature and the API is likely to change.</span>
<span class="sd">      Optional, a string representing the XLA backend: ``&#39;cpu&#39;``, ``&#39;gpu&#39;``, or</span>
<span class="sd">      ``&#39;tpu&#39;``.</span>
<span class="sd">    inline: Specify whether this function should be inlined into enclosing</span>
<span class="sd">      jaxprs (rather than being represented as an application of the xla_call</span>
<span class="sd">      primitive with its own subjaxpr). Default False.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A wrapped version of ``fun``, set up for just-in-time compilation.</span>

<span class="sd">  Examples:</span>
<span class="sd">    In the following example, ``selu`` can be compiled into a single fused kernel</span>
<span class="sd">    by XLA:</span>

<span class="sd">    &gt;&gt;&gt; import jax</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; @jax.jit</span>
<span class="sd">    ... def selu(x, alpha=1.67, lmbda=1.05):</span>
<span class="sd">    ...   return lmbda * jax.numpy.where(x &gt; 0, x, alpha * jax.numpy.exp(x) - alpha)</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; key = jax.random.key(0)</span>
<span class="sd">    &gt;&gt;&gt; x = jax.random.normal(key, (10,))</span>
<span class="sd">    &gt;&gt;&gt; print(selu(x))  # doctest: +SKIP</span>
<span class="sd">    [-0.54485  0.27744 -0.29255 -0.91421 -0.62452 -0.24748</span>
<span class="sd">    -0.85743 -0.78232  0.76827  0.59566 ]</span>

<span class="sd">    To pass arguments such as ``static_argnames`` when decorating a function, a common</span>
<span class="sd">    pattern is to use :func:`functools.partial`:</span>

<span class="sd">    &gt;&gt;&gt; from functools import partial</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; @partial(jax.jit, static_argnames=[&#39;n&#39;])</span>
<span class="sd">    ... def g(x, n):</span>
<span class="sd">    ...   for i in range(n):</span>
<span class="sd">    ...     x = x ** 2</span>
<span class="sd">    ...   return x</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; g(jnp.arange(4), 3)</span>
<span class="sd">    Array([   0,    1,  256, 6561], dtype=int32)</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">pjit</span><span class="o">.</span><span class="n">make_jit</span><span class="p">(</span>
        <span class="n">fun</span><span class="p">,</span> <span class="n">in_shardings</span><span class="p">,</span> <span class="n">out_shardings</span><span class="p">,</span> <span class="n">donate_argnums</span><span class="p">,</span> <span class="n">donate_argnames</span><span class="p">,</span>
        <span class="n">static_argnums</span><span class="p">,</span> <span class="n">static_argnames</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">backend</span><span class="p">,</span> <span class="n">abstracted_axes</span><span class="p">,</span>
        <span class="n">keep_unused</span><span class="p">,</span> <span class="n">inline</span><span class="p">,</span> <span class="n">use_resource_env</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></div>



<div class="viewcode-block" id="disable_jit">
<a class="viewcode-back" href="../../../_autosummary/jax.disable_jit.html#jax.disable_jit">[docs]</a>
<span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">disable_jit</span><span class="p">(</span><span class="n">disable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Context manager that disables :py:func:`jit` behavior under its dynamic context.</span>

<span class="sd">  For debugging it is useful to have a mechanism that disables :py:func:`jit`</span>
<span class="sd">  everywhere in a dynamic context. Note that this not only disables explicit</span>
<span class="sd">  uses of :func:`jit` by the user, but will also remove any implicit JIT compilation</span>
<span class="sd">  used by the JAX library: this includes implicit JIT computation of `body` and</span>
<span class="sd">  `cond` functions passed to higher-level primitives like :func:`~jax.lax.scan` and</span>
<span class="sd">  :func:`~jax.lax.while_loop`, JIT used in implementations of :mod:`jax.numpy` functions,</span>
<span class="sd">  and any other case where :func:`jit` is used within an API&#39;s implementation.</span>
<span class="sd">  Note however that even under `disable_jit`, individual primitive operations</span>
<span class="sd">  will still be compiled by XLA as in normal eager op-by-op execution.</span>

<span class="sd">  Values that have a data dependence on the arguments to a jitted function are</span>
<span class="sd">  traced and abstracted. For example, an abstract value may be a</span>
<span class="sd">  :py:class:`ShapedArray` instance, representing the set of all possible arrays</span>
<span class="sd">  with a given shape and dtype, but not representing one concrete array with</span>
<span class="sd">  specific values. You might notice those if you use a benign side-effecting</span>
<span class="sd">  operation in a jitted function, like a print:</span>

<span class="sd">  &gt;&gt;&gt; import jax</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; @jax.jit</span>
<span class="sd">  ... def f(x):</span>
<span class="sd">  ...   y = x * 2</span>
<span class="sd">  ...   print(&quot;Value of y is&quot;, y)</span>
<span class="sd">  ...   return y + 3</span>
<span class="sd">  ...</span>
<span class="sd">  &gt;&gt;&gt; print(f(jax.numpy.array([1, 2, 3])))  # doctest:+ELLIPSIS</span>
<span class="sd">  Value of y is Traced&lt;ShapedArray(int32[3])&gt;with&lt;DynamicJaxprTrace...&gt;</span>
<span class="sd">  [5 7 9]</span>

<span class="sd">  Here ``y`` has been abstracted by :py:func:`jit` to a :py:class:`ShapedArray`,</span>
<span class="sd">  which represents an array with a fixed shape and type but an arbitrary value.</span>
<span class="sd">  The value of ``y`` is also traced. If we want to see a concrete value while</span>
<span class="sd">  debugging, and avoid the tracer too, we can use the :py:func:`disable_jit`</span>
<span class="sd">  context manager:</span>

<span class="sd">  &gt;&gt;&gt; import jax</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; with jax.disable_jit():</span>
<span class="sd">  ...   print(f(jax.numpy.array([1, 2, 3])))</span>
<span class="sd">  ...</span>
<span class="sd">  Value of y is [2 4 6]</span>
<span class="sd">  [5 7 9]</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">config</span><span class="o">.</span><span class="n">disable_jit</span><span class="p">(</span><span class="n">disable</span><span class="p">):</span>
    <span class="k">yield</span></div>



<div class="viewcode-block" id="xla_computation">
<a class="viewcode-back" href="../../../_autosummary/jax.xla_computation.html#jax.xla_computation">[docs]</a>
<span class="k">def</span> <span class="nf">xla_computation</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
                    <span class="n">static_argnums</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(),</span>
                    <span class="n">axis_env</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="n">AxisName</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                    <span class="n">in_parts</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">out_parts</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">backend</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                    <span class="n">tuple_args</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                    <span class="n">instantiate_const_outputs</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                    <span class="n">return_shape</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                    <span class="n">donate_argnums</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">())</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Creates a function that produces its XLA computation given example args.</span>

<span class="sd">  Args:</span>
<span class="sd">    fun: Function from which to form XLA computations.</span>
<span class="sd">    static_argnums: See the :py:func:`jax.jit` docstring.</span>
<span class="sd">    axis_env: Optional, a sequence of pairs where the first element is an axis</span>
<span class="sd">      name and the second element is a positive integer representing the size of</span>
<span class="sd">      the mapped axis with that name. This parameter is useful when lowering</span>
<span class="sd">      functions that involve parallel communication collectives, and it</span>
<span class="sd">      specifies the axis name/size environment that would be set up by</span>
<span class="sd">      applications of :py:func:`jax.pmap`. See the examples below.</span>
<span class="sd">    in_parts: Optional, how each argument to ``fun`` should be partitioned or</span>
<span class="sd">      replicated. This is used to specify partitioned XLA computations, see</span>
<span class="sd">      ``sharded_jit`` for more info.</span>
<span class="sd">    out_parts: Optional, how each output of ``fun`` should be partitioned or</span>
<span class="sd">      replicated. This is used to specify partitioned XLA computations, see</span>
<span class="sd">      ``sharded_jit`` for more info.</span>
<span class="sd">    backend: This is an experimental feature and the API is likely to change.</span>
<span class="sd">      Optional, a string representing the XLA backend: ``&#39;cpu&#39;``, ``&#39;gpu&#39;``, or</span>
<span class="sd">      ``&#39;tpu&#39;``.</span>
<span class="sd">    tuple_args: Optional bool, defaults to ``False``. If ``True``, the resulting</span>
<span class="sd">      XLA computation will have a single tuple argument that is unpacked into</span>
<span class="sd">      the specified function arguments. If `None`, tupling will be enabled when</span>
<span class="sd">      there are more than 100 arguments, since some platforms have limits on</span>
<span class="sd">      argument arity.</span>
<span class="sd">    instantiate_const_outputs: Deprecated argument, does nothing.</span>
<span class="sd">    return_shape: Optional boolean, defaults to ``False``. If ``True``, the</span>
<span class="sd">      wrapped function returns a pair where the first element is the XLA</span>
<span class="sd">      computation and the second element is a pytree with the same structure as</span>
<span class="sd">      the output of ``fun`` and where the leaves are objects with ``shape``,</span>
<span class="sd">      ``dtype``, and ``named_shape`` attributes representing the corresponding</span>
<span class="sd">      types of the output leaves.</span>
<span class="sd">    donate_argnums: Specify which arguments are &quot;donated&quot; to the computation.</span>
<span class="sd">      It is safe to donate arguments if you no longer need them once the</span>
<span class="sd">      computation has finished. In some cases XLA can make use of donated</span>
<span class="sd">      buffers to reduce the amount of memory needed to perform a computation,</span>
<span class="sd">      for example recycling one of your input buffers to store a result. You</span>
<span class="sd">      should not reuse buffers that you donate to a computation, JAX will raise</span>
<span class="sd">      an error if you try to.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A wrapped version of ``fun`` that when applied to example arguments returns</span>
<span class="sd">    a built XLA Computation (see xla_client.py), from which representations of</span>
<span class="sd">    the unoptimized XLA HLO computation can be extracted using methods like</span>
<span class="sd">    ``as_hlo_text``, ``as_serialized_hlo_module_proto``, and</span>
<span class="sd">    ``as_hlo_dot_graph``. If the argument ``return_shape`` is ``True``, then the</span>
<span class="sd">    wrapped function returns a pair where the first element is the XLA</span>
<span class="sd">    Computation and the second element is a pytree representing the structure,</span>
<span class="sd">    shapes, dtypes, and named shapes of the output of ``fun``.</span>

<span class="sd">    Concrete example arguments are not always necessary. For those arguments not</span>
<span class="sd">    indicated by ``static_argnums``, any object with ``shape`` and ``dtype``</span>
<span class="sd">    attributes is acceptable (excepting namedtuples, which are treated as Python</span>
<span class="sd">    containers).</span>

<span class="sd">  For example:</span>

<span class="sd">  &gt;&gt;&gt; import jax</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; def f(x): return jax.numpy.sin(jax.numpy.cos(x))</span>
<span class="sd">  &gt;&gt;&gt; c = jax.xla_computation(f)(3.)</span>
<span class="sd">  &gt;&gt;&gt; print(c.as_hlo_text())  # doctest: +SKIP</span>
<span class="sd">  HloModule xla_computation_f.6</span>
<span class="sd">  &lt;BLANKLINE&gt;</span>
<span class="sd">  ENTRY xla_computation_f.6 {</span>
<span class="sd">    constant.2 = pred[] constant(false)</span>
<span class="sd">    parameter.1 = f32[] parameter(0)</span>
<span class="sd">    cosine.3 = f32[] cosine(parameter.1)</span>
<span class="sd">    sine.4 = f32[] sine(cosine.3)</span>
<span class="sd">    ROOT tuple.5 = (f32[]) tuple(sine.4)</span>
<span class="sd">  }</span>
<span class="sd">  &lt;BLANKLINE&gt;</span>
<span class="sd">  &lt;BLANKLINE&gt;</span>


<span class="sd">  Alternatively, the assignment to ``c`` above could be written:</span>

<span class="sd">  &gt;&gt;&gt; import types</span>
<span class="sd">  &gt;&gt;&gt; scalar = types.SimpleNamespace(shape=(), dtype=np.dtype(np.float32))</span>
<span class="sd">  &gt;&gt;&gt; c = jax.xla_computation(f)(scalar)</span>


<span class="sd">  Here&#39;s an example that involves a parallel collective and axis name:</span>

<span class="sd">  &gt;&gt;&gt; def f(x): return x - jax.lax.psum(x, &#39;i&#39;)</span>
<span class="sd">  &gt;&gt;&gt; c = jax.xla_computation(f, axis_env=[(&#39;i&#39;, 4)])(2)</span>
<span class="sd">  &gt;&gt;&gt; print(c.as_hlo_text())  # doctest: +SKIP</span>
<span class="sd">  HloModule jaxpr_computation.9</span>
<span class="sd">  primitive_computation.3 {</span>
<span class="sd">    parameter.4 = s32[] parameter(0)</span>
<span class="sd">    parameter.5 = s32[] parameter(1)</span>
<span class="sd">    ROOT add.6 = s32[] add(parameter.4, parameter.5)</span>
<span class="sd">  }</span>
<span class="sd">  ENTRY jaxpr_computation.9 {</span>
<span class="sd">    tuple.1 = () tuple()</span>
<span class="sd">    parameter.2 = s32[] parameter(0)</span>
<span class="sd">    all-reduce.7 = s32[] all-reduce(parameter.2), replica_groups={{0,1,2,3}}, to_apply=primitive_computation.3</span>
<span class="sd">    ROOT subtract.8 = s32[] subtract(parameter.2, all-reduce.7)</span>
<span class="sd">  }</span>
<span class="sd">  &lt;BLANKLINE&gt;</span>
<span class="sd">  &lt;BLANKLINE&gt;</span>

<span class="sd">  Notice the ``replica_groups`` that were generated. Here&#39;s an example that</span>
<span class="sd">  generates more interesting ``replica_groups``:</span>

<span class="sd">  &gt;&gt;&gt; from jax import lax</span>
<span class="sd">  &gt;&gt;&gt; def g(x):</span>
<span class="sd">  ...   rowsum = lax.psum(x, &#39;i&#39;)</span>
<span class="sd">  ...   colsum = lax.psum(x, &#39;j&#39;)</span>
<span class="sd">  ...   allsum = lax.psum(x, (&#39;i&#39;, &#39;j&#39;))</span>
<span class="sd">  ...   return rowsum, colsum, allsum</span>
<span class="sd">  ...</span>
<span class="sd">  &gt;&gt;&gt; axis_env = [(&#39;i&#39;, 4), (&#39;j&#39;, 2)]</span>
<span class="sd">  &gt;&gt;&gt; c = xla_computation(g, axis_env=axis_env)(5.)</span>
<span class="sd">  &gt;&gt;&gt; print(c.as_hlo_text())  # doctest: +SKIP</span>
<span class="sd">  HloModule jaxpr_computation__1.19</span>
<span class="sd">  [removed uninteresting text here]</span>
<span class="sd">  ENTRY jaxpr_computation__1.19 {</span>
<span class="sd">    tuple.1 = () tuple()</span>
<span class="sd">    parameter.2 = f32[] parameter(0)</span>
<span class="sd">    all-reduce.7 = f32[] all-reduce(parameter.2), replica_groups={{0,2,4,6},{1,3,5,7}}, to_apply=primitive_computation__1.3</span>
<span class="sd">    all-reduce.12 = f32[] all-reduce(parameter.2), replica_groups={{0,1},{2,3},{4,5},{6,7}}, to_apply=primitive_computation__1.8</span>
<span class="sd">    all-reduce.17 = f32[] all-reduce(parameter.2), replica_groups={{0,1,2,3,4,5,6,7}}, to_apply=primitive_computation__1.13</span>
<span class="sd">    ROOT tuple.18 = (f32[], f32[], f32[]) tuple(all-reduce.7, all-reduce.12, all-reduce.17)</span>
<span class="sd">  }</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">instantiate_const_outputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="s2">&quot;instantiate_const_outputs has been deprecated. Please use the ahead of&quot;</span>
        <span class="s2">&quot; time APIs. You can read more here:&quot;</span>
        <span class="s2">&quot; https://jax.readthedocs.io/en/latest/aot.html&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">in_parts</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="s2">&quot;in_parts has been deprecated. Please use the ahead of time APIs. You&quot;</span>
        <span class="s2">&quot; can read more here: https://jax.readthedocs.io/en/latest/aot.html&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">out_parts</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="s2">&quot;out_parts has been deprecated. Please use the ahead of time APIs. You&quot;</span>
        <span class="s2">&quot; can read more here: https://jax.readthedocs.io/en/latest/aot.html&quot;</span><span class="p">)</span>

  <span class="n">check_callable</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="n">static_argnums</span> <span class="o">=</span> <span class="n">_ensure_index_tuple</span><span class="p">(</span><span class="n">static_argnums</span><span class="p">)</span>
  <span class="n">donate_argnums</span> <span class="o">=</span> <span class="n">_ensure_index_tuple</span><span class="p">(</span><span class="n">donate_argnums</span><span class="p">)</span>
  <span class="n">donate_argnums</span> <span class="o">=</span> <span class="n">rebase_donate_argnums</span><span class="p">(</span><span class="n">donate_argnums</span><span class="p">,</span> <span class="n">static_argnums</span><span class="p">)</span>

  <span class="n">fun_name</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="s2">&quot;__name__&quot;</span><span class="p">,</span> <span class="s2">&quot;unknown&quot;</span><span class="p">)</span>

  <span class="n">platform</span> <span class="o">=</span> <span class="n">backend</span> <span class="k">if</span> <span class="n">backend</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">xb</span><span class="o">.</span><span class="n">get_backend</span><span class="p">()</span><span class="o">.</span><span class="n">platform</span>

  <span class="k">def</span> <span class="nf">make_axis_env</span><span class="p">(</span><span class="n">nreps</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">axis_env</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">sharding_impls</span><span class="o">.</span><span class="n">AxisEnv</span><span class="p">(</span><span class="n">nreps</span><span class="p">,</span> <span class="p">(),</span> <span class="p">())</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">nreps</span> <span class="o">=</span> <span class="n">nreps</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">size</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">axis_env</span><span class="p">)</span>
      <span class="n">names</span><span class="p">,</span> <span class="n">sizes</span> <span class="o">=</span> <span class="n">unzip2</span><span class="p">(</span><span class="n">axis_env</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">sharding_impls</span><span class="o">.</span><span class="n">AxisEnv</span><span class="p">(</span><span class="n">nreps</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">sizes</span><span class="p">)</span>

  <span class="nd">@wraps</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="nd">@api_boundary</span>
  <span class="k">def</span> <span class="nf">computation_maker</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">max</span><span class="p">(</span><span class="n">static_argnums</span> <span class="o">+</span> <span class="n">donate_argnums</span><span class="p">,</span> <span class="n">default</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;jitted function has </span><span class="si">{</span><span class="n">static_argnums</span><span class="si">=}</span><span class="s2">, </span><span class="si">{</span><span class="n">donate_argnums</span><span class="si">=}</span><span class="s2"> but &quot;</span>
                       <span class="sa">f</span><span class="s2">&quot;was called with only </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span><span class="si">}</span><span class="s2"> positional arguments.&quot;</span><span class="p">)</span>

    <span class="n">f</span> <span class="o">=</span> <span class="n">lu</span><span class="o">.</span><span class="n">wrap_init</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
    <span class="n">f</span><span class="p">,</span> <span class="n">dyn_args</span> <span class="o">=</span> <span class="n">argnums_partial_except</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">static_argnums</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">allow_invalid</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">args_flat</span><span class="p">,</span> <span class="n">in_tree</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">((</span><span class="n">dyn_args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">donate_argnums</span><span class="p">:</span>
      <span class="n">donated_invars</span> <span class="o">=</span> <span class="n">donation_vector</span><span class="p">(</span><span class="n">donate_argnums</span><span class="p">,</span> <span class="p">(),</span> <span class="n">dyn_args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">donated_invars</span> <span class="o">=</span> <span class="p">(</span><span class="kc">False</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">args_flat</span><span class="p">)</span>

    <span class="n">jaxtree_fun</span><span class="p">,</span> <span class="n">out_tree</span> <span class="o">=</span> <span class="n">flatten_fun</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">)</span>
    <span class="n">avals</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">shaped_abstractify</span><span class="p">,</span> <span class="n">args_flat</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">ExitStack</span><span class="p">()</span> <span class="k">as</span> <span class="n">stack</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">axis_name</span><span class="p">,</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">axis_env</span> <span class="ow">or</span> <span class="p">[]:</span>
        <span class="n">stack</span><span class="o">.</span><span class="n">enter_context</span><span class="p">(</span><span class="n">core</span><span class="o">.</span><span class="n">extend_axis_env</span><span class="p">(</span><span class="n">axis_name</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
      <span class="n">jaxpr</span><span class="p">,</span> <span class="n">out_avals</span><span class="p">,</span> <span class="n">consts</span><span class="p">,</span> <span class="p">()</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">trace_to_jaxpr_dynamic</span><span class="p">(</span><span class="n">jaxtree_fun</span><span class="p">,</span> <span class="n">avals</span><span class="p">)</span>
      <span class="n">jaxpr</span> <span class="o">=</span> <span class="n">dispatch</span><span class="o">.</span><span class="n">apply_outfeed_rewriter</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">axis_env</span><span class="p">:</span>
        <span class="n">jaxpr</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">remove_named_axis_effects</span><span class="p">(</span>
            <span class="n">jaxpr</span><span class="p">,</span> <span class="p">{</span><span class="n">axis_name</span> <span class="k">for</span> <span class="n">axis_name</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">axis_env</span><span class="p">}</span>
        <span class="p">)</span>
      <span class="n">axis_env_</span> <span class="o">=</span> <span class="n">make_axis_env</span><span class="p">(</span><span class="n">dispatch</span><span class="o">.</span><span class="n">jaxpr_replicas</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">))</span>
      <span class="n">ordered_effects</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
          <span class="n">effects</span><span class="o">.</span><span class="n">ordered_effects</span><span class="o">.</span><span class="n">filter_in</span><span class="p">(</span><span class="n">jaxpr</span><span class="o">.</span><span class="n">effects</span><span class="p">))</span>
      <span class="n">lowering_result</span> <span class="o">=</span> <span class="n">mlir</span><span class="o">.</span><span class="n">lower_jaxpr_to_module</span><span class="p">(</span>
          <span class="sa">f</span><span class="s2">&quot;xla_computation_</span><span class="si">{</span><span class="n">fun_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
          <span class="n">core</span><span class="o">.</span><span class="n">ClosedJaxpr</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">,</span> <span class="n">consts</span><span class="p">),</span>
          <span class="n">ordered_effects</span><span class="o">=</span><span class="n">ordered_effects</span><span class="p">,</span>
          <span class="n">backend_or_name</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span>
          <span class="n">platforms</span><span class="o">=</span><span class="p">[</span><span class="n">platform</span><span class="p">],</span>
          <span class="n">axis_context</span><span class="o">=</span><span class="n">sharding_impls</span><span class="o">.</span><span class="n">ReplicaAxisContext</span><span class="p">(</span><span class="n">axis_env_</span><span class="p">),</span>
          <span class="n">name_stack</span><span class="o">=</span><span class="n">source_info_util</span><span class="o">.</span><span class="n">new_name_stack</span><span class="p">(</span>
              <span class="n">wrap_name</span><span class="p">(</span><span class="n">fun_name</span><span class="p">,</span> <span class="s2">&quot;xla_computation&quot;</span><span class="p">)),</span>
          <span class="n">donated_args</span><span class="o">=</span><span class="n">donated_invars</span><span class="p">,</span>
          <span class="n">arg_shardings</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
          <span class="n">result_shardings</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
          <span class="n">lowering_parameters</span><span class="o">=</span><span class="n">mlir</span><span class="o">.</span><span class="n">LoweringParameters</span><span class="p">())</span>

      <span class="k">if</span> <span class="n">xla_extension_version</span> <span class="o">&gt;=</span> <span class="mi">244</span><span class="p">:</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">mlir</span><span class="o">.</span><span class="n">module_to_bytecode</span><span class="p">(</span><span class="n">lowering_result</span><span class="o">.</span><span class="n">module</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">mlir</span><span class="o">.</span><span class="n">module_to_string</span><span class="p">(</span><span class="n">lowering_result</span><span class="o">.</span><span class="n">module</span><span class="p">)</span>

      <span class="n">built</span> <span class="o">=</span> <span class="n">xc</span><span class="o">.</span><span class="n">_xla</span><span class="o">.</span><span class="n">mlir</span><span class="o">.</span><span class="n">mlir_module_to_xla_computation</span><span class="p">(</span>
          <span class="n">m</span><span class="p">,</span> <span class="n">use_tuple_args</span><span class="o">=</span><span class="n">tuple_args</span><span class="p">,</span> <span class="n">return_tuple</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">out_shapes_flat</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">ShapeDtypeStruct</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">named_shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">out_avals</span><span class="p">]</span>
    <span class="n">out_shape</span> <span class="o">=</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">out_tree</span><span class="p">(),</span> <span class="n">out_shapes_flat</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">out_aval</span> <span class="ow">in</span> <span class="n">out_avals</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out_aval</span><span class="p">,</span> <span class="n">ShapedArray</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;As we want to propagate the weak_type, we need &quot;</span>
                           <span class="s2">&quot;to get a ShapedArray, otherwise this &quot;</span>
                           <span class="s2">&quot;information is lost&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">return_shape</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">built</span><span class="p">,</span> <span class="n">out_shape</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">built</span>

  <span class="k">return</span> <span class="n">computation_maker</span></div>


<div class="viewcode-block" id="grad">
<a class="viewcode-back" href="../../../_autosummary/jax.grad.html#jax.grad">[docs]</a>
<span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">argnums</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
         <span class="n">has_aux</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">holomorphic</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
         <span class="n">allow_int</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
         <span class="n">reduce_axes</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">AxisName</span><span class="p">]</span> <span class="o">=</span> <span class="p">())</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Creates a function that evaluates the gradient of ``fun``.</span>

<span class="sd">  Args:</span>
<span class="sd">    fun: Function to be differentiated. Its arguments at positions specified by</span>
<span class="sd">      ``argnums`` should be arrays, scalars, or standard Python containers.</span>
<span class="sd">      Argument arrays in the positions specified by ``argnums`` must be of</span>
<span class="sd">      inexact (i.e., floating-point or complex) type. It</span>
<span class="sd">      should return a scalar (which includes arrays with shape ``()`` but not</span>
<span class="sd">      arrays with shape ``(1,)`` etc.)</span>
<span class="sd">    argnums: Optional, integer or sequence of integers. Specifies which</span>
<span class="sd">      positional argument(s) to differentiate with respect to (default 0).</span>
<span class="sd">    has_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the</span>
<span class="sd">      first element is considered the output of the mathematical function to be</span>
<span class="sd">      differentiated and the second element is auxiliary data. Default False.</span>
<span class="sd">    holomorphic: Optional, bool. Indicates whether ``fun`` is promised to be</span>
<span class="sd">      holomorphic. If True, inputs and outputs must be complex. Default False.</span>
<span class="sd">    allow_int: Optional, bool. Whether to allow differentiating with</span>
<span class="sd">      respect to integer valued inputs. The gradient of an integer input will</span>
<span class="sd">      have a trivial vector-space dtype (float0). Default False.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A function with the same arguments as ``fun``, that evaluates the gradient</span>
<span class="sd">    of ``fun``. If ``argnums`` is an integer then the gradient has the same</span>
<span class="sd">    shape and type as the positional argument indicated by that integer. If</span>
<span class="sd">    argnums is a tuple of integers, the gradient is a tuple of values with the</span>
<span class="sd">    same shapes and types as the corresponding arguments. If ``has_aux`` is True</span>
<span class="sd">    then a pair of (gradient, auxiliary_data) is returned.</span>

<span class="sd">  For example:</span>

<span class="sd">  &gt;&gt;&gt; import jax</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; grad_tanh = jax.grad(jax.numpy.tanh)</span>
<span class="sd">  &gt;&gt;&gt; print(grad_tanh(0.2))</span>
<span class="sd">  0.961043</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">reduce_axes</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;reduce_axes argument to grad is deprecated&quot;</span><span class="p">)</span>
  <span class="k">del</span> <span class="n">reduce_axes</span>
  <span class="n">value_and_grad_f</span> <span class="o">=</span> <span class="n">value_and_grad</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">argnums</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="n">has_aux</span><span class="p">,</span>
                                    <span class="n">holomorphic</span><span class="o">=</span><span class="n">holomorphic</span><span class="p">,</span>
                                    <span class="n">allow_int</span><span class="o">=</span><span class="n">allow_int</span><span class="p">)</span>

  <span class="n">docstr</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;Gradient of </span><span class="si">{fun}</span><span class="s2"> with respect to positional argument(s) &quot;</span>
            <span class="s2">&quot;</span><span class="si">{argnums}</span><span class="s2">. Takes the same arguments as </span><span class="si">{fun}</span><span class="s2"> but returns the &quot;</span>
            <span class="s2">&quot;gradient, which has the same shape as the arguments at &quot;</span>
            <span class="s2">&quot;positions </span><span class="si">{argnums}</span><span class="s2">.&quot;</span><span class="p">)</span>

  <span class="nd">@wraps</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">docstr</span><span class="o">=</span><span class="n">docstr</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="n">argnums</span><span class="p">)</span>
  <span class="nd">@api_boundary</span>
  <span class="k">def</span> <span class="nf">grad_f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">g</span> <span class="o">=</span> <span class="n">value_and_grad_f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">g</span>

  <span class="nd">@wraps</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">docstr</span><span class="o">=</span><span class="n">docstr</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="n">argnums</span><span class="p">)</span>
  <span class="nd">@api_boundary</span>
  <span class="k">def</span> <span class="nf">grad_f_aux</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">aux</span><span class="p">),</span> <span class="n">g</span> <span class="o">=</span> <span class="n">value_and_grad_f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">g</span><span class="p">,</span> <span class="n">aux</span>

  <span class="k">return</span> <span class="n">grad_f_aux</span> <span class="k">if</span> <span class="n">has_aux</span> <span class="k">else</span> <span class="n">grad_f</span></div>


<div class="viewcode-block" id="value_and_grad">
<a class="viewcode-back" href="../../../_autosummary/jax.value_and_grad.html#jax.value_and_grad">[docs]</a>
<span class="k">def</span> <span class="nf">value_and_grad</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">argnums</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
                   <span class="n">has_aux</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">holomorphic</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                   <span class="n">allow_int</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">reduce_axes</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">AxisName</span><span class="p">]</span> <span class="o">=</span> <span class="p">()</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Create a function that evaluates both ``fun`` and the gradient of ``fun``.</span>

<span class="sd">  Args:</span>
<span class="sd">    fun: Function to be differentiated. Its arguments at positions specified by</span>
<span class="sd">      ``argnums`` should be arrays, scalars, or standard Python containers. It</span>
<span class="sd">      should return a scalar (which includes arrays with shape ``()`` but not</span>
<span class="sd">      arrays with shape ``(1,)`` etc.)</span>
<span class="sd">    argnums: Optional, integer or sequence of integers. Specifies which</span>
<span class="sd">      positional argument(s) to differentiate with respect to (default 0).</span>
<span class="sd">    has_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the</span>
<span class="sd">      first element is considered the output of the mathematical function to be</span>
<span class="sd">      differentiated and the second element is auxiliary data. Default False.</span>
<span class="sd">    holomorphic: Optional, bool. Indicates whether ``fun`` is promised to be</span>
<span class="sd">      holomorphic. If True, inputs and outputs must be complex. Default False.</span>
<span class="sd">    allow_int: Optional, bool. Whether to allow differentiating with</span>
<span class="sd">      respect to integer valued inputs. The gradient of an integer input will</span>
<span class="sd">      have a trivial vector-space dtype (float0). Default False.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A function with the same arguments as ``fun`` that evaluates both ``fun``</span>
<span class="sd">    and the gradient of ``fun`` and returns them as a pair (a two-element</span>
<span class="sd">    tuple). If ``argnums`` is an integer then the gradient has the same shape</span>
<span class="sd">    and type as the positional argument indicated by that integer. If argnums is</span>
<span class="sd">    a sequence of integers, the gradient is a tuple of values with the same</span>
<span class="sd">    shapes and types as the corresponding arguments. If ``has_aux`` is True</span>
<span class="sd">    then a tuple of ((value, auxiliary_data), gradient) is returned.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">reduce_axes</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;reduce_axes argument to grad is deprecated&quot;</span><span class="p">)</span>
  <span class="k">del</span> <span class="n">reduce_axes</span>

  <span class="n">docstr</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;Value and gradient of </span><span class="si">{fun}</span><span class="s2"> with respect to positional &quot;</span>
            <span class="s2">&quot;argument(s) </span><span class="si">{argnums}</span><span class="s2">. Takes the same arguments as </span><span class="si">{fun}</span><span class="s2"> but &quot;</span>
            <span class="s2">&quot;returns a two-element tuple where the first element is the value &quot;</span>
            <span class="s2">&quot;of </span><span class="si">{fun}</span><span class="s2"> and the second element is the gradient, which has the &quot;</span>
            <span class="s2">&quot;same shape as the arguments at positions </span><span class="si">{argnums}</span><span class="s2">.&quot;</span><span class="p">)</span>

  <span class="n">check_callable</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="n">argnums</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">concrete_or_error</span><span class="p">(</span><span class="n">_ensure_index</span><span class="p">,</span> <span class="n">argnums</span><span class="p">)</span>

  <span class="nd">@wraps</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">docstr</span><span class="o">=</span><span class="n">docstr</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="n">argnums</span><span class="p">)</span>
  <span class="nd">@api_boundary</span>
  <span class="k">def</span> <span class="nf">value_and_grad_f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">max_argnum</span> <span class="o">=</span> <span class="n">argnums</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">argnums</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="nb">max</span><span class="p">(</span><span class="n">argnums</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">max_argnum</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;differentiating with respect to </span><span class="si">{</span><span class="n">argnums</span><span class="si">=}</span><span class="s2"> requires at least &quot;</span>
                      <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">max_argnum</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2"> positional arguments to be passed by the caller, &quot;</span>
                      <span class="sa">f</span><span class="s2">&quot;but got only </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span><span class="si">}</span><span class="s2"> positional arguments.&quot;</span><span class="p">)</span>

    <span class="n">f</span> <span class="o">=</span> <span class="n">lu</span><span class="o">.</span><span class="n">wrap_init</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
    <span class="n">f_partial</span><span class="p">,</span> <span class="n">dyn_args</span> <span class="o">=</span> <span class="n">argnums_partial</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">argnums</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span>
                                          <span class="n">require_static_args_hashable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">leaf</span> <span class="ow">in</span> <span class="n">tree_leaves</span><span class="p">(</span><span class="n">dyn_args</span><span class="p">):</span>
      <span class="n">_check_input_dtype_grad</span><span class="p">(</span><span class="n">holomorphic</span><span class="p">,</span> <span class="n">allow_int</span><span class="p">,</span> <span class="n">leaf</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">has_aux</span><span class="p">:</span>
      <span class="n">ans</span><span class="p">,</span> <span class="n">vjp_py</span> <span class="o">=</span> <span class="n">_vjp</span><span class="p">(</span><span class="n">f_partial</span><span class="p">,</span> <span class="o">*</span><span class="n">dyn_args</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">ans</span><span class="p">,</span> <span class="n">vjp_py</span><span class="p">,</span> <span class="n">aux</span> <span class="o">=</span> <span class="n">_vjp</span><span class="p">(</span>
          <span class="n">f_partial</span><span class="p">,</span> <span class="o">*</span><span class="n">dyn_args</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">_check_scalar</span><span class="p">(</span><span class="n">ans</span><span class="p">)</span>
    <span class="n">tree_map</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">_check_output_dtype_grad</span><span class="p">,</span> <span class="n">holomorphic</span><span class="p">),</span> <span class="n">ans</span><span class="p">)</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">vjp_py</span><span class="p">(</span><span class="n">lax_internal</span><span class="o">.</span><span class="n">_one</span><span class="p">(</span><span class="n">ans</span><span class="p">))</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">g</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">argnums</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">g</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">has_aux</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">ans</span><span class="p">,</span> <span class="n">g</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="p">(</span><span class="n">ans</span><span class="p">,</span> <span class="n">aux</span><span class="p">),</span> <span class="n">g</span>

  <span class="k">return</span> <span class="n">value_and_grad_f</span></div>


<span class="k">def</span> <span class="nf">_check_scalar</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;Gradient only defined for scalar-output functions. Output </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">aval</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">get_aval</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="k">except</span> <span class="ne">TypeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">msg</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;was </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">))</span> <span class="kn">from</span> <span class="nn">e</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">aval</span><span class="p">,</span> <span class="n">ShapedArray</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">aval</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">():</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">msg</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;had shape: </span><span class="si">{</span><span class="n">aval</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">msg</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;had abstract value </span><span class="si">{</span><span class="n">aval</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">_check_input_dtype_revderiv</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">holomorphic</span><span class="p">,</span> <span class="n">allow_int</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
  <span class="n">dispatch</span><span class="o">.</span><span class="n">check_arg</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">aval</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">get_aval</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">holomorphic</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">complexfloating</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> with holomorphic=True requires inputs with complex dtype, &quot;</span>
                      <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">dtypes</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">extended</span><span class="p">)</span> <span class="ow">or</span>
      <span class="n">dtypes</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">integer</span><span class="p">)</span> <span class="ow">or</span>
      <span class="n">dtypes</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">bool_</span><span class="p">)):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">allow_int</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> requires real- or complex-valued inputs (input dtype &quot;</span>
                      <span class="sa">f</span><span class="s2">&quot;that is a sub-dtype of np.inexact), but got </span><span class="si">{</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">. &quot;</span>
                      <span class="s2">&quot;If you want to use Boolean- or integer-valued inputs, use vjp &quot;</span>
                      <span class="s2">&quot;or set allow_int to True.&quot;</span><span class="p">)</span>
  <span class="k">elif</span> <span class="ow">not</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">inexact</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> requires numerical-valued inputs (input dtype that is a &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;sub-dtype of np.bool_ or np.number), but got </span><span class="si">{</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
<span class="n">_check_input_dtype_grad</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">_check_input_dtype_revderiv</span><span class="p">,</span> <span class="s2">&quot;grad&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_check_output_dtype_revderiv</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">holomorphic</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
  <span class="n">aval</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">get_aval</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">extended</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> with output element type </span><span class="si">{</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">holomorphic</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">complexfloating</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> with holomorphic=True requires outputs with complex dtype, &quot;</span>
                      <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">complexfloating</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> requires real-valued outputs (output dtype that is &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;a sub-dtype of np.floating), but got </span><span class="si">{</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">. &quot;</span>
                    <span class="s2">&quot;For holomorphic differentiation, pass holomorphic=True. &quot;</span>
                    <span class="s2">&quot;For differentiation of non-holomorphic functions involving complex &quot;</span>
                    <span class="s2">&quot;outputs, use jax.vjp directly.&quot;</span><span class="p">)</span>
  <span class="k">elif</span> <span class="ow">not</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">floating</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> requires real-valued outputs (output dtype that is &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;a sub-dtype of np.floating), but got </span><span class="si">{</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">. &quot;</span>
                    <span class="s2">&quot;For differentiation of functions with integer outputs, use &quot;</span>
                    <span class="s2">&quot;jax.vjp directly.&quot;</span><span class="p">)</span>
<span class="n">_check_output_dtype_grad</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">_check_output_dtype_revderiv</span><span class="p">,</span> <span class="s2">&quot;grad&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="jacfwd">
<a class="viewcode-back" href="../../../_autosummary/jax.jacfwd.html#jax.jacfwd">[docs]</a>
<span class="k">def</span> <span class="nf">jacfwd</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">argnums</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
           <span class="n">has_aux</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">holomorphic</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Jacobian of ``fun`` evaluated column-by-column using forward-mode AD.</span>

<span class="sd">  Args:</span>
<span class="sd">    fun: Function whose Jacobian is to be computed.</span>
<span class="sd">    argnums: Optional, integer or sequence of integers. Specifies which</span>
<span class="sd">      positional argument(s) to differentiate with respect to (default ``0``).</span>
<span class="sd">    has_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the</span>
<span class="sd">      first element is considered the output of the mathematical function to be</span>
<span class="sd">      differentiated and the second element is auxiliary data. Default False.</span>
<span class="sd">    holomorphic: Optional, bool. Indicates whether ``fun`` is promised to be</span>
<span class="sd">      holomorphic. Default False.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A function with the same arguments as ``fun``, that evaluates the Jacobian of</span>
<span class="sd">    ``fun`` using forward-mode automatic differentiation. If ``has_aux`` is True</span>
<span class="sd">    then a pair of (jacobian, auxiliary_data) is returned.</span>

<span class="sd">  &gt;&gt;&gt; import jax</span>
<span class="sd">  &gt;&gt;&gt; import jax.numpy as jnp</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; def f(x):</span>
<span class="sd">  ...   return jnp.asarray(</span>
<span class="sd">  ...     [x[0], 5*x[2], 4*x[1]**2 - 2*x[2], x[2] * jnp.sin(x[0])])</span>
<span class="sd">  ...</span>
<span class="sd">  &gt;&gt;&gt; print(jax.jacfwd(f)(jnp.array([1., 2., 3.])))</span>
<span class="sd">  [[ 1.       0.       0.     ]</span>
<span class="sd">   [ 0.       0.       5.     ]</span>
<span class="sd">   [ 0.      16.      -2.     ]</span>
<span class="sd">   [ 1.6209   0.       0.84147]]</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">check_callable</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="n">argnums</span> <span class="o">=</span> <span class="n">_ensure_index</span><span class="p">(</span><span class="n">argnums</span><span class="p">)</span>

  <span class="n">docstr</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;Jacobian of </span><span class="si">{fun}</span><span class="s2"> with respect to positional argument(s) &quot;</span>
            <span class="s2">&quot;</span><span class="si">{argnums}</span><span class="s2">. Takes the same arguments as </span><span class="si">{fun}</span><span class="s2"> but returns the &quot;</span>
            <span class="s2">&quot;jacobian of the output with respect to the arguments at &quot;</span>
            <span class="s2">&quot;positions </span><span class="si">{argnums}</span><span class="s2">.&quot;</span><span class="p">)</span>

  <span class="nd">@wraps</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">docstr</span><span class="o">=</span><span class="n">docstr</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="n">argnums</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">jacfun</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">lu</span><span class="o">.</span><span class="n">wrap_init</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
    <span class="n">f_partial</span><span class="p">,</span> <span class="n">dyn_args</span> <span class="o">=</span> <span class="n">argnums_partial</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">argnums</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span>
                                          <span class="n">require_static_args_hashable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">tree_map</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">_check_input_dtype_jacfwd</span><span class="p">,</span> <span class="n">holomorphic</span><span class="p">),</span> <span class="n">dyn_args</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">has_aux</span><span class="p">:</span>
      <span class="n">pushfwd</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">_jvp</span><span class="p">,</span> <span class="n">f_partial</span><span class="p">,</span> <span class="n">dyn_args</span><span class="p">)</span>
      <span class="n">y</span><span class="p">,</span> <span class="n">jac</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">pushfwd</span><span class="p">,</span> <span class="n">out_axes</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))(</span><span class="n">_std_basis</span><span class="p">(</span><span class="n">dyn_args</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">pushfwd</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">_jvp</span><span class="p">,</span> <span class="n">f_partial</span><span class="p">,</span> <span class="n">dyn_args</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="n">y</span><span class="p">,</span> <span class="n">jac</span><span class="p">,</span> <span class="n">aux</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">pushfwd</span><span class="p">,</span> <span class="n">out_axes</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">))(</span><span class="n">_std_basis</span><span class="p">(</span><span class="n">dyn_args</span><span class="p">))</span>
    <span class="n">tree_map</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">_check_output_dtype_jacfwd</span><span class="p">,</span> <span class="n">holomorphic</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">example_args</span> <span class="o">=</span> <span class="n">dyn_args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">argnums</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">dyn_args</span>
    <span class="n">jac_tree</span> <span class="o">=</span> <span class="n">tree_map</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">_jacfwd_unravel</span><span class="p">,</span> <span class="n">example_args</span><span class="p">),</span> <span class="n">y</span><span class="p">,</span> <span class="n">jac</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">has_aux</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">jac_tree</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">jac_tree</span><span class="p">,</span> <span class="n">aux</span>

  <span class="k">return</span> <span class="n">jacfun</span></div>


<span class="k">def</span> <span class="nf">_check_input_dtype_jacfwd</span><span class="p">(</span><span class="n">holomorphic</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
  <span class="n">dispatch</span><span class="o">.</span><span class="n">check_arg</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">aval</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">get_aval</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">extended</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;jacfwd with input element type </span><span class="si">{</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">holomorphic</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">complexfloating</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;jacfwd with holomorphic=True requires inputs with complex &quot;</span>
                      <span class="sa">f</span><span class="s2">&quot;dtype, but got </span><span class="si">{</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
  <span class="k">elif</span> <span class="ow">not</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">floating</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;jacfwd requires real-valued inputs (input dtype that is &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;a sub-dtype of np.floating), but got </span><span class="si">{</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">. &quot;</span>
                    <span class="s2">&quot;For holomorphic differentiation, pass holomorphic=True. &quot;</span>
                    <span class="s2">&quot;For differentiation of non-holomorphic functions involving &quot;</span>
                    <span class="s2">&quot;complex inputs or integer inputs, use jax.jvp directly.&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_check_output_dtype_jacfwd</span><span class="p">(</span><span class="n">holomorphic</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
  <span class="n">aval</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">get_aval</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">holomorphic</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">complexfloating</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;jacfwd with holomorphic=True requires outputs with complex dtype, &quot;</span>
                      <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

<div class="viewcode-block" id="jacrev">
<a class="viewcode-back" href="../../../_autosummary/jax.jacrev.html#jax.jacrev">[docs]</a>
<span class="k">def</span> <span class="nf">jacrev</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">argnums</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
           <span class="n">has_aux</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">holomorphic</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">allow_int</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Jacobian of ``fun`` evaluated row-by-row using reverse-mode AD.</span>

<span class="sd">  Args:</span>
<span class="sd">    fun: Function whose Jacobian is to be computed.</span>
<span class="sd">    argnums: Optional, integer or sequence of integers. Specifies which</span>
<span class="sd">      positional argument(s) to differentiate with respect to (default ``0``).</span>
<span class="sd">    has_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the</span>
<span class="sd">      first element is considered the output of the mathematical function to be</span>
<span class="sd">      differentiated and the second element is auxiliary data. Default False.</span>
<span class="sd">    holomorphic: Optional, bool. Indicates whether ``fun`` is promised to be</span>
<span class="sd">      holomorphic. Default False.</span>
<span class="sd">    allow_int: Optional, bool. Whether to allow differentiating with</span>
<span class="sd">      respect to integer valued inputs. The gradient of an integer input will</span>
<span class="sd">      have a trivial vector-space dtype (float0). Default False.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A function with the same arguments as ``fun``, that evaluates the Jacobian of</span>
<span class="sd">    ``fun`` using reverse-mode automatic differentiation. If ``has_aux`` is True</span>
<span class="sd">    then a pair of (jacobian, auxiliary_data) is returned.</span>

<span class="sd">  &gt;&gt;&gt; import jax</span>
<span class="sd">  &gt;&gt;&gt; import jax.numpy as jnp</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; def f(x):</span>
<span class="sd">  ...   return jnp.asarray(</span>
<span class="sd">  ...     [x[0], 5*x[2], 4*x[1]**2 - 2*x[2], x[2] * jnp.sin(x[0])])</span>
<span class="sd">  ...</span>
<span class="sd">  &gt;&gt;&gt; print(jax.jacrev(f)(jnp.array([1., 2., 3.])))</span>
<span class="sd">  [[ 1.       0.       0.     ]</span>
<span class="sd">   [ 0.       0.       5.     ]</span>
<span class="sd">   [ 0.      16.      -2.     ]</span>
<span class="sd">   [ 1.6209   0.       0.84147]]</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">check_callable</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>

  <span class="n">docstr</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;Jacobian of </span><span class="si">{fun}</span><span class="s2"> with respect to positional argument(s) &quot;</span>
            <span class="s2">&quot;</span><span class="si">{argnums}</span><span class="s2">. Takes the same arguments as </span><span class="si">{fun}</span><span class="s2"> but returns the &quot;</span>
            <span class="s2">&quot;jacobian of the output with respect to the arguments at &quot;</span>
            <span class="s2">&quot;positions </span><span class="si">{argnums}</span><span class="s2">.&quot;</span><span class="p">)</span>

  <span class="nd">@wraps</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">docstr</span><span class="o">=</span><span class="n">docstr</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="n">argnums</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">jacfun</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">lu</span><span class="o">.</span><span class="n">wrap_init</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
    <span class="n">f_partial</span><span class="p">,</span> <span class="n">dyn_args</span> <span class="o">=</span> <span class="n">argnums_partial</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">argnums</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span>
                                          <span class="n">require_static_args_hashable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">tree_map</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">_check_input_dtype_jacrev</span><span class="p">,</span> <span class="n">holomorphic</span><span class="p">,</span> <span class="n">allow_int</span><span class="p">),</span> <span class="n">dyn_args</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">has_aux</span><span class="p">:</span>
      <span class="n">y</span><span class="p">,</span> <span class="n">pullback</span> <span class="o">=</span> <span class="n">_vjp</span><span class="p">(</span><span class="n">f_partial</span><span class="p">,</span> <span class="o">*</span><span class="n">dyn_args</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">y</span><span class="p">,</span> <span class="n">pullback</span><span class="p">,</span> <span class="n">aux</span> <span class="o">=</span> <span class="n">_vjp</span><span class="p">(</span><span class="n">f_partial</span><span class="p">,</span> <span class="o">*</span><span class="n">dyn_args</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">tree_map</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">_check_output_dtype_jacrev</span><span class="p">,</span> <span class="n">holomorphic</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">jac</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">pullback</span><span class="p">)(</span><span class="n">_std_basis</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
    <span class="n">jac</span> <span class="o">=</span> <span class="n">jac</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">argnums</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">jac</span>
    <span class="n">example_args</span> <span class="o">=</span> <span class="n">dyn_args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">argnums</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">dyn_args</span>
    <span class="n">jac_tree</span> <span class="o">=</span> <span class="n">tree_map</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">_jacrev_unravel</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">example_args</span><span class="p">,</span> <span class="n">jac</span><span class="p">)</span>
    <span class="n">jac_tree</span> <span class="o">=</span> <span class="n">tree_transpose</span><span class="p">(</span><span class="n">tree_structure</span><span class="p">(</span><span class="n">example_args</span><span class="p">),</span> <span class="n">tree_structure</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">jac_tree</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">has_aux</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">jac_tree</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">jac_tree</span><span class="p">,</span> <span class="n">aux</span>

  <span class="k">return</span> <span class="n">jacfun</span></div>

<span class="n">jacobian</span> <span class="o">=</span> <span class="n">jacrev</span>

<span class="n">_check_input_dtype_jacrev</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">_check_input_dtype_revderiv</span><span class="p">,</span> <span class="s2">&quot;jacrev&quot;</span><span class="p">)</span>
<span class="n">_check_output_dtype_jacrev</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">_check_output_dtype_revderiv</span><span class="p">,</span> <span class="s2">&quot;jacrev&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="hessian">
<a class="viewcode-back" href="../../../_autosummary/jax.hessian.html#jax.hessian">[docs]</a>
<span class="k">def</span> <span class="nf">hessian</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">argnums</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
            <span class="n">has_aux</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">holomorphic</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Hessian of ``fun`` as a dense array.</span>

<span class="sd">  Args:</span>
<span class="sd">    fun: Function whose Hessian is to be computed.  Its arguments at positions</span>
<span class="sd">      specified by ``argnums`` should be arrays, scalars, or standard Python</span>
<span class="sd">      containers thereof. It should return arrays, scalars, or standard Python</span>
<span class="sd">      containers thereof.</span>
<span class="sd">    argnums: Optional, integer or sequence of integers. Specifies which</span>
<span class="sd">      positional argument(s) to differentiate with respect to (default ``0``).</span>
<span class="sd">    has_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the</span>
<span class="sd">      first element is considered the output of the mathematical function to be</span>
<span class="sd">      differentiated and the second element is auxiliary data. Default False.</span>
<span class="sd">    holomorphic: Optional, bool. Indicates whether ``fun`` is promised to be</span>
<span class="sd">      holomorphic. Default False.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A function with the same arguments as ``fun``, that evaluates the Hessian of</span>
<span class="sd">    ``fun``.</span>

<span class="sd">  &gt;&gt;&gt; import jax</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; g = lambda x: x[0]**3 - 2*x[0]*x[1] - x[1]**6</span>
<span class="sd">  &gt;&gt;&gt; print(jax.hessian(g)(jax.numpy.array([1., 2.])))</span>
<span class="sd">  [[   6.   -2.]</span>
<span class="sd">   [  -2. -480.]]</span>

<span class="sd">  :py:func:`hessian` is a generalization of the usual definition of the Hessian</span>
<span class="sd">  that supports nested Python containers (i.e. pytrees) as inputs and outputs.</span>
<span class="sd">  The tree structure of ``jax.hessian(fun)(x)`` is given by forming a tree</span>
<span class="sd">  product of the structure of ``fun(x)`` with a tree product of two copies of</span>
<span class="sd">  the structure of ``x``. A tree product of two tree structures is formed by</span>
<span class="sd">  replacing each leaf of the first tree with a copy of the second. For example:</span>

<span class="sd">  &gt;&gt;&gt; import jax.numpy as jnp</span>
<span class="sd">  &gt;&gt;&gt; f = lambda dct: {&quot;c&quot;: jnp.power(dct[&quot;a&quot;], dct[&quot;b&quot;])}</span>
<span class="sd">  &gt;&gt;&gt; print(jax.hessian(f)({&quot;a&quot;: jnp.arange(2.) + 1., &quot;b&quot;: jnp.arange(2.) + 2.}))</span>
<span class="sd">  {&#39;c&#39;: {&#39;a&#39;: {&#39;a&#39;: Array([[[ 2.,  0.], [ 0.,  0.]],</span>
<span class="sd">                           [[ 0.,  0.], [ 0., 12.]]], dtype=float32),</span>
<span class="sd">               &#39;b&#39;: Array([[[ 1.      ,  0.      ], [ 0.      ,  0.      ]],</span>
<span class="sd">                           [[ 0.      ,  0.      ], [ 0.      , 12.317766]]], dtype=float32)},</span>
<span class="sd">         &#39;b&#39;: {&#39;a&#39;: Array([[[ 1.      ,  0.      ], [ 0.      ,  0.      ]],</span>
<span class="sd">                           [[ 0.      ,  0.      ], [ 0.      , 12.317766]]], dtype=float32),</span>
<span class="sd">               &#39;b&#39;: Array([[[0.      , 0.      ], [0.      , 0.      ]],</span>
<span class="sd">                           [[0.      , 0.      ], [0.      , 3.843624]]], dtype=float32)}}}</span>

<span class="sd">  Thus each leaf in the tree structure of ``jax.hessian(fun)(x)`` corresponds to</span>
<span class="sd">  a leaf of ``fun(x)`` and a pair of leaves of ``x``. For each leaf in</span>
<span class="sd">  ``jax.hessian(fun)(x)``, if the corresponding array leaf of ``fun(x)`` has</span>
<span class="sd">  shape ``(out_1, out_2, ...)`` and the corresponding array leaves of ``x`` have</span>
<span class="sd">  shape ``(in_1_1, in_1_2, ...)`` and ``(in_2_1, in_2_2, ...)`` respectively,</span>
<span class="sd">  then the Hessian leaf has shape ``(out_1, out_2, ..., in_1_1, in_1_2, ...,</span>
<span class="sd">  in_2_1, in_2_2, ...)``. In other words, the Python tree structure represents</span>
<span class="sd">  the block structure of the Hessian, with blocks determined by the input and</span>
<span class="sd">  output pytrees.</span>

<span class="sd">  In particular, an array is produced (with no pytrees involved) when the</span>
<span class="sd">  function input ``x`` and output ``fun(x)`` are each a single array, as in the</span>
<span class="sd">  ``g`` example above. If ``fun(x)`` has shape ``(out1, out2, ...)`` and ``x``</span>
<span class="sd">  has shape ``(in1, in2, ...)`` then ``jax.hessian(fun)(x)`` has shape</span>
<span class="sd">  ``(out1, out2, ..., in1, in2, ..., in1, in2, ...)``. To flatten pytrees into</span>
<span class="sd">  1D vectors, consider using :py:func:`jax.flatten_util.flatten_pytree`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">jacfwd</span><span class="p">(</span><span class="n">jacrev</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">argnums</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="n">has_aux</span><span class="p">,</span> <span class="n">holomorphic</span><span class="o">=</span><span class="n">holomorphic</span><span class="p">),</span>
                <span class="n">argnums</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="n">has_aux</span><span class="p">,</span> <span class="n">holomorphic</span><span class="o">=</span><span class="n">holomorphic</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_std_basis</span><span class="p">(</span><span class="n">pytree</span><span class="p">):</span>
  <span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
  <span class="n">leaves</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">(</span><span class="n">pytree</span><span class="p">)</span>
  <span class="n">ndim</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">leaves</span><span class="p">))</span>
  <span class="n">dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">result_type</span><span class="p">(</span><span class="o">*</span><span class="n">leaves</span><span class="p">)</span>
  <span class="n">flat_basis</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">_unravel_array_into_pytree</span><span class="p">(</span><span class="n">pytree</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">flat_basis</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_jacfwd_unravel</span><span class="p">(</span><span class="n">input_pytree</span><span class="p">,</span> <span class="n">output_pytree_leaf</span><span class="p">,</span> <span class="n">arr</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">_unravel_array_into_pytree</span><span class="p">(</span>
    <span class="n">input_pytree</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_pytree_leaf</span><span class="p">,</span> <span class="n">arr</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_jacrev_unravel</span><span class="p">(</span><span class="n">output_pytree</span><span class="p">,</span> <span class="n">input_pytree_leaf</span><span class="p">,</span> <span class="n">arr</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">_unravel_array_into_pytree</span><span class="p">(</span>
    <span class="n">output_pytree</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">input_pytree_leaf</span><span class="p">,</span> <span class="n">arr</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_possible_downcast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">example</span><span class="p">):</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">dtypes</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">complexfloating</span><span class="p">)</span> <span class="ow">and</span>
      <span class="ow">not</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">_dtype</span><span class="p">(</span><span class="n">example</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">complexfloating</span><span class="p">)):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">real</span>
  <span class="n">dtype</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">example</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">_dtype</span><span class="p">(</span><span class="n">example</span><span class="p">)</span>
  <span class="n">weak_type</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">example</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">is_weakly_typed</span><span class="p">(</span><span class="n">example</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">lax_internal</span><span class="o">.</span><span class="n">_convert_element_type</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">weak_type</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_unravel_array_into_pytree</span><span class="p">(</span><span class="n">pytree</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">example</span><span class="p">,</span> <span class="n">arr</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Unravel an array into a PyTree with a given structure.</span>
<span class="sd">  Args:</span>
<span class="sd">      pytree: The pytree that provides the structure.</span>
<span class="sd">      axis: The parameter axis is either -1, 0, or 1.  It controls the</span>
<span class="sd">        resulting shapes.</span>
<span class="sd">      example: If specified, cast the components to the matching dtype/weak_type,</span>
<span class="sd">        or else use the pytree leaf type if example is None.</span>
<span class="sd">      arr: The array to be unraveled.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">leaves</span><span class="p">,</span> <span class="n">treedef</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">(</span><span class="n">pytree</span><span class="p">)</span>
  <span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span> <span class="o">%</span> <span class="n">arr</span><span class="o">.</span><span class="n">ndim</span>
  <span class="n">shapes</span> <span class="o">=</span> <span class="p">[</span><span class="n">arr</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="n">axis</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="o">+</span> <span class="n">arr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">:]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">leaves</span><span class="p">]</span>
  <span class="n">parts</span> <span class="o">=</span> <span class="n">_split</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">leaves</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])),</span> <span class="n">axis</span><span class="p">)</span>
  <span class="n">reshaped_parts</span> <span class="o">=</span> <span class="p">[</span>
      <span class="n">_possible_downcast</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">shape</span><span class="p">),</span> <span class="n">leaf</span> <span class="k">if</span> <span class="n">example</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">example</span><span class="p">)</span>
      <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">leaf</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">parts</span><span class="p">,</span> <span class="n">shapes</span><span class="p">,</span> <span class="n">leaves</span><span class="p">)]</span>
  <span class="k">return</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">treedef</span><span class="p">,</span> <span class="n">reshaped_parts</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">_split</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>


<div class="viewcode-block" id="vmap">
<a class="viewcode-back" href="../../../_autosummary/jax.vmap.html#jax.vmap">[docs]</a>
<span class="k">def</span> <span class="nf">vmap</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">F</span><span class="p">,</span>
         <span class="n">in_axes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">|</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
         <span class="n">out_axes</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
         <span class="n">axis_name</span><span class="p">:</span> <span class="n">AxisName</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
         <span class="n">axis_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
         <span class="n">spmd_axis_name</span><span class="p">:</span> <span class="n">AxisName</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">AxisName</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
         <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">F</span><span class="p">:</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Vectorizing map. Creates a function which maps ``fun`` over argument axes.</span>

<span class="sd">  Args:</span>
<span class="sd">    fun: Function to be mapped over additional axes.</span>
<span class="sd">    in_axes: An integer, None, or sequence of values specifying which input</span>
<span class="sd">      array axes to map over.</span>

<span class="sd">      If each positional argument to ``fun`` is an array, then ``in_axes`` can</span>
<span class="sd">      be an integer, a None, or a tuple of integers and Nones with length equal</span>
<span class="sd">      to the number of positional arguments to ``fun``. An integer or ``None``</span>
<span class="sd">      indicates which array axis to map over for all arguments (with ``None``</span>
<span class="sd">      indicating not to map any axis), and a tuple indicates which axis to map</span>
<span class="sd">      for each corresponding positional argument. Axis integers must be in the</span>
<span class="sd">      range ``[-ndim, ndim)`` for each array, where ``ndim`` is the number of</span>
<span class="sd">      dimensions (axes) of the corresponding input array.</span>

<span class="sd">      If the positional arguments to ``fun`` are container (pytree) types, ``in_axes``</span>
<span class="sd">      must be a sequence with length equal to the number of positional arguments to</span>
<span class="sd">      ``fun``, and for each argument the corresponding element of ``in_axes`` can</span>
<span class="sd">      be a container with a matching pytree structure specifying the mapping of its</span>
<span class="sd">      container elements. In other words, ``in_axes`` must be a container tree prefix</span>
<span class="sd">      of the positional argument tuple passed to ``fun``. See this link for more detail:</span>
<span class="sd">      https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees</span>

<span class="sd">      Either ``axis_size`` must be provided explicitly, or at least one</span>
<span class="sd">      positional argument must have ``in_axes`` not None. The sizes of the</span>
<span class="sd">      mapped input axes for all mapped positional arguments must all be equal.</span>

<span class="sd">      Arguments passed as keywords are always mapped over their leading axis</span>
<span class="sd">      (i.e. axis index 0).</span>

<span class="sd">      See below for examples.</span>

<span class="sd">    out_axes: An integer, None, or (nested) standard Python container</span>
<span class="sd">      (tuple/list/dict) thereof indicating where the mapped axis should appear</span>
<span class="sd">      in the output. All outputs with a mapped axis must have a non-None</span>
<span class="sd">      ``out_axes`` specification. Axis integers must be in the range ``[-ndim,</span>
<span class="sd">      ndim)`` for each output array, where ``ndim`` is the number of dimensions</span>
<span class="sd">      (axes) of the array returned by the :func:`vmap`-ed function, which is one</span>
<span class="sd">      more than the number of dimensions (axes) of the corresponding array</span>
<span class="sd">      returned by ``fun``.</span>
<span class="sd">    axis_name: Optional, a hashable Python object used to identify the mapped</span>
<span class="sd">      axis so that parallel collectives can be applied.</span>
<span class="sd">    axis_size: Optional, an integer indicating the size of the axis to be</span>
<span class="sd">      mapped. If not provided, the mapped axis size is inferred from arguments.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Batched/vectorized version of ``fun`` with arguments that correspond to</span>
<span class="sd">    those of ``fun``, but with extra array axes at positions indicated by</span>
<span class="sd">    ``in_axes``, and a return value that corresponds to that of ``fun``, but</span>
<span class="sd">    with extra array axes at positions indicated by ``out_axes``.</span>

<span class="sd">  For example, we can implement a matrix-matrix product using a vector dot</span>
<span class="sd">  product:</span>

<span class="sd">  &gt;&gt;&gt; import jax.numpy as jnp</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; vv = lambda x, y: jnp.vdot(x, y)  #  ([a], [a]) -&gt; []</span>
<span class="sd">  &gt;&gt;&gt; mv = vmap(vv, (0, None), 0)      #  ([b,a], [a]) -&gt; [b]      (b is the mapped axis)</span>
<span class="sd">  &gt;&gt;&gt; mm = vmap(mv, (None, 1), 1)      #  ([b,a], [a,c]) -&gt; [b,c]  (c is the mapped axis)</span>

<span class="sd">  Here we use ``[a,b]`` to indicate an array with shape (a,b). Here are some</span>
<span class="sd">  variants:</span>

<span class="sd">  &gt;&gt;&gt; mv1 = vmap(vv, (0, 0), 0)   #  ([b,a], [b,a]) -&gt; [b]        (b is the mapped axis)</span>
<span class="sd">  &gt;&gt;&gt; mv2 = vmap(vv, (0, 1), 0)   #  ([b,a], [a,b]) -&gt; [b]        (b is the mapped axis)</span>
<span class="sd">  &gt;&gt;&gt; mm2 = vmap(mv2, (1, 1), 0)  #  ([b,c,a], [a,c,b]) -&gt; [c,b]  (c is the mapped axis)</span>

<span class="sd">  Here&#39;s an example of using container types in ``in_axes`` to specify which</span>
<span class="sd">  axes of the container elements to map over:</span>

<span class="sd">  &gt;&gt;&gt; A, B, C, D = 2, 3, 4, 5</span>
<span class="sd">  &gt;&gt;&gt; x = jnp.ones((A, B))</span>
<span class="sd">  &gt;&gt;&gt; y = jnp.ones((B, C))</span>
<span class="sd">  &gt;&gt;&gt; z = jnp.ones((C, D))</span>
<span class="sd">  &gt;&gt;&gt; def foo(tree_arg):</span>
<span class="sd">  ...   x, (y, z) = tree_arg</span>
<span class="sd">  ...   return jnp.dot(x, jnp.dot(y, z))</span>
<span class="sd">  &gt;&gt;&gt; tree = (x, (y, z))</span>
<span class="sd">  &gt;&gt;&gt; print(foo(tree))</span>
<span class="sd">  [[12. 12. 12. 12. 12.]</span>
<span class="sd">   [12. 12. 12. 12. 12.]]</span>
<span class="sd">  &gt;&gt;&gt; from jax import vmap</span>
<span class="sd">  &gt;&gt;&gt; K = 6  # batch size</span>
<span class="sd">  &gt;&gt;&gt; x = jnp.ones((K, A, B))  # batch axis in different locations</span>
<span class="sd">  &gt;&gt;&gt; y = jnp.ones((B, K, C))</span>
<span class="sd">  &gt;&gt;&gt; z = jnp.ones((C, D, K))</span>
<span class="sd">  &gt;&gt;&gt; tree = (x, (y, z))</span>
<span class="sd">  &gt;&gt;&gt; vfoo = vmap(foo, in_axes=((0, (1, 2)),))</span>
<span class="sd">  &gt;&gt;&gt; print(vfoo(tree).shape)</span>
<span class="sd">  (6, 2, 5)</span>

<span class="sd">  Here&#39;s another example using container types in ``in_axes``, this time a</span>
<span class="sd">  dictionary, to specify the elements of the container to map over:</span>

<span class="sd">  &gt;&gt;&gt; dct = {&#39;a&#39;: 0., &#39;b&#39;: jnp.arange(5.)}</span>
<span class="sd">  &gt;&gt;&gt; x = 1.</span>
<span class="sd">  &gt;&gt;&gt; def foo(dct, x):</span>
<span class="sd">  ...  return dct[&#39;a&#39;] + dct[&#39;b&#39;] + x</span>
<span class="sd">  &gt;&gt;&gt; out = vmap(foo, in_axes=({&#39;a&#39;: None, &#39;b&#39;: 0}, None))(dct, x)</span>
<span class="sd">  &gt;&gt;&gt; print(out)</span>
<span class="sd">  [1. 2. 3. 4. 5.]</span>

<span class="sd">  The results of a vectorized function can be mapped or unmapped. For example,</span>
<span class="sd">  the function below returns a pair with the first element mapped and the second</span>
<span class="sd">  unmapped. Only for unmapped results we can specify ``out_axes`` to be ``None``</span>
<span class="sd">  (to keep it unmapped).</span>

<span class="sd">  &gt;&gt;&gt; print(vmap(lambda x, y: (x + y, y * 2.), in_axes=(0, None), out_axes=(0, None))(jnp.arange(2.), 4.))</span>
<span class="sd">  (Array([4., 5.], dtype=float32), 8.0)</span>

<span class="sd">  If the ``out_axes`` is specified for an unmapped result, the result is</span>
<span class="sd">  broadcast across the mapped axis:</span>

<span class="sd">  &gt;&gt;&gt; print(vmap(lambda x, y: (x + y, y * 2.), in_axes=(0, None), out_axes=0)(jnp.arange(2.), 4.))</span>
<span class="sd">  (Array([4., 5.], dtype=float32), Array([8., 8.], dtype=float32, weak_type=True))</span>

<span class="sd">  If the ``out_axes`` is specified for a mapped result, the result is transposed</span>
<span class="sd">  accordingly.</span>

<span class="sd">  Finally, here&#39;s an example using ``axis_name`` together with collectives:</span>

<span class="sd">  &gt;&gt;&gt; xs = jnp.arange(3. * 4.).reshape(3, 4)</span>
<span class="sd">  &gt;&gt;&gt; print(vmap(lambda x: lax.psum(x, &#39;i&#39;), axis_name=&#39;i&#39;)(xs))</span>
<span class="sd">  [[12. 15. 18. 21.]</span>
<span class="sd">   [12. 15. 18. 21.]</span>
<span class="sd">   [12. 15. 18. 21.]]</span>

<span class="sd">  See the :py:func:`jax.pmap` docstring for more examples involving collectives.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">check_callable</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="n">docstr</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;Vectorized version of </span><span class="si">{fun}</span><span class="s2">. Takes similar arguments as </span><span class="si">{fun}</span><span class="s2"> &quot;</span>
            <span class="s2">&quot;but with additional array axes over which </span><span class="si">{fun}</span><span class="s2"> is mapped.&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">fun</span><span class="o">.</span><span class="vm">__doc__</span><span class="p">:</span>
    <span class="n">docstr</span> <span class="o">+=</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">Original documentation:</span><span class="se">\n\n</span><span class="s2">&quot;</span>
    <span class="n">docstr</span> <span class="o">+=</span> <span class="n">fun</span><span class="o">.</span><span class="vm">__doc__</span>

  <span class="n">axis_name</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">no_axis_name</span> <span class="k">if</span> <span class="n">axis_name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">axis_name</span>
  <span class="k">if</span> <span class="n">spmd_axis_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="n">spmd_axis_name</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="n">spmd_axis_name</span> <span class="o">=</span> <span class="p">(</span><span class="n">spmd_axis_name</span><span class="p">,)</span>

  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">in_axes</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
    <span class="c1"># To be a tree prefix of the positional args tuple, in_axes can never be a</span>
    <span class="c1"># list: if in_axes is not a leaf, it must be a tuple of trees. However,</span>
    <span class="c1"># in cases like these users expect tuples and lists to be treated</span>
    <span class="c1"># essentially interchangeably, so we canonicalize lists to tuples here</span>
    <span class="c1"># rather than raising an error. https://github.com/google/jax/issues/2367</span>
    <span class="n">in_axes</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">in_axes</span><span class="p">)</span>

  <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">in_axes</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="n">in_axes</span><span class="p">)</span> <span class="ow">in</span> <span class="p">{</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="o">*</span><span class="n">batching</span><span class="o">.</span><span class="n">spec_types</span><span class="p">}):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;vmap in_axes must be an int, None, or a tuple of entries corresponding &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;to the positional arguments passed to the function, but got </span><span class="si">{</span><span class="n">in_axes</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="ow">in</span> <span class="p">{</span><span class="nb">int</span><span class="p">,</span> <span class="o">*</span><span class="n">batching</span><span class="o">.</span><span class="n">spec_types</span><span class="p">}</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">tree_leaves</span><span class="p">(</span><span class="n">in_axes</span><span class="p">)):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;vmap in_axes must be an int, None, or (nested) container &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;with those types as leaves, but got </span><span class="si">{</span><span class="n">in_axes</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="ow">in</span> <span class="p">{</span><span class="nb">int</span><span class="p">,</span> <span class="o">*</span><span class="n">batching</span><span class="o">.</span><span class="n">spec_types</span><span class="p">}</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">tree_leaves</span><span class="p">(</span><span class="n">out_axes</span><span class="p">)):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;vmap out_axes must be an int, None, or (nested) container &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;with those types as leaves, but got </span><span class="si">{</span><span class="n">out_axes</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

  <span class="nd">@wraps</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">docstr</span><span class="o">=</span><span class="n">docstr</span><span class="p">)</span>
  <span class="nd">@api_boundary</span>
  <span class="k">def</span> <span class="nf">vmap_f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">in_axes</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">in_axes</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;vmap in_axes must be an int, None, or a tuple of entries corresponding &quot;</span>
                       <span class="s2">&quot;to the positional arguments passed to the function, &quot;</span>
                       <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">in_axes</span><span class="p">)</span><span class="si">=}</span><span class="s2">, </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span><span class="si">=}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">args_flat</span><span class="p">,</span> <span class="n">in_tree</span>  <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">((</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">),</span> <span class="n">is_leaf</span><span class="o">=</span><span class="n">batching</span><span class="o">.</span><span class="n">is_vmappable</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">lu</span><span class="o">.</span><span class="n">wrap_init</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
    <span class="n">flat_fun</span><span class="p">,</span> <span class="n">out_tree</span> <span class="o">=</span> <span class="n">batching</span><span class="o">.</span><span class="n">flatten_fun_for_vmap</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">)</span>
    <span class="n">in_axes_flat</span> <span class="o">=</span> <span class="n">flatten_axes</span><span class="p">(</span><span class="s2">&quot;vmap in_axes&quot;</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">,</span> <span class="p">(</span><span class="n">in_axes</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">kws</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">axis_size_</span> <span class="o">=</span> <span class="p">(</span><span class="n">axis_size</span> <span class="k">if</span> <span class="n">axis_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span>
                  <span class="n">_mapped_axis_size</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">,</span> <span class="n">args_flat</span><span class="p">,</span> <span class="n">in_axes_flat</span><span class="p">,</span> <span class="s2">&quot;vmap&quot;</span><span class="p">))</span>
    <span class="n">out_flat</span> <span class="o">=</span> <span class="n">batching</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span>
        <span class="n">flat_fun</span><span class="p">,</span> <span class="n">axis_name</span><span class="p">,</span> <span class="n">axis_size_</span><span class="p">,</span> <span class="n">in_axes_flat</span><span class="p">,</span>
        <span class="k">lambda</span><span class="p">:</span> <span class="n">flatten_axes</span><span class="p">(</span><span class="s2">&quot;vmap out_axes&quot;</span><span class="p">,</span> <span class="n">out_tree</span><span class="p">(),</span> <span class="n">out_axes</span><span class="p">),</span>
        <span class="n">spmd_axis_name</span><span class="o">=</span><span class="n">spmd_axis_name</span>
    <span class="p">)</span><span class="o">.</span><span class="n">call_wrapped</span><span class="p">(</span><span class="o">*</span><span class="n">args_flat</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">out_tree</span><span class="p">(),</span> <span class="n">out_flat</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">cast</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="n">vmap_f</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_mapped_axis_size</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">tree</span><span class="p">,</span> <span class="n">vals</span><span class="p">,</span> <span class="n">dims</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">vals</span><span class="p">:</span>
    <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">vals</span><span class="p">)</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> wrapped function must be passed at least one argument &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;containing an array, got empty *args=</span><span class="si">{</span><span class="n">args</span><span class="si">}</span><span class="s2"> and **kwargs=</span><span class="si">{</span><span class="n">kwargs</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>

  <span class="k">def</span> <span class="nf">_get_axis_size</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">shape</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AxisSize</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span>
                     <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">core</span><span class="o">.</span><span class="n">AxisSize</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span>
    <span class="k">except</span> <span class="p">(</span><span class="ne">IndexError</span><span class="p">,</span> <span class="ne">TypeError</span><span class="p">)</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">min_rank</span> <span class="o">=</span> <span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">axis</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="o">-</span><span class="n">axis</span>
      <span class="c1"># TODO(mattjj): better error message here</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> was requested to map its argument along axis </span><span class="si">{</span><span class="n">axis</span><span class="si">}</span><span class="s2">, &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;which implies that its rank should be at least </span><span class="si">{</span><span class="n">min_rank</span><span class="si">}</span><span class="s2">, &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;but is only </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span><span class="si">}</span><span class="s2"> (its shape is </span><span class="si">{</span><span class="n">shape</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>

  <span class="n">sizes</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">dedup_referents</span><span class="p">(</span><span class="n">_get_axis_size</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">d</span><span class="p">)</span>
                               <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">vals</span><span class="p">,</span> <span class="n">dims</span><span class="p">)</span> <span class="k">if</span> <span class="n">d</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sizes</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">sz</span><span class="p">,</span> <span class="o">=</span> <span class="n">sizes</span>
    <span class="k">return</span> <span class="n">sz</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">sizes</span><span class="p">:</span>
    <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> must have at least one non-None value in in_axes&quot;</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_get_argument_type</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">shaped_abstractify</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">str_short</span><span class="p">()</span>
    <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span> <span class="c1">#Catch all for user specified objects that can&#39;t be interpreted as a data type</span>
      <span class="k">return</span> <span class="s2">&quot;unknown&quot;</span>
  <span class="n">msg</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> got inconsistent sizes for array axes to be mapped:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">]</span>
  <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">vals</span><span class="p">)</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">ba</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">except</span> <span class="p">(</span><span class="ne">TypeError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">):</span>
    <span class="n">ba</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="k">if</span> <span class="n">ba</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">args_paths</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;args</span><span class="si">{</span><span class="n">keystr</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="si">}</span><span class="s1"> &#39;</span>
                  <span class="sa">f</span><span class="s1">&#39;of type </span><span class="si">{</span><span class="n">_get_argument_type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span>
                  <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">generate_key_paths</span><span class="p">(</span><span class="n">args</span><span class="p">)]</span>
    <span class="n">kwargs_paths</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;kwargs</span><span class="si">{</span><span class="n">keystr</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="si">}</span><span class="s1"> &#39;</span>
                    <span class="sa">f</span><span class="s1">&#39;of type </span><span class="si">{</span><span class="n">_get_argument_type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span>
                    <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">generate_key_paths</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)]</span>
    <span class="n">key_paths</span> <span class="o">=</span> <span class="p">[</span><span class="o">*</span><span class="n">args_paths</span><span class="p">,</span> <span class="o">*</span><span class="n">kwargs_paths</span><span class="p">]</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">key_paths</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;argument </span><span class="si">{</span><span class="n">name</span><span class="si">}{</span><span class="n">keystr</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="si">}</span><span class="s1"> &#39;</span>
                 <span class="sa">f</span><span class="s1">&#39;of type </span><span class="si">{</span><span class="n">_get_argument_type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span>
                 <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">ba</span><span class="o">.</span><span class="n">arguments</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                 <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">generate_key_paths</span><span class="p">(</span><span class="n">arg</span><span class="p">)]</span>
  <span class="n">all_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="n">_get_axis_size</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">d</span><span class="p">)</span> <span class="k">if</span> <span class="n">d</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
               <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">vals</span><span class="p">,</span> <span class="n">dims</span><span class="p">)]</span>
  <span class="n">size_counts</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">Counter</span><span class="p">(</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">all_sizes</span> <span class="k">if</span> <span class="n">s</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
  <span class="p">(</span><span class="n">sz</span><span class="p">,</span> <span class="n">ct</span><span class="p">),</span> <span class="o">*</span><span class="n">other_counts</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">size_counts</span><span class="o">.</span><span class="n">most_common</span><span class="p">()</span>
  <span class="k">def</span> <span class="nf">_all_sizes_index</span><span class="p">(</span><span class="n">sz</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">isz</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">all_sizes</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">core</span><span class="o">.</span><span class="n">definitely_equal</span><span class="p">(</span><span class="n">isz</span><span class="p">,</span> <span class="n">sz</span><span class="p">):</span> <span class="k">return</span> <span class="n">i</span>
    <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="p">(</span><span class="n">sz</span><span class="p">,</span> <span class="n">all_sizes</span><span class="p">)</span>

  <span class="n">ex</span><span class="p">,</span> <span class="o">*</span><span class="n">examples</span> <span class="o">=</span> <span class="p">(</span><span class="n">key_paths</span><span class="p">[</span><span class="n">_all_sizes_index</span><span class="p">(</span><span class="n">sz</span><span class="p">)]</span> <span class="k">for</span> <span class="n">sz</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">counts</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">,</span> <span class="o">*</span><span class="n">axs</span> <span class="o">=</span> <span class="p">(</span><span class="n">dims</span><span class="p">[</span><span class="n">_all_sizes_index</span><span class="p">(</span><span class="n">sz</span><span class="p">)]</span> <span class="k">for</span> <span class="n">sz</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">counts</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">ct</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">msg</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  * one axis had size </span><span class="si">{</span><span class="n">sz</span><span class="si">}</span><span class="s2">: axis </span><span class="si">{</span><span class="n">ax</span><span class="si">}</span><span class="s2"> of </span><span class="si">{</span><span class="n">ex</span><span class="si">}</span><span class="s2">;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">msg</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  * most axes (</span><span class="si">{</span><span class="n">ct</span><span class="si">}</span><span class="s2"> of them) had size </span><span class="si">{</span><span class="n">sz</span><span class="si">}</span><span class="s2">, e.g. axis </span><span class="si">{</span><span class="n">ax</span><span class="si">}</span><span class="s2"> of </span><span class="si">{</span><span class="n">ex</span><span class="si">}</span><span class="s2">;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">ex</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="p">(</span><span class="n">sz</span><span class="p">,</span> <span class="n">ct</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">examples</span><span class="p">,</span> <span class="n">axs</span><span class="p">,</span> <span class="n">other_counts</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">ct</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="n">msg</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  * one axis had size </span><span class="si">{</span><span class="n">sz</span><span class="si">}</span><span class="s2">: axis </span><span class="si">{</span><span class="n">ax</span><span class="si">}</span><span class="s2"> of </span><span class="si">{</span><span class="n">ex</span><span class="si">}</span><span class="s2">;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">msg</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  * some axes (</span><span class="si">{</span><span class="n">ct</span><span class="si">}</span><span class="s2"> of them) had size </span><span class="si">{</span><span class="n">sz</span><span class="si">}</span><span class="s2">, e.g. axis </span><span class="si">{</span><span class="n">ax</span><span class="si">}</span><span class="s2"> of </span><span class="si">{</span><span class="n">ex</span><span class="si">}</span><span class="s2">;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
  <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">msg</span><span class="p">)[:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>  <span class="c1"># remove last semicolon and newline</span>


<div class="viewcode-block" id="pmap">
<a class="viewcode-back" href="../../../_autosummary/jax.pmap.html#jax.pmap">[docs]</a>
<span class="k">def</span> <span class="nf">pmap</span><span class="p">(</span>
    <span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">axis_name</span><span class="p">:</span> <span class="n">AxisName</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">in_axes</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">out_axes</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">static_broadcasted_argnums</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(),</span>
    <span class="n">devices</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">xc</span><span class="o">.</span><span class="n">Device</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># noqa: F811</span>
    <span class="n">backend</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">axis_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">donate_argnums</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(),</span>
    <span class="n">global_arg_shapes</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="o">...</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Parallel map with support for collective operations.</span>

<span class="sd">  The purpose of :py:func:`pmap` is to express single-program multiple-data</span>
<span class="sd">  (SPMD) programs. Applying :py:func:`pmap` to a function will compile the</span>
<span class="sd">  function with XLA (similarly to :py:func:`jit`), then execute it in parallel</span>
<span class="sd">  on XLA devices, such as multiple GPUs or multiple TPU cores. Semantically it</span>
<span class="sd">  is comparable to :py:func:`vmap` because both transformations map a function</span>
<span class="sd">  over array axes, but where :py:func:`vmap` vectorizes functions by pushing the</span>
<span class="sd">  mapped axis down into primitive operations, :py:func:`pmap` instead replicates</span>
<span class="sd">  the function and executes each replica on its own XLA device in parallel.</span>

<span class="sd">  The mapped axis size must be less than or equal to the number of local XLA</span>
<span class="sd">  devices available, as returned by :py:func:`jax.local_device_count()` (unless</span>
<span class="sd">  ``devices`` is specified, see below). For nested :py:func:`pmap` calls, the</span>
<span class="sd">  product of the mapped axis sizes must be less than or equal to the number of</span>
<span class="sd">  XLA devices.</span>

<span class="sd">  .. note::</span>
<span class="sd">    :py:func:`pmap` compiles ``fun``, so while it can be combined with</span>
<span class="sd">    :py:func:`jit`, it&#39;s usually unnecessary.</span>

<span class="sd">  :py:func:`pmap` requires that all of the participating devices are identical.</span>
<span class="sd">  For example, it is not possible to use :py:func:`pmap` to parallelize a</span>
<span class="sd">  computation across two different models of GPU. It is currently an error for</span>
<span class="sd">  the same device to participate twice in the same `pmap`.</span>

<span class="sd">  **Multi-process platforms:** On multi-process platforms such as TPU pods,</span>
<span class="sd">  :py:func:`pmap` is designed to be used in SPMD Python programs, where every</span>
<span class="sd">  process is running the same Python code such that all processes run the same</span>
<span class="sd">  pmapped function in the same order. Each process should still call the pmapped</span>
<span class="sd">  function with mapped axis size equal to the number of *local* devices (unless</span>
<span class="sd">  ``devices`` is specified, see below), and an array of the same leading axis</span>
<span class="sd">  size will be returned as usual. However, any collective operations in ``fun``</span>
<span class="sd">  will be computed over *all* participating devices, including those on other</span>
<span class="sd">  processes, via device-to-device communication.  Conceptually, this can be</span>
<span class="sd">  thought of as running a pmap over a single array sharded across processes,</span>
<span class="sd">  where each process &quot;sees&quot; only its local shard of the input and output. The</span>
<span class="sd">  SPMD model requires that the same multi-process pmaps must be run in the same</span>
<span class="sd">  order on all devices, but they can be interspersed with arbitrary operations</span>
<span class="sd">  running in a single process.</span>

<span class="sd">  Args:</span>
<span class="sd">    fun: Function to be mapped over argument axes. Its arguments and return</span>
<span class="sd">      value should be arrays, scalars, or (nested) standard Python containers</span>
<span class="sd">      (tuple/list/dict) thereof. Positional arguments indicated by</span>
<span class="sd">      ``static_broadcasted_argnums`` can be anything at all, provided they are</span>
<span class="sd">      hashable and have an equality operation defined.</span>
<span class="sd">    axis_name: Optional, a hashable Python object used to identify the mapped</span>
<span class="sd">      axis so that parallel collectives can be applied.</span>
<span class="sd">    in_axes: A non-negative integer, None, or nested Python container thereof</span>
<span class="sd">      that specifies which axes of positional arguments to map over. Arguments</span>
<span class="sd">      passed as keywords are always mapped over their leading axis (i.e. axis</span>
<span class="sd">      index 0). See :py:func:`vmap` for details.</span>
<span class="sd">    out_axes: A non-negative integer, None, or nested Python container thereof</span>
<span class="sd">      indicating where the mapped axis should appear in the output. All outputs</span>
<span class="sd">      with a mapped axis must have a non-None ``out_axes`` specification</span>
<span class="sd">      (see :py:func:`vmap`).</span>
<span class="sd">    static_broadcasted_argnums: An int or collection of ints specifying which</span>
<span class="sd">      positional arguments to treat as static (compile-time constant).</span>
<span class="sd">      Operations that only depend on static arguments will be constant-folded.</span>
<span class="sd">      Calling the pmapped function with different values for these constants</span>
<span class="sd">      will trigger recompilation. If the pmapped function is called with fewer</span>
<span class="sd">      positional arguments than indicated by ``static_broadcasted_argnums`` then</span>
<span class="sd">      an error is raised. Each of the static arguments will be broadcasted to</span>
<span class="sd">      all devices. Arguments that are not arrays or containers thereof must be</span>
<span class="sd">      marked as static. Defaults to ().</span>

<span class="sd">      Static arguments must be hashable, meaning both ``__hash__`` and</span>
<span class="sd">      ``__eq__`` are implemented, and should be immutable.</span>

<span class="sd">    devices: This is an experimental feature and the API is likely to change.</span>
<span class="sd">      Optional, a sequence of Devices to map over. (Available devices can be</span>
<span class="sd">      retrieved via jax.devices()). Must be given identically for each process</span>
<span class="sd">      in multi-process settings (and will therefore include devices across</span>
<span class="sd">      processes). If specified, the size of the mapped axis must be equal to</span>
<span class="sd">      the number of devices in the sequence local to the given process. Nested</span>
<span class="sd">      :py:func:`pmap` s with ``devices`` specified in either the inner or outer</span>
<span class="sd">      :py:func:`pmap` are not yet supported.</span>
<span class="sd">    backend: This is an experimental feature and the API is likely to change.</span>
<span class="sd">      Optional, a string representing the XLA backend. &#39;cpu&#39;, &#39;gpu&#39;, or &#39;tpu&#39;.</span>
<span class="sd">    axis_size: Optional; the size of the mapped axis.</span>
<span class="sd">    donate_argnums: Specify which positional argument buffers are &quot;donated&quot; to</span>
<span class="sd">      the computation. It is safe to donate argument buffers if you no longer need</span>
<span class="sd">      them once the computation has finished. In some cases XLA can make use of</span>
<span class="sd">      donated buffers to reduce the amount of memory needed to perform a</span>
<span class="sd">      computation, for example recycling one of your input buffers to store a</span>
<span class="sd">      result. You should not reuse buffers that you donate to a computation, JAX</span>
<span class="sd">      will raise an error if you try to.</span>
<span class="sd">      Note that donate_argnums only work for positional arguments, and keyword</span>
<span class="sd">      arguments will not be donated.</span>

<span class="sd">      For more details on buffer donation see the</span>
<span class="sd">      `FAQ &lt;https://jax.readthedocs.io/en/latest/faq.html#buffer-donation&gt;`_.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A parallelized version of ``fun`` with arguments that correspond to those of</span>
<span class="sd">    ``fun`` but with extra array axes at positions indicated by ``in_axes`` and</span>
<span class="sd">    with output that has an additional leading array axis (with the same size).</span>

<span class="sd">  For example, assuming 8 XLA devices are available, :py:func:`pmap` can be used</span>
<span class="sd">  as a map along a leading array axis:</span>

<span class="sd">  &gt;&gt;&gt; import jax.numpy as jnp</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; out = pmap(lambda x: x ** 2)(jnp.arange(8))  # doctest: +SKIP</span>
<span class="sd">  &gt;&gt;&gt; print(out)  # doctest: +SKIP</span>
<span class="sd">  [0, 1, 4, 9, 16, 25, 36, 49]</span>

<span class="sd">  When the leading dimension is smaller than the number of available devices JAX</span>
<span class="sd">  will simply run on a subset of devices:</span>

<span class="sd">  &gt;&gt;&gt; x = jnp.arange(3 * 2 * 2.).reshape((3, 2, 2))</span>
<span class="sd">  &gt;&gt;&gt; y = jnp.arange(3 * 2 * 2.).reshape((3, 2, 2)) ** 2</span>
<span class="sd">  &gt;&gt;&gt; out = pmap(jnp.dot)(x, y)  # doctest: +SKIP</span>
<span class="sd">  &gt;&gt;&gt; print(out)  # doctest: +SKIP</span>
<span class="sd">  [[[    4.     9.]</span>
<span class="sd">    [   12.    29.]]</span>
<span class="sd">   [[  244.   345.]</span>
<span class="sd">    [  348.   493.]]</span>
<span class="sd">   [[ 1412.  1737.]</span>
<span class="sd">    [ 1740.  2141.]]]</span>

<span class="sd">  If your leading dimension is larger than the number of available devices you</span>
<span class="sd">  will get an error:</span>

<span class="sd">  &gt;&gt;&gt; pmap(lambda x: x ** 2)(jnp.arange(9))  # doctest: +SKIP</span>
<span class="sd">  ValueError: ... requires 9 replicas, but only 8 XLA devices are available</span>

<span class="sd">  As with :py:func:`vmap`, using ``None`` in ``in_axes`` indicates that an</span>
<span class="sd">  argument doesn&#39;t have an extra axis and should be broadcasted, rather than</span>
<span class="sd">  mapped, across the replicas:</span>

<span class="sd">  &gt;&gt;&gt; x, y = jnp.arange(2.), 4.</span>
<span class="sd">  &gt;&gt;&gt; out = pmap(lambda x, y: (x + y, y * 2.), in_axes=(0, None))(x, y)  # doctest: +SKIP</span>
<span class="sd">  &gt;&gt;&gt; print(out)  # doctest: +SKIP</span>
<span class="sd">  ([4., 5.], [8., 8.])</span>

<span class="sd">  Note that :py:func:`pmap` always returns values mapped over their leading axis,</span>
<span class="sd">  equivalent to using ``out_axes=0`` in :py:func:`vmap`.</span>

<span class="sd">  In addition to expressing pure maps, :py:func:`pmap` can also be used to express</span>
<span class="sd">  parallel single-program multiple-data (SPMD) programs that communicate via</span>
<span class="sd">  collective operations. For example:</span>

<span class="sd">  &gt;&gt;&gt; f = lambda x: x / jax.lax.psum(x, axis_name=&#39;i&#39;)</span>
<span class="sd">  &gt;&gt;&gt; out = pmap(f, axis_name=&#39;i&#39;)(jnp.arange(4.))  # doctest: +SKIP</span>
<span class="sd">  &gt;&gt;&gt; print(out)  # doctest: +SKIP</span>
<span class="sd">  [ 0.          0.16666667  0.33333334  0.5       ]</span>
<span class="sd">  &gt;&gt;&gt; print(out.sum())  # doctest: +SKIP</span>
<span class="sd">  1.0</span>

<span class="sd">  In this example, ``axis_name`` is a string, but it can be any Python object</span>
<span class="sd">  with ``__hash__`` and ``__eq__`` defined.</span>

<span class="sd">  The argument ``axis_name`` to :py:func:`pmap` names the mapped axis so that</span>
<span class="sd">  collective operations, like :func:`jax.lax.psum`, can refer to it. Axis names</span>
<span class="sd">  are important particularly in the case of nested :py:func:`pmap` functions,</span>
<span class="sd">  where collective operations can operate over distinct axes:</span>

<span class="sd">  &gt;&gt;&gt; from functools import partial</span>
<span class="sd">  &gt;&gt;&gt; import jax</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; @partial(pmap, axis_name=&#39;rows&#39;)</span>
<span class="sd">  ... @partial(pmap, axis_name=&#39;cols&#39;)</span>
<span class="sd">  ... def normalize(x):</span>
<span class="sd">  ...   row_normed = x / jax.lax.psum(x, &#39;rows&#39;)</span>
<span class="sd">  ...   col_normed = x / jax.lax.psum(x, &#39;cols&#39;)</span>
<span class="sd">  ...   doubly_normed = x / jax.lax.psum(x, (&#39;rows&#39;, &#39;cols&#39;))</span>
<span class="sd">  ...   return row_normed, col_normed, doubly_normed</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; x = jnp.arange(8.).reshape((4, 2))</span>
<span class="sd">  &gt;&gt;&gt; row_normed, col_normed, doubly_normed = normalize(x)  # doctest: +SKIP</span>
<span class="sd">  &gt;&gt;&gt; print(row_normed.sum(0))  # doctest: +SKIP</span>
<span class="sd">  [ 1.  1.]</span>
<span class="sd">  &gt;&gt;&gt; print(col_normed.sum(1))  # doctest: +SKIP</span>
<span class="sd">  [ 1.  1.  1.  1.]</span>
<span class="sd">  &gt;&gt;&gt; print(doubly_normed.sum((0, 1)))  # doctest: +SKIP</span>
<span class="sd">  1.0</span>

<span class="sd">  On multi-process platforms, collective operations operate over all devices,</span>
<span class="sd">  including those on other processes. For example, assuming the following code</span>
<span class="sd">  runs on two processes with 4 XLA devices each:</span>

<span class="sd">  &gt;&gt;&gt; f = lambda x: x + jax.lax.psum(x, axis_name=&#39;i&#39;)</span>
<span class="sd">  &gt;&gt;&gt; data = jnp.arange(4) if jax.process_index() == 0 else jnp.arange(4, 8)</span>
<span class="sd">  &gt;&gt;&gt; out = pmap(f, axis_name=&#39;i&#39;)(data)  # doctest: +SKIP</span>
<span class="sd">  &gt;&gt;&gt; print(out)  # doctest: +SKIP</span>
<span class="sd">  [28 29 30 31] # on process 0</span>
<span class="sd">  [32 33 34 35] # on process 1</span>

<span class="sd">  Each process passes in a different length-4 array, corresponding to its 4</span>
<span class="sd">  local devices, and the psum operates over all 8 values. Conceptually, the two</span>
<span class="sd">  length-4 arrays can be thought of as a sharded length-8 array (in this example</span>
<span class="sd">  equivalent to jnp.arange(8)) that is mapped over, with the length-8 mapped</span>
<span class="sd">  axis given name &#39;i&#39;. The pmap call on each process then returns the</span>
<span class="sd">  corresponding length-4 output shard.</span>

<span class="sd">  The ``devices`` argument can be used to specify exactly which devices are used</span>
<span class="sd">  to run the parallel computation. For example, again assuming a single process</span>
<span class="sd">  with 8 devices, the following code defines two parallel computations, one</span>
<span class="sd">  which runs on the first six devices and one on the remaining two:</span>

<span class="sd">  &gt;&gt;&gt; from functools import partial</span>
<span class="sd">  &gt;&gt;&gt; @partial(pmap, axis_name=&#39;i&#39;, devices=jax.devices()[:6])</span>
<span class="sd">  ... def f1(x):</span>
<span class="sd">  ...   return x / jax.lax.psum(x, axis_name=&#39;i&#39;)</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; @partial(pmap, axis_name=&#39;i&#39;, devices=jax.devices()[-2:])</span>
<span class="sd">  ... def f2(x):</span>
<span class="sd">  ...   return jax.lax.psum(x ** 2, axis_name=&#39;i&#39;)</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; print(f1(jnp.arange(6.)))  # doctest: +SKIP</span>
<span class="sd">  [0.         0.06666667 0.13333333 0.2        0.26666667 0.33333333]</span>
<span class="sd">  &gt;&gt;&gt; print(f2(jnp.array([2., 3.])))  # doctest: +SKIP</span>
<span class="sd">  [ 13.  13.]</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">global_arg_shapes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="s2">&quot;global_arg_shapes only worked with sharded_jit which has long been&quot;</span>
        <span class="s2">&quot; removed from JAX. Please migrate to pjit and remove global_arg_shapes&quot;</span>
        <span class="s2">&quot; from pmap.&quot;</span><span class="p">)</span>

  <span class="c1"># TODO(yashkatariya): Move this out after shard_map is out of experimental and</span>
  <span class="c1"># in _src</span>
  <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">pmap_shmap_merge</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">jax.experimental.shard_map</span> <span class="kn">import</span> <span class="n">pmap</span>
    <span class="k">return</span> <span class="n">pmap</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">axis_name</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="n">in_axes</span><span class="p">,</span> <span class="n">out_axes</span><span class="o">=</span><span class="n">out_axes</span><span class="p">,</span>
                <span class="n">static_broadcasted_argnums</span><span class="o">=</span><span class="n">static_broadcasted_argnums</span><span class="p">,</span>
                <span class="n">devices</span><span class="o">=</span><span class="n">devices</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span>
                <span class="n">axis_size</span><span class="o">=</span><span class="n">axis_size</span><span class="p">,</span>
                <span class="n">donate_argnums</span><span class="o">=</span><span class="n">donate_argnums</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">_cpp_pmap</span><span class="p">(</span>
      <span class="n">fun</span><span class="p">,</span>
      <span class="n">axis_name</span><span class="p">,</span>
      <span class="n">in_axes</span><span class="o">=</span><span class="n">in_axes</span><span class="p">,</span>
      <span class="n">out_axes</span><span class="o">=</span><span class="n">out_axes</span><span class="p">,</span>
      <span class="n">static_broadcasted_argnums</span><span class="o">=</span><span class="n">static_broadcasted_argnums</span><span class="p">,</span>
      <span class="n">devices</span><span class="o">=</span><span class="n">devices</span><span class="p">,</span>
      <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span>
      <span class="n">axis_size</span><span class="o">=</span><span class="n">axis_size</span><span class="p">,</span>
      <span class="n">donate_argnums</span><span class="o">=</span><span class="n">donate_argnums</span><span class="p">)</span></div>



<span class="k">class</span> <span class="nc">PmapCallInfo</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
  <span class="n">flat_fun</span><span class="p">:</span> <span class="n">lu</span><span class="o">.</span><span class="n">WrappedFun</span>
  <span class="n">in_tree</span><span class="p">:</span> <span class="n">PyTreeDef</span>
  <span class="n">out_tree</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[],</span> <span class="n">PyTreeDef</span><span class="p">]</span>
  <span class="n">flat_args</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span>
  <span class="n">donated_invars</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span>
  <span class="n">in_axes_flat</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span><span class="p">]</span>
  <span class="n">local_axis_size</span><span class="p">:</span> <span class="nb">int</span>
  <span class="n">out_axes_thunk</span><span class="p">:</span> <span class="n">Callable</span>
  <span class="n">devices</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">xc</span><span class="o">.</span><span class="n">Device</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span>
  <span class="n">global_axis_size</span><span class="p">:</span> <span class="nb">int</span>
  <span class="n">is_explicit_global_axis_size</span><span class="p">:</span> <span class="nb">bool</span>


<span class="k">def</span> <span class="nf">_get_global_axis_size</span><span class="p">(</span><span class="n">local_axis_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">in_devices</span><span class="p">,</span> <span class="n">backend_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                          <span class="n">global_axis_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Determine global_axis_size for multi-host pmap.&quot;&quot;&quot;</span>
  <span class="c1"># TODO(mattjj,skyewm): revive this check (inner_pmap always False now)</span>
  <span class="c1"># if xb.process_count() &gt; 1 and global_axis_size is None and inner_pmap:</span>
  <span class="c1">#   raise ValueError(&quot;&#39;axis_size&#39; must be specified for nested multi-host pmaps&quot;)</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">xb</span><span class="o">.</span><span class="n">process_count</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">global_axis_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span>
      <span class="n">global_axis_size</span> <span class="o">!=</span> <span class="n">local_axis_size</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Specified axis_size </span><span class="si">{</span><span class="n">global_axis_size</span><span class="si">}</span><span class="s2"> doesn&#39;t match received &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;axis_size </span><span class="si">{</span><span class="n">local_axis_size</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">in_devices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">backend_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">backend</span> <span class="o">=</span> <span class="n">xb</span><span class="o">.</span><span class="n">get_device_backend</span><span class="p">(</span><span class="n">in_devices</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">backend</span> <span class="o">=</span> <span class="n">xb</span><span class="o">.</span><span class="n">get_backend</span><span class="p">(</span><span class="n">backend_name</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">global_axis_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">xb</span><span class="o">.</span><span class="n">process_count</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="n">global_axis_size</span> <span class="o">=</span> <span class="n">local_axis_size</span>
    <span class="k">elif</span> <span class="n">in_devices</span><span class="p">:</span>
      <span class="n">global_axis_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">in_devices</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">global_axis_size</span> <span class="o">=</span> <span class="n">local_axis_size</span> <span class="o">*</span> <span class="n">xb</span><span class="o">.</span><span class="n">process_count</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>
      <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span>
          <span class="nb">len</span><span class="p">(</span><span class="n">xb</span><span class="o">.</span><span class="n">local_devices</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span> <span class="n">backend</span><span class="p">))</span> <span class="o">==</span> <span class="n">xb</span><span class="o">.</span><span class="n">local_device_count</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">pi</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">xb</span><span class="o">.</span><span class="n">process_count</span><span class="p">(</span><span class="n">backend</span><span class="p">)))</span>
  <span class="k">return</span> <span class="n">global_axis_size</span>


<span class="k">def</span> <span class="nf">_prepare_pmap</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">in_axes</span><span class="p">,</span> <span class="n">out_axes</span><span class="p">,</span> <span class="n">static_broadcasted_tuple</span><span class="p">,</span>
                  <span class="n">donate_tuple</span><span class="p">,</span> <span class="n">in_devices</span><span class="p">,</span> <span class="n">backend_name</span><span class="p">,</span>
                  <span class="n">axis_size</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">in_devices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">in_devices</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;devices&#39; argument to pmap must be non-empty, or None.&quot;</span><span class="p">)</span>

  <span class="n">src</span> <span class="o">=</span> <span class="n">fun_sourceinfo</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="n">signature</span> <span class="o">=</span> <span class="n">api_util</span><span class="o">.</span><span class="n">fun_signature</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>

  <span class="n">dbg</span> <span class="o">=</span> <span class="n">debug_info</span><span class="p">(</span><span class="s1">&#39;pmap&#39;</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">signature</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span>
                   <span class="n">static_broadcasted_tuple</span><span class="p">,</span> <span class="p">())</span>

  <span class="n">f</span> <span class="o">=</span> <span class="n">lu</span><span class="o">.</span><span class="n">wrap_init</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">static_broadcasted_tuple</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">max</span><span class="p">(</span><span class="n">static_broadcasted_tuple</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="sa">f</span><span class="s2">&quot;pmapped function has static_broadcasted_argnums=</span><span class="si">{</span><span class="n">static_broadcasted_tuple</span><span class="si">}</span><span class="s2">&quot;</span>
          <span class="sa">f</span><span class="s2">&quot; but was called with only </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span><span class="si">}</span><span class="s2"> positional &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;argument</span><span class="si">{</span><span class="s1">&#39;s&#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;&#39;</span><span class="si">}</span><span class="s2">. &quot;</span>
          <span class="s2">&quot;All static broadcasted arguments must be passed positionally.&quot;</span><span class="p">)</span>
    <span class="n">dyn_argnums</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">))</span>
                   <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">static_broadcasted_tuple</span><span class="p">]</span>
    <span class="n">f</span><span class="p">,</span> <span class="n">dyn_args</span> <span class="o">=</span> <span class="n">argnums_partial</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">dyn_argnums</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">in_axes</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
      <span class="n">dyn_in_axes</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">in_axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">dyn_argnums</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">dyn_in_axes</span> <span class="o">=</span> <span class="n">in_axes</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">dyn_args</span><span class="p">,</span> <span class="n">dyn_in_axes</span> <span class="o">=</span> <span class="n">args</span><span class="p">,</span> <span class="n">in_axes</span>
  <span class="n">args</span><span class="p">,</span> <span class="n">in_tree</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">((</span><span class="n">dyn_args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

  <span class="k">if</span> <span class="n">donate_tuple</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">config</span><span class="o">.</span><span class="n">debug_nans</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
    <span class="n">donated_invars</span> <span class="o">=</span> <span class="n">donation_vector</span><span class="p">(</span><span class="n">donate_tuple</span><span class="p">,</span> <span class="p">(),</span> <span class="n">dyn_args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">donated_invars</span> <span class="o">=</span> <span class="p">(</span><span class="kc">False</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">in_axes_flat</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">broadcast_prefix</span><span class="p">((</span><span class="n">dyn_in_axes</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="n">dyn_args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">),</span>
                                          <span class="n">is_leaf</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">))</span>
  <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
    <span class="n">e</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">prefix_errors</span><span class="p">((</span><span class="n">dyn_in_axes</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="n">dyn_args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>
    <span class="n">ex</span> <span class="o">=</span> <span class="n">e</span><span class="p">(</span><span class="s1">&#39;pmap in_axes&#39;</span><span class="p">)</span>
    <span class="n">msg</span><span class="p">,</span> <span class="o">=</span> <span class="n">ex</span><span class="o">.</span><span class="n">args</span>
    <span class="n">msg</span> <span class="o">+=</span> <span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">The &#39;full pytree&#39; here is the tuple of arguments passed &quot;</span>
            <span class="s2">&quot;positionally to the pmapped function, and the value of `in_axes` &quot;</span>
            <span class="s2">&quot;must be a tree prefix of that tuple. But it was not a prefix.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">kwargs</span><span class="p">:</span>
      <span class="n">msg</span> <span class="o">+=</span> <span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">When some arguments are passed by keyword to the pmapped &quot;</span>
              <span class="s2">&quot;function, they are not included in the comparison to `in_axes`. &quot;</span>
              <span class="s2">&quot;Instead, each argument passed by keyword is mapped over its &quot;</span>
              <span class="s2">&quot;leading axis. See the description of `in_axes` in the `pmap` &quot;</span>
              <span class="s2">&quot;docstring: &quot;</span>
              <span class="s2">&quot;https://jax.readthedocs.io/en/latest/_autosummary/jax.pmap.html#jax.pmap&quot;</span><span class="p">)</span>
    <span class="n">msg</span> <span class="o">+=</span> <span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">Check that the value of the `in_axes` argument to `pmap` &quot;</span>
            <span class="s2">&quot;is a tree prefix of the tuple of arguments passed positionally to &quot;</span>
            <span class="s2">&quot;the pmapped function.&quot;</span><span class="p">)</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span> <span class="kn">from</span> <span class="kc">None</span>
  <span class="n">local_axis_size</span> <span class="o">=</span> <span class="n">_mapped_axis_size</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">in_axes_flat</span><span class="p">,</span> <span class="s2">&quot;pmap&quot;</span><span class="p">)</span>

  <span class="n">f</span><span class="p">,</span> <span class="n">res_paths</span> <span class="o">=</span> <span class="n">result_paths</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
  <span class="n">f</span><span class="p">,</span> <span class="n">out_axes_thunk</span> <span class="o">=</span> <span class="n">flat_out_axes</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">out_axes</span><span class="p">)</span>
  <span class="n">flat_fun</span><span class="p">,</span> <span class="n">out_tree</span> <span class="o">=</span> <span class="n">flatten_fun</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">)</span>
  <span class="n">flat_fun</span> <span class="o">=</span> <span class="n">debug_info_final</span><span class="p">(</span><span class="n">flat_fun</span><span class="p">,</span> <span class="n">dbg</span><span class="p">,</span> <span class="n">res_paths</span><span class="p">)</span>

  <span class="n">is_explicit_global_axis_size</span> <span class="o">=</span> <span class="n">axis_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
  <span class="n">global_axis_size</span> <span class="o">=</span> <span class="n">_get_global_axis_size</span><span class="p">(</span><span class="n">local_axis_size</span><span class="p">,</span> <span class="n">in_devices</span><span class="p">,</span>
                                           <span class="n">backend_name</span><span class="p">,</span> <span class="n">axis_size</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">PmapCallInfo</span><span class="p">(</span><span class="n">flat_fun</span><span class="o">=</span><span class="n">flat_fun</span><span class="p">,</span>
                      <span class="n">in_tree</span><span class="o">=</span><span class="n">in_tree</span><span class="p">,</span>
                      <span class="n">out_tree</span><span class="o">=</span><span class="n">out_tree</span><span class="p">,</span>
                      <span class="n">flat_args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>
                      <span class="n">donated_invars</span><span class="o">=</span><span class="n">donated_invars</span><span class="p">,</span>
                      <span class="n">in_axes_flat</span><span class="o">=</span><span class="n">in_axes_flat</span><span class="p">,</span>
                      <span class="n">local_axis_size</span><span class="o">=</span><span class="n">local_axis_size</span><span class="p">,</span>
                      <span class="n">out_axes_thunk</span><span class="o">=</span><span class="n">out_axes_thunk</span><span class="p">,</span>
                      <span class="n">devices</span><span class="o">=</span><span class="kc">None</span> <span class="k">if</span> <span class="n">in_devices</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">in_devices</span><span class="p">),</span>
                      <span class="n">global_axis_size</span><span class="o">=</span><span class="n">global_axis_size</span><span class="p">,</span>
                      <span class="n">is_explicit_global_axis_size</span><span class="o">=</span><span class="n">is_explicit_global_axis_size</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_shared_code_pmap</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">axis_name</span><span class="p">,</span> <span class="n">static_broadcasted_argnums</span><span class="p">,</span>
                      <span class="n">donate_argnums</span><span class="p">,</span> <span class="n">in_axes</span><span class="p">,</span> <span class="n">out_axes</span><span class="p">):</span>
  <span class="c1"># axis_size is an optional integer representing the global axis size.  The</span>
  <span class="c1"># aggregate size (across all processes) size of the mapped axis must match the</span>
  <span class="c1"># given value.</span>
  <span class="n">check_callable</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="n">axis_name</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">_TempAxisName</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span> <span class="k">if</span> <span class="n">axis_name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">axis_name</span>
  <span class="n">static_broadcasted_tuple</span> <span class="o">=</span> <span class="n">_ensure_index_tuple</span><span class="p">(</span><span class="n">static_broadcasted_argnums</span><span class="p">)</span>
  <span class="n">donate_tuple</span> <span class="o">=</span> <span class="n">rebase_donate_argnums</span><span class="p">(</span>
      <span class="n">_ensure_index_tuple</span><span class="p">(</span><span class="n">donate_argnums</span><span class="p">),</span> <span class="n">static_broadcasted_tuple</span><span class="p">)</span>

  <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">int</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">tree_leaves</span><span class="p">(</span><span class="n">in_axes</span><span class="p">)):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;pmap in_axes must be an int, None, or (nested) container &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;with those types as leaves, but got </span><span class="si">{</span><span class="n">in_axes</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">int</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">tree_leaves</span><span class="p">(</span><span class="n">out_axes</span><span class="p">)):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;pmap out_axes must be an int, None, or (nested) container &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;with those types as leaves, but got </span><span class="si">{</span><span class="n">out_axes</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">axis_name</span><span class="p">,</span> <span class="n">static_broadcasted_tuple</span><span class="p">,</span> <span class="n">donate_tuple</span>


<span class="k">class</span> <span class="nc">_PmapFastpathData</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
  <span class="n">version</span><span class="p">:</span> <span class="nb">int</span>  <span class="c1"># For forward and backward compatibility</span>
  <span class="n">xla_executable</span><span class="p">:</span> <span class="n">xc</span><span class="o">.</span><span class="n">LoadedExecutable</span>
  <span class="n">in_handler</span><span class="p">:</span> <span class="n">Any</span>
  <span class="n">out_handler</span><span class="p">:</span> <span class="n">Any</span>
  <span class="n">out_pytree_def</span><span class="p">:</span> <span class="n">Any</span>
  <span class="c1"># Data needed to handle the inputs.</span>
  <span class="n">input_devices</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">xc</span><span class="o">.</span><span class="n">Device</span><span class="p">]</span>
  <span class="n">input_indices</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">sharding_specs</span><span class="o">.</span><span class="n">Index</span><span class="p">]</span>
  <span class="n">input_array_shardings</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span>
  <span class="c1"># Data needed to build the Array from C++.</span>
  <span class="n">out_avals</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span>
  <span class="n">out_array_shardings</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span>
  <span class="n">out_committed</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">_cpp_pmap</span><span class="p">(</span>
    <span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">axis_name</span><span class="p">:</span> <span class="n">AxisName</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">in_axes</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">out_axes</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">static_broadcasted_argnums</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(),</span>
    <span class="n">devices</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">xc</span><span class="o">.</span><span class="n">Device</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># noqa: F811</span>
    <span class="n">backend</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">axis_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">donate_argnums</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(),</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
  <span class="n">axis_name</span><span class="p">,</span> <span class="n">static_broadcasted_tuple</span><span class="p">,</span> <span class="n">donate_tuple</span> <span class="o">=</span> <span class="n">_shared_code_pmap</span><span class="p">(</span>
      <span class="n">fun</span><span class="p">,</span> <span class="n">axis_name</span><span class="p">,</span> <span class="n">static_broadcasted_argnums</span><span class="p">,</span> <span class="n">donate_argnums</span><span class="p">,</span> <span class="n">in_axes</span><span class="p">,</span>
      <span class="n">out_axes</span><span class="p">)</span>
  <span class="k">del</span> <span class="n">static_broadcasted_argnums</span><span class="p">,</span> <span class="n">donate_argnums</span>

  <span class="nd">@api_boundary</span>
  <span class="k">def</span> <span class="nf">cache_miss</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">_prepare_pmap</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">in_axes</span><span class="p">,</span> <span class="n">out_axes</span><span class="p">,</span> <span class="n">static_broadcasted_tuple</span><span class="p">,</span>
                      <span class="n">donate_tuple</span><span class="p">,</span> <span class="n">devices</span><span class="p">,</span> <span class="n">backend</span><span class="p">,</span>
                      <span class="n">axis_size</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">flat_args</span><span class="p">:</span>
      <span class="n">dispatch</span><span class="o">.</span><span class="n">check_arg</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span>

    <span class="n">params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span>
        <span class="n">axis_name</span><span class="o">=</span><span class="n">axis_name</span><span class="p">,</span>
        <span class="n">axis_size</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">local_axis_size</span><span class="p">,</span>
        <span class="n">global_axis_size</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">global_axis_size</span><span class="p">,</span>
        <span class="n">devices</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">devices</span><span class="p">,</span>
        <span class="n">in_axes</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">in_axes_flat</span><span class="p">,</span>
        <span class="n">out_axes_thunk</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">out_axes_thunk</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">flat_fun</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span>
        <span class="n">donated_invars</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">donated_invars</span><span class="p">,</span>
        <span class="n">is_explicit_global_axis_size</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">is_explicit_global_axis_size</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">map_bind_continuation</span><span class="p">,</span> <span class="n">top_trace</span><span class="p">,</span> <span class="n">fun_</span><span class="p">,</span> <span class="n">tracers</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">core</span><span class="o">.</span><span class="n">map_bind_with_continuation</span><span class="p">(</span><span class="n">pxla</span><span class="o">.</span><span class="n">xla_pmap_p</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">flat_fun</span><span class="p">,</span>
                                        <span class="o">*</span><span class="n">p</span><span class="o">.</span><span class="n">flat_args</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">))</span>
    <span class="n">execute</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">top_trace</span><span class="p">,</span> <span class="n">core</span><span class="o">.</span><span class="n">EvalTrace</span><span class="p">):</span>
      <span class="n">execute</span> <span class="o">=</span> <span class="n">pxla</span><span class="o">.</span><span class="n">xla_pmap_impl_lazy</span><span class="p">(</span><span class="n">fun_</span><span class="p">,</span> <span class="o">*</span><span class="n">tracers</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span>
      <span class="n">out</span> <span class="o">=</span> <span class="n">map_bind_continuation</span><span class="p">(</span><span class="n">execute</span><span class="p">(</span><span class="o">*</span><span class="n">tracers</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">out</span> <span class="o">=</span> <span class="n">map_bind_continuation</span><span class="p">(</span>
          <span class="n">pxla</span><span class="o">.</span><span class="n">xla_pmap_p</span><span class="o">.</span><span class="n">process</span><span class="p">(</span><span class="n">top_trace</span><span class="p">,</span> <span class="n">fun_</span><span class="p">,</span> <span class="n">tracers</span><span class="p">,</span> <span class="n">params</span><span class="p">))</span>

    <span class="n">out_tree</span><span class="p">,</span> <span class="n">out_flat</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">out_tree</span><span class="p">,</span> <span class="n">out</span>
    <span class="n">out_pytree_def</span> <span class="o">=</span> <span class="n">out_tree</span><span class="p">()</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">out_pytree_def</span><span class="p">,</span> <span class="n">out_flat</span><span class="p">)</span>

    <span class="c1">### Decide whether we can support the C++ fast path</span>
    <span class="n">use_fastpath</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">execute</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">execute</span><span class="p">,</span> <span class="n">pxla</span><span class="o">.</span><span class="n">ExecuteReplicated</span><span class="p">):</span>
      <span class="n">execute_replicated</span> <span class="o">=</span> <span class="n">typing</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">pxla</span><span class="o">.</span><span class="n">ExecuteReplicated</span><span class="p">,</span> <span class="n">execute</span><span class="p">)</span>
      <span class="n">use_fastpath</span> <span class="o">=</span> <span class="p">(</span>
        <span class="c1"># TODO(sharadmv): Enable effects in replicated computation</span>
        <span class="ow">not</span> <span class="n">execute_replicated</span><span class="o">.</span><span class="n">has_unordered_effects</span>
        <span class="ow">and</span> <span class="ow">not</span> <span class="n">execute_replicated</span><span class="o">.</span><span class="n">has_host_callbacks</span> <span class="ow">and</span>
        <span class="c1"># No tracers in the outputs.</span>
        <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">xc</span><span class="o">.</span><span class="n">ArrayImpl</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">out_flat</span><span class="p">))</span>

    <span class="c1">### If we can use the fastpath, we return required info to the caller.</span>
    <span class="k">if</span> <span class="n">use_fastpath</span><span class="p">:</span>
      <span class="n">execute_replicated</span> <span class="o">=</span> <span class="n">typing</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">pxla</span><span class="o">.</span><span class="n">ExecuteReplicated</span><span class="p">,</span> <span class="n">execute</span><span class="p">)</span>
      <span class="n">out_handler</span> <span class="o">=</span> <span class="n">execute_replicated</span><span class="o">.</span><span class="n">out_handler</span>
      <span class="n">in_handler</span> <span class="o">=</span> <span class="n">execute_replicated</span><span class="o">.</span><span class="n">in_handler</span>

      <span class="n">out_array_shardings</span> <span class="o">=</span> <span class="p">[</span><span class="n">out</span><span class="o">.</span><span class="n">sharding</span> <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">out_flat</span><span class="p">]</span>
      <span class="n">out_committed</span> <span class="o">=</span> <span class="p">[</span><span class="n">out</span><span class="o">.</span><span class="n">_committed</span> <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">out_flat</span><span class="p">]</span>
      <span class="n">fastpath_data</span> <span class="o">=</span> <span class="n">_PmapFastpathData</span><span class="p">(</span>
          <span class="n">version</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
          <span class="n">xla_executable</span><span class="o">=</span><span class="n">execute_replicated</span><span class="o">.</span><span class="n">xla_executable</span><span class="p">,</span>
          <span class="n">in_handler</span><span class="o">=</span><span class="n">in_handler</span><span class="p">,</span>
          <span class="n">out_handler</span><span class="o">=</span><span class="n">out_handler</span><span class="p">,</span>
          <span class="n">out_pytree_def</span><span class="o">=</span><span class="n">out_pytree_def</span><span class="p">,</span>
          <span class="n">input_devices</span><span class="o">=</span><span class="n">in_handler</span><span class="o">.</span><span class="n">local_devices</span><span class="p">,</span>
          <span class="n">input_indices</span><span class="o">=</span><span class="n">in_handler</span><span class="o">.</span><span class="n">input_indices</span><span class="p">,</span>
          <span class="n">input_array_shardings</span><span class="o">=</span><span class="n">in_handler</span><span class="o">.</span><span class="n">in_shardings</span><span class="p">,</span>
          <span class="n">out_avals</span><span class="o">=</span><span class="n">out_handler</span><span class="o">.</span><span class="n">out_avals</span><span class="p">,</span>
          <span class="n">out_array_shardings</span><span class="o">=</span><span class="n">out_array_shardings</span><span class="p">,</span>
          <span class="n">out_committed</span><span class="o">=</span><span class="n">out_committed</span><span class="p">,</span>
      <span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
      <span class="n">fastpath_data</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">fastpath_data</span>

  <span class="n">cpp_mapped_f</span> <span class="o">=</span> <span class="n">pmap_lib</span><span class="o">.</span><span class="n">pmap</span><span class="p">(</span>
      <span class="n">fun</span><span class="p">,</span> <span class="n">cache_miss</span><span class="p">,</span> <span class="n">static_broadcasted_tuple</span><span class="p">,</span>
      <span class="n">pxla</span><span class="o">.</span><span class="n">shard_arg</span> <span class="k">if</span> <span class="n">xla_extension_version</span> <span class="o">&gt;=</span> <span class="mi">229</span> <span class="k">else</span> <span class="n">pxla</span><span class="o">.</span><span class="n">temp_shard_arg</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
      <span class="n">pytree_registry</span><span class="o">=</span><span class="n">tree_util</span><span class="o">.</span><span class="n">default_registry</span><span class="p">)</span>
  <span class="n">_pmap_cache_clears</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">cpp_mapped_f</span><span class="p">)</span>

  <span class="n">pmap_f</span> <span class="o">=</span> <span class="n">wraps</span><span class="p">(</span><span class="n">fun</span><span class="p">)(</span><span class="n">cpp_mapped_f</span><span class="p">)</span>

  <span class="n">pmap_f</span><span class="o">.</span><span class="n">lower</span> <span class="o">=</span> <span class="n">_pmap_lower</span><span class="p">(</span>
      <span class="n">fun</span><span class="p">,</span> <span class="n">axis_name</span><span class="p">,</span> <span class="n">in_axes</span><span class="p">,</span> <span class="n">out_axes</span><span class="p">,</span> <span class="n">static_broadcasted_tuple</span><span class="p">,</span> <span class="n">devices</span><span class="p">,</span>
      <span class="n">backend</span><span class="p">,</span> <span class="n">axis_size</span><span class="p">,</span> <span class="n">donate_tuple</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">pmap_f</span>

<span class="n">_pmap_cache_clears</span> <span class="o">=</span> <span class="n">weakref</span><span class="o">.</span><span class="n">WeakSet</span><span class="p">()</span>  <span class="c1"># type: ignore</span>


<span class="k">def</span> <span class="nf">_pmap_lower</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">axis_name</span><span class="p">,</span> <span class="n">in_axes</span><span class="p">,</span> <span class="n">out_axes</span><span class="p">,</span> <span class="n">static_broadcasted_tuple</span><span class="p">,</span>
                <span class="n">devices</span><span class="p">,</span> <span class="n">backend</span><span class="p">,</span> <span class="n">axis_size</span><span class="p">,</span> <span class="n">donate_tuple</span><span class="p">):</span>  <span class="c1"># noqa: F811</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Make a ``lower`` method for pmapped functions.&quot;&quot;&quot;</span>
  <span class="c1"># If the function we returned from ``pmap`` were a class instance,</span>
  <span class="c1"># this might naturally be a method, with ``fun`` as a ``self`` and</span>
  <span class="c1"># all the other arguments stored as attributes.</span>
  <span class="nd">@api_boundary</span>
  <span class="k">def</span> <span class="nf">lower</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">stages</span><span class="o">.</span><span class="n">Lowered</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Lower a parallel-mapped form of this function for the given arguments.</span>

<span class="sd">    A parallel-mapped and lowered function is staged out of Python and</span>
<span class="sd">    translated to a compiler&#39;s input language, possibly in a</span>
<span class="sd">    backend-dependent manner. It is ready for compilation but is not yet</span>
<span class="sd">    compiled. It represents a function intended for SPMD execution on</span>
<span class="sd">    multiple devices.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A ``Lowered`` instance representing the post-map lowering.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">lowering_parameters</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span>
        <span class="s1">&#39;_experimental_lowering_parameters&#39;</span><span class="p">,</span> <span class="n">mlir</span><span class="o">.</span><span class="n">LoweringParameters</span><span class="p">())</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">_prepare_pmap</span><span class="p">(</span>
        <span class="n">fun</span><span class="p">,</span> <span class="n">in_axes</span><span class="p">,</span> <span class="n">out_axes</span><span class="p">,</span> <span class="n">static_broadcasted_tuple</span><span class="p">,</span> <span class="n">donate_tuple</span><span class="p">,</span>
        <span class="n">devices</span><span class="p">,</span> <span class="n">backend</span><span class="p">,</span> <span class="n">axis_size</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
    <span class="n">abstract_args</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">shaped_abstractify</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">flat_args</span><span class="p">))</span>
    <span class="n">computation</span> <span class="o">=</span> <span class="n">pxla</span><span class="o">.</span><span class="n">lower_parallel_callable</span><span class="p">(</span>
        <span class="n">p</span><span class="o">.</span><span class="n">flat_fun</span><span class="p">,</span> <span class="n">backend</span><span class="p">,</span> <span class="n">axis_name</span><span class="p">,</span>
        <span class="n">axis_size</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">local_axis_size</span><span class="p">,</span> <span class="n">global_axis_size</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">global_axis_size</span><span class="p">,</span>
        <span class="n">devices</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">devices</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">flat_fun</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span>
        <span class="n">in_axes</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">in_axes_flat</span><span class="p">,</span>
        <span class="n">out_axes_thunk</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">out_axes_thunk</span><span class="p">,</span>
        <span class="n">donated_invars</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">donated_invars</span><span class="p">,</span>
        <span class="n">is_explicit_global_axis_size</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">is_explicit_global_axis_size</span><span class="p">,</span>
        <span class="n">avals</span><span class="o">=</span><span class="n">abstract_args</span><span class="p">,</span>
        <span class="n">lowering_parameters</span><span class="o">=</span><span class="n">lowering_parameters</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">stages</span><span class="o">.</span><span class="n">Lowered</span><span class="o">.</span><span class="n">from_flat_info</span><span class="p">(</span>
        <span class="n">computation</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">in_tree</span><span class="p">,</span> <span class="n">abstract_args</span><span class="p">,</span> <span class="n">donate_tuple</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">out_tree</span><span class="p">())</span>

  <span class="k">return</span> <span class="n">lower</span>

<div class="viewcode-block" id="jvp">
<a class="viewcode-back" href="../../../_autosummary/jax.jvp.html#jax.jvp">[docs]</a>
<span class="k">def</span> <span class="nf">jvp</span><span class="p">(</span>
    <span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">primals</span><span class="p">,</span> <span class="n">tangents</span><span class="p">,</span> <span class="n">has_aux</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Computes a (forward-mode) Jacobian-vector product of ``fun``.</span>

<span class="sd">  Args:</span>
<span class="sd">    fun: Function to be differentiated. Its arguments should be arrays, scalars,</span>
<span class="sd">      or standard Python containers of arrays or scalars. It should return an</span>
<span class="sd">      array, scalar, or standard Python container of arrays or scalars.</span>
<span class="sd">    primals: The primal values at which the Jacobian of ``fun`` should be</span>
<span class="sd">      evaluated. Should be either a tuple or a list of arguments,</span>
<span class="sd">      and its length should be equal to the number of positional parameters of</span>
<span class="sd">      ``fun``.</span>
<span class="sd">    tangents: The tangent vector for which the Jacobian-vector product should be</span>
<span class="sd">      evaluated. Should be either a tuple or a list of tangents, with the same</span>
<span class="sd">      tree structure and array shapes as ``primals``.</span>
<span class="sd">    has_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the</span>
<span class="sd">     first element is considered the output of the mathematical function to be</span>
<span class="sd">     differentiated and the second element is auxiliary data. Default False.</span>

<span class="sd">  Returns:</span>
<span class="sd">    If ``has_aux`` is ``False``, returns a ``(primals_out, tangents_out)`` pair,</span>
<span class="sd">    where ``primals_out`` is ``fun(*primals)``,</span>
<span class="sd">    and ``tangents_out`` is the Jacobian-vector product of</span>
<span class="sd">    ``function`` evaluated at ``primals`` with ``tangents``. The</span>
<span class="sd">    ``tangents_out`` value has the same Python tree structure and shapes as</span>
<span class="sd">    ``primals_out``. If ``has_aux`` is ``True``, returns a</span>
<span class="sd">    ``(primals_out, tangents_out, aux)`` tuple where ``aux``</span>
<span class="sd">    is the auxiliary data returned by ``fun``.</span>

<span class="sd">  For example:</span>

<span class="sd">  &gt;&gt;&gt; import jax</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; primals, tangents = jax.jvp(jax.numpy.sin, (0.1,), (0.2,))</span>
<span class="sd">  &gt;&gt;&gt; print(primals)</span>
<span class="sd">  0.09983342</span>
<span class="sd">  &gt;&gt;&gt; print(tangents)</span>
<span class="sd">  0.19900084</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">check_callable</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">_jvp</span><span class="p">(</span><span class="n">lu</span><span class="o">.</span><span class="n">wrap_init</span><span class="p">(</span><span class="n">fun</span><span class="p">),</span> <span class="n">primals</span><span class="p">,</span> <span class="n">tangents</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="n">has_aux</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_jvp</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">lu</span><span class="o">.</span><span class="n">WrappedFun</span><span class="p">,</span> <span class="n">primals</span><span class="p">,</span> <span class="n">tangents</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Variant of jvp() that takes an lu.WrappedFun.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">primals</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span> <span class="ow">or</span>
      <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tangents</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;primal and tangent arguments to jax.jvp must be tuples or lists; &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;found </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">primals</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">tangents</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

  <span class="n">ps_flat</span><span class="p">,</span> <span class="n">tree_def</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">(</span><span class="n">primals</span><span class="p">)</span>
  <span class="n">ts_flat</span><span class="p">,</span> <span class="n">tree_def_2</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">(</span><span class="n">tangents</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">tree_def</span> <span class="o">!=</span> <span class="n">tree_def_2</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;primal and tangent arguments to jax.jvp must have the same tree &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;structure; primals have tree structure </span><span class="si">{</span><span class="n">tree_def</span><span class="si">}</span><span class="s2"> whereas tangents have &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;tree structure </span><span class="si">{</span><span class="n">tree_def_2</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span><span class="n">ps_flat</span><span class="p">,</span> <span class="n">ts_flat</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">core</span><span class="o">.</span><span class="n">primal_dtype_to_tangent_dtype</span><span class="p">(</span><span class="n">_dtype</span><span class="p">(</span><span class="n">p</span><span class="p">))</span> <span class="o">!=</span> <span class="n">_dtype</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;primal and tangent arguments to jax.jvp do not match; &quot;</span>
                      <span class="s2">&quot;dtypes must be equal, or in case of int/bool primal dtype &quot;</span>
                      <span class="s2">&quot;the tangent dtype must be float0.&quot;</span>
                      <span class="sa">f</span><span class="s2">&quot;Got primal dtype </span><span class="si">{</span><span class="n">_dtype</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="si">}</span><span class="s2"> and so expected tangent dtype &quot;</span>
                      <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">core</span><span class="o">.</span><span class="n">primal_dtype_to_tangent_dtype</span><span class="p">(</span><span class="n">_dtype</span><span class="p">(</span><span class="n">p</span><span class="p">))</span><span class="si">}</span><span class="s2">, but got &quot;</span>
                      <span class="sa">f</span><span class="s2">&quot;tangent dtype </span><span class="si">{</span><span class="n">_dtype</span><span class="p">(</span><span class="n">t</span><span class="p">)</span><span class="si">}</span><span class="s2"> instead.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">!=</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;jvp called with different primal and tangent shapes;&quot;</span>
                       <span class="sa">f</span><span class="s2">&quot;Got primal shape </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="si">}</span><span class="s2"> and tangent shape as </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">t</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

  <span class="k">if</span> <span class="ow">not</span> <span class="n">has_aux</span><span class="p">:</span>
    <span class="n">flat_fun</span><span class="p">,</span> <span class="n">out_tree</span> <span class="o">=</span> <span class="n">flatten_fun_nokwargs</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">tree_def</span><span class="p">)</span>
    <span class="n">out_primals</span><span class="p">,</span> <span class="n">out_tangents</span> <span class="o">=</span> <span class="n">ad</span><span class="o">.</span><span class="n">jvp</span><span class="p">(</span><span class="n">flat_fun</span><span class="p">)</span><span class="o">.</span><span class="n">call_wrapped</span><span class="p">(</span><span class="n">ps_flat</span><span class="p">,</span> <span class="n">ts_flat</span><span class="p">)</span>
    <span class="n">out_tree</span> <span class="o">=</span> <span class="n">out_tree</span><span class="p">()</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">tree_unflatten</span><span class="p">(</span><span class="n">out_tree</span><span class="p">,</span> <span class="n">out_primals</span><span class="p">),</span>
            <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">out_tree</span><span class="p">,</span> <span class="n">out_tangents</span><span class="p">))</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">flat_fun</span><span class="p">,</span> <span class="n">out_aux_trees</span> <span class="o">=</span> <span class="n">flatten_fun_nokwargs2</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">tree_def</span><span class="p">)</span>
    <span class="n">jvp_fun</span><span class="p">,</span> <span class="n">aux</span> <span class="o">=</span> <span class="n">ad</span><span class="o">.</span><span class="n">jvp</span><span class="p">(</span><span class="n">flat_fun</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">out_primals</span><span class="p">,</span> <span class="n">out_tangents</span> <span class="o">=</span> <span class="n">jvp_fun</span><span class="o">.</span><span class="n">call_wrapped</span><span class="p">(</span><span class="n">ps_flat</span><span class="p">,</span> <span class="n">ts_flat</span><span class="p">)</span>
    <span class="n">out_tree</span><span class="p">,</span> <span class="n">aux_tree</span> <span class="o">=</span> <span class="n">out_aux_trees</span><span class="p">()</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">tree_unflatten</span><span class="p">(</span><span class="n">out_tree</span><span class="p">,</span> <span class="n">out_primals</span><span class="p">),</span>
            <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">out_tree</span><span class="p">,</span> <span class="n">out_tangents</span><span class="p">),</span>
            <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">aux_tree</span><span class="p">,</span> <span class="n">aux</span><span class="p">()))</span>

<span class="nd">@overload</span>
<span class="k">def</span> <span class="nf">linearize</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="o">*</span><span class="n">primals</span><span class="p">,</span> <span class="n">has_aux</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="kc">False</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
              <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]:</span>
  <span class="o">...</span>

<span class="nd">@overload</span>
<span class="k">def</span> <span class="nf">linearize</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="o">*</span><span class="n">primals</span><span class="p">,</span> <span class="n">has_aux</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="kc">True</span><span class="p">]</span>
              <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
  <span class="o">...</span>

<div class="viewcode-block" id="linearize">
<a class="viewcode-back" href="../../../_autosummary/jax.linearize.html#jax.linearize">[docs]</a>
<span class="k">def</span> <span class="nf">linearize</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="o">*</span><span class="n">primals</span><span class="p">,</span> <span class="n">has_aux</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
              <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Produces a linear approximation to ``fun`` using :py:func:`jvp` and partial eval.</span>

<span class="sd">  Args:</span>
<span class="sd">    fun: Function to be differentiated. Its arguments should be arrays, scalars,</span>
<span class="sd">      or standard Python containers of arrays or scalars. It should return an</span>
<span class="sd">      array, scalar, or standard python container of arrays or scalars.</span>
<span class="sd">    primals: The primal values at which the Jacobian of ``fun`` should be</span>
<span class="sd">      evaluated. Should be a tuple of arrays, scalar, or standard Python</span>
<span class="sd">      container thereof. The length of the tuple is equal to the number of</span>
<span class="sd">      positional parameters of ``fun``.</span>
<span class="sd">    has_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the first</span>
<span class="sd">      element is considered the output of the mathematical function to be linearized,</span>
<span class="sd">      and the second is auxiliary data. Default False.</span>

<span class="sd">  Returns:</span>
<span class="sd">    If ``has_aux`` is ``False``, returns a pair where the first element is the value of</span>
<span class="sd">    ``f(*primals)`` and the second element is a function that evaluates the</span>
<span class="sd">    (forward-mode) Jacobian-vector product of ``fun`` evaluated at ``primals`` without</span>
<span class="sd">    re-doing the linearization work. If ``has_aux`` is ``True``, returns a</span>
<span class="sd">    ``(primals_out, lin_fn, aux)`` tuple where ``aux`` is the auxiliary data returned by</span>
<span class="sd">    ``fun``.</span>

<span class="sd">  In terms of values computed, :py:func:`linearize` behaves much like a curried</span>
<span class="sd">  :py:func:`jvp`, where these two code blocks compute the same values::</span>

<span class="sd">    y, out_tangent = jax.jvp(f, (x,), (in_tangent,))</span>

<span class="sd">    y, f_jvp = jax.linearize(f, x)</span>
<span class="sd">    out_tangent = f_jvp(in_tangent)</span>

<span class="sd">  However, the difference is that :py:func:`linearize` uses partial evaluation</span>
<span class="sd">  so that the function ``f`` is not re-linearized on calls to ``f_jvp``. In</span>
<span class="sd">  general that means the memory usage scales with the size of the computation,</span>
<span class="sd">  much like in reverse-mode. (Indeed, :py:func:`linearize` has a similar</span>
<span class="sd">  signature to :py:func:`vjp`!)</span>

<span class="sd">  This function is mainly useful if you want to apply ``f_jvp`` multiple times,</span>
<span class="sd">  i.e. to evaluate a pushforward for many different input tangent vectors at the</span>
<span class="sd">  same linearization point. Moreover if all the input tangent vectors are known</span>
<span class="sd">  at once, it can be more efficient to vectorize using :py:func:`vmap`, as in::</span>

<span class="sd">    pushfwd = partial(jvp, f, (x,))</span>
<span class="sd">    y, out_tangents = vmap(pushfwd, out_axes=(None, 0))((in_tangents,))</span>

<span class="sd">  By using :py:func:`vmap` and :py:func:`jvp` together like this we avoid the stored-linearization</span>
<span class="sd">  memory cost that scales with the depth of the computation, which is incurred</span>
<span class="sd">  by both :py:func:`linearize` and :py:func:`vjp`.</span>

<span class="sd">  Here&#39;s a more complete example of using :py:func:`linearize`:</span>

<span class="sd">  &gt;&gt;&gt; import jax</span>
<span class="sd">  &gt;&gt;&gt; import jax.numpy as jnp</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; def f(x): return 3. * jnp.sin(x) + jnp.cos(x / 2.)</span>
<span class="sd">  ...</span>
<span class="sd">  &gt;&gt;&gt; jax.jvp(f, (2.,), (3.,))</span>
<span class="sd">  (Array(3.26819, dtype=float32, weak_type=True), Array(-5.00753, dtype=float32, weak_type=True))</span>
<span class="sd">  &gt;&gt;&gt; y, f_jvp = jax.linearize(f, 2.)</span>
<span class="sd">  &gt;&gt;&gt; print(y)</span>
<span class="sd">  3.2681944</span>
<span class="sd">  &gt;&gt;&gt; print(f_jvp(3.))</span>
<span class="sd">  -5.007528</span>
<span class="sd">  &gt;&gt;&gt; print(f_jvp(4.))</span>
<span class="sd">  -6.676704</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">check_callable</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="n">f</span> <span class="o">=</span> <span class="n">lu</span><span class="o">.</span><span class="n">wrap_init</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="n">primals_flat</span><span class="p">,</span> <span class="n">in_tree</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">(</span><span class="n">primals</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">has_aux</span><span class="p">:</span>
    <span class="n">jaxtree_fun</span><span class="p">,</span> <span class="n">out_tree</span> <span class="o">=</span> <span class="n">flatten_fun_nokwargs2</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">jaxtree_fun</span><span class="p">,</span> <span class="n">out_tree</span> <span class="o">=</span> <span class="n">flatten_fun_nokwargs</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">)</span>
  <span class="n">out_primals</span><span class="p">,</span> <span class="n">out_pvals</span><span class="p">,</span> <span class="n">jaxpr</span><span class="p">,</span> <span class="n">consts</span><span class="p">,</span> <span class="o">*</span><span class="n">maybe_aux</span> <span class="o">=</span> <span class="n">ad</span><span class="o">.</span><span class="n">linearize</span><span class="p">(</span>
      <span class="n">jaxtree_fun</span><span class="p">,</span> <span class="o">*</span><span class="n">primals_flat</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="n">has_aux</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">has_aux</span><span class="p">:</span>
    <span class="n">out_tree</span><span class="p">,</span> <span class="n">aux_tree</span> <span class="o">=</span> <span class="n">out_tree</span><span class="p">()</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">out_tree</span> <span class="o">=</span> <span class="n">out_tree</span><span class="p">()</span>
  <span class="n">out_primal_py</span> <span class="o">=</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">out_tree</span><span class="p">,</span> <span class="n">out_primals</span><span class="p">)</span>
  <span class="n">primal_avals</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">core</span><span class="o">.</span><span class="n">get_aval</span><span class="p">,</span> <span class="n">primals_flat</span><span class="p">))</span>
  <span class="c1"># Ensure that lifted_jvp is a PyTree</span>
  <span class="n">lifted_jvp</span> <span class="o">=</span> <span class="n">Partial</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">_lift_linearized</span><span class="p">,</span> <span class="n">jaxpr</span><span class="p">,</span> <span class="n">primal_avals</span><span class="p">,</span>
                               <span class="p">(</span><span class="n">in_tree</span><span class="p">,</span> <span class="n">out_tree</span><span class="p">),</span> <span class="n">out_pvals</span><span class="p">),</span> <span class="n">consts</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">has_aux</span><span class="p">:</span>
    <span class="p">[</span><span class="n">aux</span><span class="p">]</span> <span class="o">=</span> <span class="n">maybe_aux</span>
    <span class="k">return</span> <span class="n">out_primal_py</span><span class="p">,</span> <span class="n">lifted_jvp</span><span class="p">,</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">aux_tree</span><span class="p">,</span> <span class="n">aux</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="p">[]</span> <span class="o">=</span> <span class="n">maybe_aux</span>
    <span class="k">return</span> <span class="n">out_primal_py</span><span class="p">,</span> <span class="n">lifted_jvp</span></div>


<span class="k">def</span> <span class="nf">_lift_linearized</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">,</span> <span class="n">primal_avals</span><span class="p">,</span> <span class="n">io_tree</span><span class="p">,</span> <span class="n">out_pvals</span><span class="p">,</span> <span class="n">consts</span><span class="p">,</span> <span class="o">*</span><span class="n">py_args</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">fun</span><span class="p">(</span><span class="o">*</span><span class="n">tangents</span><span class="p">):</span>
    <span class="n">tangent_avals</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">core</span><span class="o">.</span><span class="n">get_aval</span><span class="p">,</span> <span class="n">tangents</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">primal_aval</span><span class="p">,</span> <span class="n">tangent_aval</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">primal_avals</span><span class="p">,</span> <span class="n">tangent_avals</span><span class="p">):</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">core</span><span class="o">.</span><span class="n">typecompat</span><span class="p">(</span><span class="n">primal_aval</span><span class="o">.</span><span class="n">at_least_vspace</span><span class="p">(),</span> <span class="n">tangent_aval</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;linearized function called on tangent values inconsistent with &quot;</span>
                         <span class="s2">&quot;the original primal values: &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;got </span><span class="si">{</span><span class="n">tangent_aval</span><span class="si">}</span><span class="s2"> for primal aval </span><span class="si">{</span><span class="n">primal_aval</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">tangents_out</span> <span class="o">=</span> <span class="n">eval_jaxpr</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">,</span> <span class="n">consts</span><span class="p">,</span> <span class="o">*</span><span class="n">tangents</span><span class="p">)</span>
    <span class="n">tangents_out_</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">tangents_out</span><span class="p">)</span>
    <span class="n">full_out</span> <span class="o">=</span> <span class="p">[</span><span class="n">pval</span><span class="o">.</span><span class="n">get_known</span><span class="p">()</span> <span class="k">if</span> <span class="n">pval</span><span class="o">.</span><span class="n">is_known</span><span class="p">()</span> <span class="k">else</span> <span class="nb">next</span><span class="p">(</span><span class="n">tangents_out_</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">pval</span> <span class="ow">in</span> <span class="n">out_pvals</span><span class="p">]</span>
    <span class="k">assert</span> <span class="nb">next</span><span class="p">(</span><span class="n">tangents_out_</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="n">full_out</span>

  <span class="k">return</span> <span class="n">apply_flat_fun_nokwargs</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">io_tree</span><span class="p">,</span> <span class="n">py_args</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_vjp_pullback_wrapper</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">cotangent_dtypes</span><span class="p">,</span> <span class="n">cotangent_shapes</span><span class="p">,</span> <span class="n">io_tree</span><span class="p">,</span>
                          <span class="n">fun</span><span class="p">,</span> <span class="o">*</span><span class="n">py_args_</span><span class="p">):</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">py_args_</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The function returned by `jax.vjp` applied to </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> was called &quot;</span>
           <span class="sa">f</span><span class="s2">&quot;with </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">py_args_</span><span class="p">)</span><span class="si">}</span><span class="s2"> arguments, but functions returned by &quot;</span>
           <span class="s2">&quot;`jax.vjp` must be called with a single argument corresponding to &quot;</span>
           <span class="sa">f</span><span class="s2">&quot;the single value returned by </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> (even if that returned &quot;</span>
           <span class="s2">&quot;value is a tuple or other container).</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;For example, if we have:</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;  def f(x):</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;    return (x, x)</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;  _, f_vjp = jax.vjp(f, 1.0)</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;the function `f` returns a single tuple as output, and so we call &quot;</span>
           <span class="s2">&quot;`f_vjp` with a single tuple as its argument:</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;  x_bar, = f_vjp((2.0, 2.0))</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;If we instead call `f_vjp(2.0, 2.0)`, with the values &#39;splatted &quot;</span>
           <span class="s2">&quot;out&#39; as arguments rather than in a tuple, this error can arise.&quot;</span><span class="p">)</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
  <span class="n">py_args</span><span class="p">,</span> <span class="o">=</span> <span class="n">py_args_</span>
  <span class="n">in_tree_expected</span><span class="p">,</span> <span class="n">out_tree</span> <span class="o">=</span> <span class="n">io_tree</span>
  <span class="n">args</span><span class="p">,</span> <span class="n">in_tree</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">(</span><span class="n">py_args</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">in_tree</span> <span class="o">!=</span> <span class="n">in_tree_expected</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Tree structure of cotangent input </span><span class="si">{</span><span class="n">in_tree</span><span class="si">}</span><span class="s2">, does not match structure of &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;primal output </span><span class="si">{</span><span class="n">in_tree_expected</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">arg</span><span class="p">,</span> <span class="n">ct_dtype</span><span class="p">,</span> <span class="n">ct_shape</span> <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">cotangent_dtypes</span><span class="p">,</span> <span class="n">cotangent_shapes</span><span class="p">):</span>
    <span class="n">expected_tangent_dtype</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">primal_dtype_to_tangent_dtype</span><span class="p">(</span><span class="n">_dtype</span><span class="p">(</span><span class="n">arg</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">expected_tangent_dtype</span> <span class="o">!=</span> <span class="n">ct_dtype</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
          <span class="sa">f</span><span class="s2">&quot;Type of cotangent input to vjp pullback function (</span><span class="si">{</span><span class="n">ct_dtype</span><span class="si">}</span><span class="s2">) is not &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;the expected tangent type (</span><span class="si">{</span><span class="n">expected_tangent_dtype</span><span class="si">}</span><span class="s2">) of corresponding primal output &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;with dtype </span><span class="si">{</span><span class="n">_dtype</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span> <span class="o">!=</span> <span class="n">ct_shape</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="sa">f</span><span class="s2">&quot;Shape of cotangent input to vjp pullback function </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span><span class="si">}</span><span class="s2"> &quot;</span>
          <span class="s2">&quot;must be the same as the shape of corresponding primal input &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">ct_shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
  <span class="n">ans</span> <span class="o">=</span> <span class="n">fun</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">out_tree</span><span class="p">,</span> <span class="n">ans</span><span class="p">)</span>

<span class="nd">@overload</span>
<span class="k">def</span> <span class="nf">vjp</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">T</span><span class="p">],</span>
        <span class="o">*</span><span class="n">primals</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="n">has_aux</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="kc">False</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">reduce_axes</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">AxisName</span><span class="p">]</span> <span class="o">=</span> <span class="p">())</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">T</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]:</span>
  <span class="o">...</span>

<span class="nd">@overload</span>
<span class="k">def</span> <span class="nf">vjp</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">T</span><span class="p">,</span> <span class="n">U</span><span class="p">]],</span> <span class="o">*</span><span class="n">primals</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="n">has_aux</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="kc">True</span><span class="p">],</span>
        <span class="n">reduce_axes</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">AxisName</span><span class="p">]</span> <span class="o">=</span> <span class="p">())</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">T</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">U</span><span class="p">]:</span>
  <span class="o">...</span>
<div class="viewcode-block" id="vjp">
<a class="viewcode-back" href="../../../_autosummary/jax.vjp.html#jax.vjp">[docs]</a>
<span class="k">def</span> <span class="nf">vjp</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
    <span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="o">*</span><span class="n">primals</span><span class="p">,</span> <span class="n">has_aux</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">reduce_axes</span><span class="o">=</span><span class="p">()</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Compute a (reverse-mode) vector-Jacobian product of ``fun``.</span>

<span class="sd">  :py:func:`grad` is implemented as a special case of :py:func:`vjp`.</span>

<span class="sd">  Args:</span>
<span class="sd">    fun: Function to be differentiated. Its arguments should be arrays, scalars,</span>
<span class="sd">      or standard Python containers of arrays or scalars. It should return an</span>
<span class="sd">      array, scalar, or standard Python container of arrays or scalars.</span>
<span class="sd">    primals: A sequence of primal values at which the Jacobian of ``fun``</span>
<span class="sd">      should be evaluated. The number of ``primals`` should be equal to the</span>
<span class="sd">      number of positional parameters of ``fun``. Each primal value should be</span>
<span class="sd">      an array, a scalar, or a pytree (standard Python containers) thereof.</span>
<span class="sd">    has_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the</span>
<span class="sd">     first element is considered the output of the mathematical function to be</span>
<span class="sd">     differentiated and the second element is auxiliary data. Default False.</span>

<span class="sd">  Returns:</span>
<span class="sd">    If ``has_aux`` is ``False``, returns a ``(primals_out, vjpfun)`` pair, where</span>
<span class="sd">    ``primals_out`` is ``fun(*primals)``. If ``has_aux`` is ``True``, returns a</span>
<span class="sd">    ``(primals_out, vjpfun, aux)`` tuple where ``aux`` is the auxiliary data</span>
<span class="sd">    returned by ``fun``.</span>

<span class="sd">    ``vjpfun`` is a function from a cotangent vector with the same shape as</span>
<span class="sd">    ``primals_out`` to a tuple of cotangent vectors with the same number and</span>
<span class="sd">    shapes as ``primals``, representing the vector-Jacobian product of ``fun``</span>
<span class="sd">    evaluated at ``primals``.</span>

<span class="sd">  &gt;&gt;&gt; import jax</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; def f(x, y):</span>
<span class="sd">  ...   return jax.numpy.sin(x), jax.numpy.cos(y)</span>
<span class="sd">  ...</span>
<span class="sd">  &gt;&gt;&gt; primals, f_vjp = jax.vjp(f, 0.5, 1.0)</span>
<span class="sd">  &gt;&gt;&gt; xbar, ybar = f_vjp((-0.7, 0.3))</span>
<span class="sd">  &gt;&gt;&gt; print(xbar)</span>
<span class="sd">  -0.61430776</span>
<span class="sd">  &gt;&gt;&gt; print(ybar)</span>
<span class="sd">  -0.2524413</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">reduce_axes</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;reduce_axes argument to vjp is deprecated&quot;</span><span class="p">)</span>
  <span class="k">del</span> <span class="n">reduce_axes</span>
  <span class="n">check_callable</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">_vjp</span><span class="p">(</span>
      <span class="n">lu</span><span class="o">.</span><span class="n">wrap_init</span><span class="p">(</span><span class="n">fun</span><span class="p">),</span> <span class="o">*</span><span class="n">primals</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="n">has_aux</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_vjp</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">lu</span><span class="o">.</span><span class="n">WrappedFun</span><span class="p">,</span> <span class="o">*</span><span class="n">primals</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Variant of vjp() that takes an lu.WrappedFun.&quot;&quot;&quot;</span>
  <span class="n">primals_flat</span><span class="p">,</span> <span class="n">in_tree</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">(</span><span class="n">primals</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">primals_flat</span><span class="p">:</span> <span class="n">dispatch</span><span class="o">.</span><span class="n">check_arg</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">has_aux</span><span class="p">:</span>
    <span class="n">flat_fun</span><span class="p">,</span> <span class="n">out_tree</span> <span class="o">=</span> <span class="n">flatten_fun_nokwargs</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">)</span>
    <span class="n">out_primal</span><span class="p">,</span> <span class="n">out_vjp</span> <span class="o">=</span> <span class="n">ad</span><span class="o">.</span><span class="n">vjp</span><span class="p">(</span><span class="n">flat_fun</span><span class="p">,</span> <span class="n">primals_flat</span><span class="p">)</span>
    <span class="n">out_tree</span> <span class="o">=</span> <span class="n">out_tree</span><span class="p">()</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">flat_fun</span><span class="p">,</span> <span class="n">out_aux_trees</span> <span class="o">=</span> <span class="n">flatten_fun_nokwargs2</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">)</span>
    <span class="n">out_primal</span><span class="p">,</span> <span class="n">out_vjp</span><span class="p">,</span> <span class="n">aux</span> <span class="o">=</span> <span class="n">ad</span><span class="o">.</span><span class="n">vjp</span><span class="p">(</span>
        <span class="n">flat_fun</span><span class="p">,</span> <span class="n">primals_flat</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">out_tree</span><span class="p">,</span> <span class="n">aux_tree</span> <span class="o">=</span> <span class="n">out_aux_trees</span><span class="p">()</span>
  <span class="n">out_primal_py</span> <span class="o">=</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">out_tree</span><span class="p">,</span> <span class="n">out_primal</span><span class="p">)</span>
  <span class="n">ct_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">primal_dtype_to_tangent_dtype</span><span class="p">(</span><span class="n">_dtype</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">out_primal</span><span class="p">]</span>
  <span class="n">ct_shapes</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">out_primal</span><span class="p">]</span>
  <span class="c1"># Ensure that vjp_py is a PyTree so that we can pass it from the forward to the</span>
  <span class="c1"># backward pass in a custom VJP.</span>
  <span class="n">vjp_py</span> <span class="o">=</span> <span class="n">Partial</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">_vjp_pullback_wrapper</span><span class="p">,</span> <span class="n">fun</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span>
                           <span class="n">ct_dtypes</span><span class="p">,</span> <span class="n">ct_shapes</span><span class="p">,</span> <span class="p">(</span><span class="n">out_tree</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">)),</span>
                   <span class="n">out_vjp</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">has_aux</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">out_primal_py</span><span class="p">,</span> <span class="n">vjp_py</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">out_primal_py</span><span class="p">,</span> <span class="n">vjp_py</span><span class="p">,</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">aux_tree</span><span class="p">,</span> <span class="n">aux</span><span class="p">)</span>


<div class="viewcode-block" id="linear_transpose">
<a class="viewcode-back" href="../../../_autosummary/jax.linear_transpose.html#jax.linear_transpose">[docs]</a>
<span class="k">def</span> <span class="nf">linear_transpose</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="o">*</span><span class="n">primals</span><span class="p">,</span> <span class="n">reduce_axes</span><span class="o">=</span><span class="p">())</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Transpose a function that is promised to be linear.</span>

<span class="sd">  For linear functions, this transformation is equivalent to :py:func:`vjp`, but</span>
<span class="sd">  avoids the overhead of computing the forward pass.</span>

<span class="sd">  The outputs of the transposed function will always have the exact same dtypes</span>
<span class="sd">  as ``primals``, even if some values are truncated (e.g., from complex to</span>
<span class="sd">  float, or from float64 to float32). To avoid truncation, use dtypes in</span>
<span class="sd">  ``primals`` that match the full range of desired outputs from the transposed</span>
<span class="sd">  function. Integer dtypes are not supported.</span>

<span class="sd">  Args:</span>
<span class="sd">    fun: the linear function to be transposed.</span>
<span class="sd">    *primals: a positional argument tuple of arrays, scalars, or (nested)</span>
<span class="sd">      standard Python containers (tuples, lists, dicts, namedtuples, i.e.,</span>
<span class="sd">      pytrees) of those types used for evaluating the shape/dtype of</span>
<span class="sd">      ``fun(*primals)``. These arguments may be real scalars/ndarrays, but that</span>
<span class="sd">      is not required: only the ``shape`` and ``dtype`` attributes are accessed.</span>
<span class="sd">      See below for an example. (Note that the duck-typed objects cannot be</span>
<span class="sd">      namedtuples because those are treated as standard Python containers.)</span>

<span class="sd">  Returns:</span>
<span class="sd">    A callable that calculates the transpose of ``fun``. Valid input into this</span>
<span class="sd">    function must have the same shape/dtypes/structure as the result of</span>
<span class="sd">    ``fun(*primals)``. Output will be a tuple, with the same</span>
<span class="sd">    shape/dtypes/structure as ``primals``.</span>

<span class="sd">  &gt;&gt;&gt; import jax</span>
<span class="sd">  &gt;&gt;&gt; import types</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; f = lambda x, y: 0.5 * x - 0.5 * y</span>
<span class="sd">  &gt;&gt;&gt; scalar = types.SimpleNamespace(shape=(), dtype=np.dtype(np.float32))</span>
<span class="sd">  &gt;&gt;&gt; f_transpose = jax.linear_transpose(f, scalar, scalar)</span>
<span class="sd">  &gt;&gt;&gt; f_transpose(1.0)</span>
<span class="sd">  (Array(0.5, dtype=float32), Array(-0.5, dtype=float32))</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">reduce_axes</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;reduce_axes argument to transpose is deprecated&quot;</span><span class="p">)</span>
  <span class="k">del</span> <span class="n">reduce_axes</span>
  <span class="n">primals_flat</span><span class="p">,</span> <span class="n">in_tree</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">(</span><span class="n">primals</span><span class="p">)</span>
  <span class="n">flat_fun</span><span class="p">,</span> <span class="n">out_tree</span> <span class="o">=</span> <span class="n">flatten_fun_nokwargs</span><span class="p">(</span><span class="n">lu</span><span class="o">.</span><span class="n">wrap_init</span><span class="p">(</span><span class="n">fun</span><span class="p">),</span> <span class="n">in_tree</span><span class="p">)</span>
  <span class="n">in_avals</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">shaped_abstractify</span><span class="p">,</span> <span class="n">primals_flat</span><span class="p">)</span>
  <span class="n">in_dtypes</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">dtypes</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">in_avals</span><span class="p">)</span>

  <span class="n">in_pvals</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">pe</span><span class="o">.</span><span class="n">PartialVal</span><span class="o">.</span><span class="n">unknown</span><span class="p">,</span> <span class="n">in_avals</span><span class="p">)</span>
  <span class="n">jaxpr</span><span class="p">,</span> <span class="n">out_pvals</span><span class="p">,</span> <span class="n">const</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">trace_to_jaxpr_nounits</span><span class="p">(</span><span class="n">flat_fun</span><span class="p">,</span> <span class="n">in_pvals</span><span class="p">,</span>
                                                      <span class="n">instantiate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">jaxpr</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">dce_jaxpr</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">,</span> <span class="p">[</span><span class="kc">True</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">jaxpr</span><span class="o">.</span><span class="n">outvars</span><span class="p">),</span> <span class="kc">True</span><span class="p">)</span>
  <span class="n">out_avals</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">unzip2</span><span class="p">(</span><span class="n">out_pvals</span><span class="p">)</span>
  <span class="n">out_dtypes</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">dtypes</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">out_avals</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">all</span><span class="p">(</span><span class="n">dtypes</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">inexact</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">in_dtypes</span> <span class="o">+</span> <span class="n">out_dtypes</span><span class="p">)</span>
          <span class="ow">or</span> <span class="nb">all</span><span class="p">(</span><span class="n">dtypes</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">integer</span><span class="p">)</span>
                 <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">in_dtypes</span> <span class="o">+</span> <span class="n">out_dtypes</span><span class="p">)):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;linear_transpose only supports [float or complex] -&gt; &quot;</span>
                    <span class="s2">&quot;[float or complex], and integer -&gt; integer functions, &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">in_dtypes</span><span class="si">}</span><span class="s2"> -&gt; </span><span class="si">{</span><span class="n">out_dtypes</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

  <span class="nd">@api_boundary</span>
  <span class="k">def</span> <span class="nf">transposed_fun</span><span class="p">(</span><span class="n">const</span><span class="p">,</span> <span class="n">out_cotangent</span><span class="p">):</span>
    <span class="n">out_cts</span><span class="p">,</span> <span class="n">out_tree2</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">(</span><span class="n">out_cotangent</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">out_tree</span><span class="p">()</span> <span class="o">!=</span> <span class="n">out_tree2</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;cotangent tree does not match function output, &quot;</span>
                      <span class="sa">f</span><span class="s2">&quot;expected </span><span class="si">{</span><span class="n">out_tree</span><span class="p">()</span><span class="si">}</span><span class="s2"> but got </span><span class="si">{</span><span class="n">out_tree2</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">core</span><span class="o">.</span><span class="n">typecheck</span><span class="p">,</span> <span class="n">out_avals</span><span class="p">,</span> <span class="n">out_cts</span><span class="p">)):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;cotangent type does not match function output, &quot;</span>
                      <span class="sa">f</span><span class="s2">&quot;expected </span><span class="si">{</span><span class="n">out_avals</span><span class="si">}</span><span class="s2"> but got </span><span class="si">{</span><span class="n">out_cts</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">dummies</span> <span class="o">=</span> <span class="p">[</span><span class="n">ad</span><span class="o">.</span><span class="n">UndefinedPrimal</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">in_avals</span><span class="p">]</span>
    <span class="n">in_cts</span> <span class="o">=</span> <span class="n">ad</span><span class="o">.</span><span class="n">backward_pass</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">const</span><span class="p">,</span> <span class="n">dummies</span><span class="p">,</span> <span class="n">out_cts</span><span class="p">)</span>
    <span class="n">in_cts</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">ad</span><span class="o">.</span><span class="n">instantiate_zeros</span><span class="p">,</span> <span class="n">in_cts</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">in_tree</span><span class="p">,</span> <span class="n">in_cts</span><span class="p">)</span>

  <span class="c1"># Ensure that transposed_fun is a PyTree</span>
  <span class="k">return</span> <span class="n">Partial</span><span class="p">(</span><span class="n">transposed_fun</span><span class="p">,</span> <span class="n">const</span><span class="p">)</span></div>



<span class="k">def</span> <span class="nf">_flat_axes_specs</span><span class="p">(</span><span class="n">abstracted_axes</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
                     <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">pe</span><span class="o">.</span><span class="n">AbstractedAxesSpec</span><span class="p">]:</span>
  <span class="k">if</span> <span class="n">kwargs</span><span class="p">:</span> <span class="k">raise</span> <span class="ne">NotImplementedError</span>
  <span class="k">def</span> <span class="nf">ax_leaf</span><span class="p">(</span><span class="n">l</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">and</span> <span class="n">all_leaves</span><span class="p">(</span><span class="n">l</span><span class="o">.</span><span class="n">values</span><span class="p">())</span> <span class="ow">or</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="n">all_leaves</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">broadcast_prefix</span><span class="p">(</span><span class="n">abstracted_axes</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">ax_leaf</span><span class="p">)</span>


<span class="c1"># TODO(phawkins): for some reason mypy cannot determine these overloads are</span>
<span class="c1"># non-overlapping. Pytype is happy with them.</span>
<span class="nd">@overload</span>
<span class="k">def</span> <span class="nf">make_jaxpr</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
               <span class="n">static_argnums</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(),</span>
               <span class="n">axis_env</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="n">AxisName</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
               <span class="n">return_shape</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="kc">False</span><span class="p">]</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
               <span class="n">abstracted_axes</span><span class="p">:</span> <span class="n">Any</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
               <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">core</span><span class="o">.</span><span class="n">ClosedJaxpr</span><span class="p">]:</span>
  <span class="o">...</span>

<span class="nd">@overload</span>
<span class="k">def</span> <span class="nf">make_jaxpr</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
               <span class="n">static_argnums</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(),</span>
               <span class="n">axis_env</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="n">AxisName</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
               <span class="n">return_shape</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="kc">True</span><span class="p">]</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span>
               <span class="n">abstracted_axes</span><span class="p">:</span> <span class="n">Any</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
               <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">ClosedJaxpr</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
  <span class="o">...</span>

<div class="viewcode-block" id="make_jaxpr">
<a class="viewcode-back" href="../../../_autosummary/jax.make_jaxpr.html#jax.make_jaxpr">[docs]</a>
<span class="k">def</span> <span class="nf">make_jaxpr</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
               <span class="n">static_argnums</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(),</span>
               <span class="n">axis_env</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="n">AxisName</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
               <span class="n">return_shape</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
               <span class="n">abstracted_axes</span><span class="p">:</span> <span class="n">Any</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
               <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">(</span><span class="n">core</span><span class="o">.</span><span class="n">ClosedJaxpr</span> <span class="o">|</span>
                                        <span class="nb">tuple</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">ClosedJaxpr</span><span class="p">,</span> <span class="n">Any</span><span class="p">])]:</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Creates a function that produces its jaxpr given example args.</span>

<span class="sd">  Args:</span>
<span class="sd">    fun: The function whose ``jaxpr`` is to be computed. Its positional</span>
<span class="sd">      arguments and return value should be arrays, scalars, or standard Python</span>
<span class="sd">      containers (tuple/list/dict) thereof.</span>
<span class="sd">    static_argnums: See the :py:func:`jax.jit` docstring.</span>
<span class="sd">    axis_env: Optional, a sequence of pairs where the first element is an axis</span>
<span class="sd">      name and the second element is a positive integer representing the size of</span>
<span class="sd">      the mapped axis with that name. This parameter is useful when lowering</span>
<span class="sd">      functions that involve parallel communication collectives, and it</span>
<span class="sd">      specifies the axis name/size environment that would be set up by</span>
<span class="sd">      applications of :py:func:`jax.pmap`.</span>
<span class="sd">    return_shape: Optional boolean, defaults to ``False``. If ``True``, the</span>
<span class="sd">      wrapped function returns a pair where the first element is the</span>
<span class="sd">      ``ClosedJaxpr`` representation of ``fun`` and the second element is a</span>
<span class="sd">      pytree with the same structure as the output of ``fun`` and where the</span>
<span class="sd">      leaves are objects with ``shape``, ``dtype``, and ``named_shape``</span>
<span class="sd">      attributes representing the corresponding types of the output leaves.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A wrapped version of ``fun`` that when applied to example arguments returns</span>
<span class="sd">    a ``ClosedJaxpr`` representation of ``fun`` on those arguments. If the</span>
<span class="sd">    argument ``return_shape`` is ``True``, then the returned function instead</span>
<span class="sd">    returns a pair where the first element is the ``ClosedJaxpr``</span>
<span class="sd">    representation of ``fun`` and the second element is a pytree representing</span>
<span class="sd">    the structure, shape, dtypes, and named shapes of the output of ``fun``.</span>

<span class="sd">  A ``jaxpr`` is JAX&#39;s intermediate representation for program traces. The</span>
<span class="sd">  ``jaxpr`` language is based on the simply-typed first-order lambda calculus</span>
<span class="sd">  with let-bindings. :py:func:`make_jaxpr` adapts a function to return its</span>
<span class="sd">  ``jaxpr``, which we can inspect to understand what JAX is doing internally.</span>
<span class="sd">  The ``jaxpr`` returned is a trace of ``fun`` abstracted to</span>
<span class="sd">  :py:class:`ShapedArray` level. Other levels of abstraction exist internally.</span>

<span class="sd">  We do not describe the semantics of the ``jaxpr`` language in detail here, but</span>
<span class="sd">  instead give a few examples.</span>

<span class="sd">  &gt;&gt;&gt; import jax</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; def f(x): return jax.numpy.sin(jax.numpy.cos(x))</span>
<span class="sd">  &gt;&gt;&gt; print(f(3.0))</span>
<span class="sd">  -0.83602</span>
<span class="sd">  &gt;&gt;&gt; jax.make_jaxpr(f)(3.0)</span>
<span class="sd">  { lambda ; a:f32[]. let b:f32[] = cos a; c:f32[] = sin b in (c,) }</span>
<span class="sd">  &gt;&gt;&gt; jax.make_jaxpr(jax.grad(f))(3.0)</span>
<span class="sd">  { lambda ; a:f32[]. let</span>
<span class="sd">      b:f32[] = cos a</span>
<span class="sd">      c:f32[] = sin a</span>
<span class="sd">      _:f32[] = sin b</span>
<span class="sd">      d:f32[] = cos b</span>
<span class="sd">      e:f32[] = mul 1.0 d</span>
<span class="sd">      f:f32[] = neg e</span>
<span class="sd">      g:f32[] = mul f c</span>
<span class="sd">    in (g,) }</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">check_callable</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="n">static_argnums</span> <span class="o">=</span> <span class="n">_ensure_index_tuple</span><span class="p">(</span><span class="n">static_argnums</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">abstractify</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
    <span class="n">flat_args</span><span class="p">,</span> <span class="n">in_tree</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">((</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">abstracted_axes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">map</span><span class="p">(</span><span class="n">shaped_abstractify</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">),</span> <span class="n">in_tree</span><span class="p">,</span> <span class="p">[</span><span class="kc">True</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">axes_specs</span> <span class="o">=</span> <span class="n">_flat_axes_specs</span><span class="p">(</span><span class="n">abstracted_axes</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
      <span class="n">in_type</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">infer_lambda_input_type</span><span class="p">(</span><span class="n">axes_specs</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">)</span>
      <span class="n">in_avals</span><span class="p">,</span> <span class="n">keep_inputs</span> <span class="o">=</span> <span class="n">unzip2</span><span class="p">(</span><span class="n">in_type</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">in_avals</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">,</span> <span class="n">keep_inputs</span>

  <span class="nd">@wraps</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="nd">@api_boundary</span>
  <span class="k">def</span> <span class="nf">make_jaxpr_f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">lu</span><span class="o">.</span><span class="n">wrap_init</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">static_argnums</span><span class="p">:</span>
      <span class="n">dyn_argnums</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">))</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">static_argnums</span><span class="p">]</span>
      <span class="n">f</span><span class="p">,</span> <span class="n">args</span> <span class="o">=</span> <span class="n">argnums_partial</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">dyn_argnums</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
    <span class="n">in_avals</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">,</span> <span class="n">keep_inputs</span> <span class="o">=</span> <span class="n">abstractify</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
    <span class="n">in_type</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">in_avals</span><span class="p">,</span> <span class="n">keep_inputs</span><span class="p">))</span>
    <span class="n">f</span><span class="p">,</span> <span class="n">out_tree</span> <span class="o">=</span> <span class="n">flatten_fun</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">lu</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">in_type</span><span class="p">)</span>
    <span class="n">debug_info</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">debug_info</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">,</span> <span class="n">out_tree</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;make_jaxpr&#39;</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">ExitStack</span><span class="p">()</span> <span class="k">as</span> <span class="n">stack</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">axis_name</span><span class="p">,</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">axis_env</span> <span class="ow">or</span> <span class="p">[]:</span>
        <span class="n">stack</span><span class="o">.</span><span class="n">enter_context</span><span class="p">(</span><span class="n">core</span><span class="o">.</span><span class="n">extend_axis_env</span><span class="p">(</span><span class="n">axis_name</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
      <span class="n">jaxpr</span><span class="p">,</span> <span class="n">out_type</span><span class="p">,</span> <span class="n">consts</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">trace_to_jaxpr_dynamic2</span><span class="p">(</span>
          <span class="n">f</span><span class="p">,</span> <span class="n">debug_info</span><span class="o">=</span><span class="n">debug_info</span><span class="p">)</span>
    <span class="n">closed_jaxpr</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">ClosedJaxpr</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">,</span> <span class="n">consts</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">return_shape</span><span class="p">:</span>
      <span class="n">out_avals</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">unzip2</span><span class="p">(</span><span class="n">out_type</span><span class="p">)</span>
      <span class="n">out_shapes_flat</span> <span class="o">=</span> <span class="p">[</span>
          <span class="n">ShapeDtypeStruct</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">named_shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">out_avals</span><span class="p">]</span>
      <span class="k">return</span> <span class="n">closed_jaxpr</span><span class="p">,</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">out_tree</span><span class="p">(),</span> <span class="n">out_shapes_flat</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">closed_jaxpr</span>

  <span class="n">make_jaxpr_f</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;jax&quot;</span>
  <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="s2">&quot;__qualname__&quot;</span><span class="p">):</span>
    <span class="n">make_jaxpr_f</span><span class="o">.</span><span class="vm">__qualname__</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;make_jaxpr(</span><span class="si">{</span><span class="n">fun</span><span class="o">.</span><span class="vm">__qualname__</span><span class="si">}</span><span class="s2">)&quot;</span>
  <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="s2">&quot;__name__&quot;</span><span class="p">):</span>
    <span class="n">make_jaxpr_f</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;make_jaxpr(</span><span class="si">{</span><span class="n">fun</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">)&quot;</span>
  <span class="k">return</span> <span class="n">make_jaxpr_f</span></div>


<span class="k">def</span> <span class="nf">_infer_src_sharding</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Sharding</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
  <span class="k">if</span> <span class="n">src</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">src</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">array</span><span class="o">.</span><span class="n">ArrayImpl</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">sharding</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">core</span><span class="o">.</span><span class="n">Tracer</span><span class="p">):</span>
    <span class="n">aval</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">get_aval</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">aval</span><span class="p">,</span> <span class="n">ConcreteArray</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">val</span><span class="p">,</span> <span class="n">array</span><span class="o">.</span><span class="n">ArrayImpl</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">aval</span><span class="o">.</span><span class="n">val</span><span class="o">.</span><span class="n">sharding</span>
  <span class="k">return</span> <span class="kc">None</span>


<span class="c1"># TODO(yashkatariya): Generalize is_compatible_aval (maybe renamed) and use that</span>
<span class="c1"># to check if shardings are compatible with the input.</span>
<span class="k">def</span> <span class="nf">_check_sharding</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">Sharding</span><span class="p">):</span>
    <span class="n">aval</span> <span class="o">=</span> <span class="n">shaped_abstractify</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">XLACompatibleSharding</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">PmapSharding</span><span class="p">):</span>
      <span class="n">pjit</span><span class="o">.</span><span class="n">pjit_check_aval_sharding</span><span class="p">(</span>
          <span class="p">(</span><span class="n">s</span><span class="p">,),</span> <span class="p">(</span><span class="n">aval</span><span class="p">,),</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;device_put args&quot;</span><span class="p">,</span> <span class="n">allow_uneven_sharding</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">s</span><span class="o">.</span><span class="n">shard_shape</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># should raise an Error if incompatible</span>


<div class="viewcode-block" id="device_put">
<a class="viewcode-back" href="../../../_autosummary/jax.device_put.html#jax.device_put">[docs]</a>
<span class="k">def</span> <span class="nf">device_put</span><span class="p">(</span>
    <span class="n">x</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="n">xc</span><span class="o">.</span><span class="n">Device</span> <span class="o">|</span> <span class="n">Sharding</span> <span class="o">|</span> <span class="n">Layout</span> <span class="o">|</span> <span class="n">Any</span> <span class="o">|</span> <span class="n">TransferToMemoryKind</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span> <span class="n">src</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="n">xc</span><span class="o">.</span><span class="n">Device</span> <span class="o">|</span> <span class="n">Sharding</span> <span class="o">|</span> <span class="n">Layout</span> <span class="o">|</span> <span class="n">Any</span> <span class="o">|</span> <span class="n">TransferToMemoryKind</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Transfers ``x`` to ``device``.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: An array, scalar, or (nested) standard Python container thereof.</span>
<span class="sd">    device: The (optional) :py:class:`Device`, `Sharding`, or a (nested)</span>
<span class="sd">      `Sharding` in standard Python container (must be a tree prefix of ``x``),</span>
<span class="sd">      representing the device(s) to which ``x`` should be transferred. If</span>
<span class="sd">      given, then the result is committed to the device(s).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A copy of ``x`` that resides on ``device``.</span>

<span class="sd">  If the ``device`` parameter is ``None``, then this operation behaves like the</span>
<span class="sd">  identity function if the operand is on any device already, otherwise it</span>
<span class="sd">  transfers the data to the default device, uncommitted.</span>

<span class="sd">  For more details on data placement see the</span>
<span class="sd">  :ref:`FAQ on data placement &lt;faq-data-placement&gt;`.</span>

<span class="sd">  This function is always asynchronous, i.e. returns immediately without</span>
<span class="sd">  blocking the calling Python thread until any transfers are completed.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">config</span><span class="o">.</span><span class="n">explicit_device_put_scope</span><span class="p">():</span>
    <span class="k">if</span> <span class="p">((</span><span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span>
         <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="p">(</span><span class="n">xc</span><span class="o">.</span><span class="n">Device</span><span class="p">,</span> <span class="n">Sharding</span><span class="p">,</span> <span class="n">TransferToMemoryKind</span><span class="p">)))</span> <span class="ow">and</span>
        <span class="p">(</span><span class="n">src</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span>
         <span class="nb">isinstance</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="p">(</span><span class="n">xc</span><span class="o">.</span><span class="n">Device</span><span class="p">,</span> <span class="n">Sharding</span><span class="p">,</span> <span class="n">TransferToMemoryKind</span><span class="p">)))):</span>
      <span class="k">for</span> <span class="n">leaf</span> <span class="ow">in</span> <span class="n">tree_leaves</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="n">_check_sharding</span><span class="p">(</span><span class="n">leaf</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">tree_map</span><span class="p">(</span>
          <span class="k">lambda</span> <span class="n">y</span><span class="p">:</span> <span class="n">dispatch</span><span class="o">.</span><span class="n">device_put_p</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span>
              <span class="n">y</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="n">_infer_src_sharding</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">y</span><span class="p">)),</span> <span class="n">x</span><span class="p">)</span>

    <span class="n">x_flat</span><span class="p">,</span> <span class="n">treedef</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">device_flat</span> <span class="o">=</span> <span class="n">flatten_axes</span><span class="p">(</span><span class="s2">&quot;device_put device&quot;</span><span class="p">,</span> <span class="n">treedef</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
    <span class="n">src_flat</span> <span class="o">=</span> <span class="n">flatten_axes</span><span class="p">(</span><span class="s2">&quot;device_put source&quot;</span><span class="p">,</span> <span class="n">treedef</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">x_leaf</span><span class="p">,</span> <span class="n">device_leaf</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">x_flat</span><span class="p">,</span> <span class="n">device_flat</span><span class="p">):</span>
      <span class="n">_check_sharding</span><span class="p">(</span><span class="n">x_leaf</span><span class="p">,</span> <span class="n">device_leaf</span><span class="p">)</span>
    <span class="n">out_flat</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">dispatch</span><span class="o">.</span><span class="n">device_put_p</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">xf</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">d</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="n">_infer_src_sharding</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">xf</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">xf</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">x_flat</span><span class="p">,</span> <span class="n">device_flat</span><span class="p">,</span> <span class="n">src_flat</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">treedef</span><span class="p">,</span> <span class="n">out_flat</span><span class="p">)</span></div>



<div class="viewcode-block" id="device_put_sharded">
<a class="viewcode-back" href="../../../_autosummary/jax.device_put_sharded.html#jax.device_put_sharded">[docs]</a>
<span class="k">def</span> <span class="nf">device_put_sharded</span><span class="p">(</span><span class="n">shards</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">devices</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">xc</span><span class="o">.</span><span class="n">Device</span><span class="p">]):</span>  <span class="c1"># noqa: F811</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Transfer array shards to specified devices and form Array(s).</span>

<span class="sd">  Args:</span>
<span class="sd">    shards: A sequence of arrays, scalars, or (nested) standard Python</span>
<span class="sd">      containers thereof representing the shards to be stacked together to form</span>
<span class="sd">      the output. The length of ``shards`` must equal the length of ``devices``.</span>
<span class="sd">    devices: A sequence of :py:class:`Device` instances representing the devices</span>
<span class="sd">      to which corresponding shards in ``shards`` will be transferred.</span>

<span class="sd">  This function is always asynchronous, i.e. returns immediately.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A Array or (nested) Python container thereof representing the</span>
<span class="sd">    elements of ``shards`` stacked together, with each shard backed by physical</span>
<span class="sd">    device memory specified by the corresponding entry in ``devices``.</span>

<span class="sd">  Examples:</span>
<span class="sd">    Passing a list of arrays for ``shards`` results in a sharded array</span>
<span class="sd">    containing a stacked version of the inputs:</span>

<span class="sd">    &gt;&gt;&gt; import jax</span>
<span class="sd">    &gt;&gt;&gt; devices = jax.local_devices()</span>
<span class="sd">    &gt;&gt;&gt; x = [jax.numpy.ones(5) for device in devices]</span>
<span class="sd">    &gt;&gt;&gt; y = jax.device_put_sharded(x, devices)</span>
<span class="sd">    &gt;&gt;&gt; np.allclose(y, jax.numpy.stack(x))</span>
<span class="sd">    True</span>

<span class="sd">    Passing a list of nested container objects with arrays at the leaves for</span>
<span class="sd">    ``shards`` corresponds to stacking the shards at each leaf. This requires</span>
<span class="sd">    all entries in the list to have the same tree structure:</span>

<span class="sd">    &gt;&gt;&gt; x = [(i, jax.numpy.arange(i, i + 4)) for i in range(len(devices))]</span>
<span class="sd">    &gt;&gt;&gt; y = jax.device_put_sharded(x, devices)</span>
<span class="sd">    &gt;&gt;&gt; type(y)</span>
<span class="sd">    &lt;class &#39;tuple&#39;&gt;</span>
<span class="sd">    &gt;&gt;&gt; y0 = jax.device_put_sharded([a for a, b in x], devices)</span>
<span class="sd">    &gt;&gt;&gt; y1 = jax.device_put_sharded([b for a, b in x], devices)</span>
<span class="sd">    &gt;&gt;&gt; np.allclose(y[0], y0)</span>
<span class="sd">    True</span>
<span class="sd">    &gt;&gt;&gt; np.allclose(y[1], y1)</span>
<span class="sd">    True</span>

<span class="sd">  See Also:</span>
<span class="sd">    - device_put</span>
<span class="sd">    - device_put_replicated</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># TODO(jakevdp): provide a default for devices that considers both local</span>
  <span class="c1"># devices and pods</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shards</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;device_put_sharded `shards` input must be a sequence; &quot;</span>
                     <span class="sa">f</span><span class="s2">&quot;got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">shards</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shards</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;len(shards) = </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">shards</span><span class="p">)</span><span class="si">}</span><span class="s2"> must equal &quot;</span>
                     <span class="sa">f</span><span class="s2">&quot;len(devices) = </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_device_put_sharded</span><span class="p">(</span><span class="o">*</span><span class="n">xs</span><span class="p">):</span>
    <span class="n">avals</span> <span class="o">=</span> <span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">raise_to_shaped</span><span class="p">(</span><span class="n">core</span><span class="o">.</span><span class="n">get_aval</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xs</span><span class="p">]</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">a1</span> <span class="o">==</span> <span class="n">a2</span> <span class="k">for</span> <span class="n">a1</span><span class="p">,</span> <span class="n">a2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">avals</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">avals</span><span class="p">[</span><span class="mi">1</span><span class="p">:])):</span>
      <span class="n">a1</span><span class="p">,</span> <span class="n">a2</span> <span class="o">=</span> <span class="nb">next</span><span class="p">((</span><span class="n">a1</span><span class="p">,</span> <span class="n">a2</span><span class="p">)</span> <span class="k">for</span> <span class="n">a1</span><span class="p">,</span> <span class="n">a2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">avals</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">avals</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
                    <span class="k">if</span> <span class="n">a1</span> <span class="o">!=</span> <span class="n">a2</span><span class="p">)</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;the shards passed to device_put_sharded must have &quot;</span>
                       <span class="sa">f</span><span class="s2">&quot;consistent shape and dtype, but got </span><span class="si">{</span><span class="n">a1</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">a2</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">stacked_aval</span> <span class="o">=</span> <span class="n">avals</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">),)</span> <span class="o">+</span> <span class="n">avals</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">sharding_spec</span> <span class="o">=</span> <span class="n">sharding_specs</span><span class="o">.</span><span class="n">create_pmap_sharding_spec</span><span class="p">(</span><span class="n">stacked_aval</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">sharding</span> <span class="o">=</span> <span class="n">PmapSharding</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">devices</span><span class="p">),</span> <span class="n">sharding_spec</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">stacked_aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">extended</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">stacked_aval</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">_rules</span><span class="o">.</span><span class="n">device_put_sharded</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">stacked_aval</span><span class="p">,</span> <span class="n">sharding</span><span class="p">,</span> <span class="n">devices</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">pmap_no_rank_reduction</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
      <span class="n">ys</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xs</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">basearray</span><span class="o">.</span><span class="n">Array</span><span class="p">)):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="kc">None</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">ys</span> <span class="o">=</span> <span class="n">xs</span>
    <span class="k">return</span> <span class="n">pxla</span><span class="o">.</span><span class="n">batched_device_put</span><span class="p">(</span><span class="n">stacked_aval</span><span class="p">,</span> <span class="n">sharding</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="n">devices</span><span class="p">))</span>


  <span class="k">with</span> <span class="n">config</span><span class="o">.</span><span class="n">explicit_device_put_scope</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">tree_map</span><span class="p">(</span><span class="n">_device_put_sharded</span><span class="p">,</span> <span class="o">*</span><span class="n">shards</span><span class="p">)</span></div>



<div class="viewcode-block" id="device_put_replicated">
<a class="viewcode-back" href="../../../_autosummary/jax.device_put_replicated.html#jax.device_put_replicated">[docs]</a>
<span class="k">def</span> <span class="nf">device_put_replicated</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">devices</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">xc</span><span class="o">.</span><span class="n">Device</span><span class="p">]):</span>  <span class="c1"># noqa: F811</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Transfer array(s) to each specified device and form Array(s).</span>

<span class="sd">  Args:</span>
<span class="sd">    x: an array, scalar, or (nested) standard Python container thereof</span>
<span class="sd">      representing the array to be replicated to form the output.</span>
<span class="sd">    devices: A sequence of :py:class:`Device` instances representing the devices</span>
<span class="sd">      to which ``x`` will be transferred.</span>

<span class="sd">  This function is always asynchronous, i.e. returns immediately.</span>

<span class="sd">  Returns:</span>
<span class="sd">    An Array or (nested) Python container thereof representing the</span>
<span class="sd">    value of ``x`` broadcasted along a new leading axis of size</span>
<span class="sd">    ``len(devices)``, with each slice along that new leading axis backed by</span>
<span class="sd">    memory on the device specified by the corresponding entry in ``devices``.</span>

<span class="sd">  Examples:</span>
<span class="sd">    Passing an array:</span>

<span class="sd">    &gt;&gt;&gt; import jax</span>
<span class="sd">    &gt;&gt;&gt; devices = jax.local_devices()</span>
<span class="sd">    &gt;&gt;&gt; x = jax.numpy.array([1., 2., 3.])</span>
<span class="sd">    &gt;&gt;&gt; y = jax.device_put_replicated(x, devices)</span>
<span class="sd">    &gt;&gt;&gt; np.allclose(y, jax.numpy.stack([x for _ in devices]))</span>
<span class="sd">    True</span>

<span class="sd">  See Also:</span>
<span class="sd">    - device_put</span>
<span class="sd">    - device_put_sharded</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">devices</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">devices</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`devices` argument to `device_put_replicated must be &quot;</span>
                     <span class="s2">&quot;a non-empty sequence.&quot;</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">_device_put_replicated</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">aval</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">unmapped_aval</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">),</span> <span class="n">core</span><span class="o">.</span><span class="n">no_axis_name</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
                              <span class="n">core</span><span class="o">.</span><span class="n">raise_to_shaped</span><span class="p">(</span><span class="n">core</span><span class="o">.</span><span class="n">get_aval</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">aval</span><span class="p">,</span> <span class="n">ShapedArray</span><span class="p">)</span>
    <span class="n">sharding_spec</span> <span class="o">=</span> <span class="n">sharding_specs</span><span class="o">.</span><span class="n">create_pmap_sharding_spec</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">pmap_no_rank_reduction</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">basearray</span><span class="o">.</span><span class="n">Array</span><span class="p">)):</span>
        <span class="n">buf</span> <span class="o">=</span> <span class="n">device_put</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="kc">None</span><span class="p">],</span> <span class="n">devices</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">buf</span> <span class="o">=</span> <span class="n">device_put</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">devices</span><span class="p">[</span><span class="mi">0</span><span class="p">])[</span><span class="kc">None</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">buf</span> <span class="o">=</span> <span class="n">device_put</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">devices</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">sharding</span> <span class="o">=</span> <span class="n">PmapSharding</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">devices</span><span class="p">),</span> <span class="n">sharding_spec</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">extended</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">_rules</span><span class="o">.</span><span class="n">device_put_replicated</span><span class="p">(</span><span class="n">buf</span><span class="p">,</span> <span class="n">aval</span><span class="p">,</span> <span class="n">sharding</span><span class="p">,</span> <span class="n">devices</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">xla</span><span class="o">.</span><span class="n">aval_to_xla_shapes</span><span class="p">(</span><span class="n">aval</span><span class="p">))</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">pxla</span><span class="o">.</span><span class="n">batched_device_put</span><span class="p">(</span><span class="n">aval</span><span class="p">,</span> <span class="n">sharding</span><span class="p">,</span> <span class="p">[</span><span class="n">buf</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">),</span> <span class="n">devices</span><span class="p">)</span>

  <span class="k">with</span> <span class="n">config</span><span class="o">.</span><span class="n">explicit_device_put_scope</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">tree_map</span><span class="p">(</span><span class="n">_device_put_replicated</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></div>



<span class="c1"># TODO(mattjj): consider revising</span>
<span class="k">def</span> <span class="nf">_device_get</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">core</span><span class="o">.</span><span class="n">Tracer</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">toarray</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">__array__</span>
  <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">x</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">toarray</span><span class="p">()</span>

<div class="viewcode-block" id="device_get">
<a class="viewcode-back" href="../../../_autosummary/jax.device_get.html#jax.device_get">[docs]</a>
<span class="k">def</span> <span class="nf">device_get</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Any</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Transfer ``x`` to host.</span>

<span class="sd">  If ``x`` is a pytree, then the individual buffers are copied in parallel.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: An array, scalar, Array or (nested) standard Python container thereof</span>
<span class="sd">      representing the array to be transferred to host.</span>

<span class="sd">  Returns:</span>
<span class="sd">    An array or (nested) Python container thereof representing the</span>
<span class="sd">    value of ``x``.</span>

<span class="sd">  Examples:</span>
<span class="sd">    Passing a Array:</span>

<span class="sd">    &gt;&gt;&gt; import jax</span>
<span class="sd">    &gt;&gt;&gt; x = jax.numpy.array([1., 2., 3.])</span>
<span class="sd">    &gt;&gt;&gt; jax.device_get(x)</span>
<span class="sd">    array([1., 2., 3.], dtype=float32)</span>

<span class="sd">    Passing a scalar (has no effect):</span>

<span class="sd">    &gt;&gt;&gt; jax.device_get(1)</span>
<span class="sd">    1</span>

<span class="sd">  See Also:</span>
<span class="sd">    - device_put</span>
<span class="sd">    - device_put_sharded</span>
<span class="sd">    - device_put_replicated</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">config</span><span class="o">.</span><span class="n">explicit_device_get_scope</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">tree_leaves</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="n">y</span><span class="o">.</span><span class="n">copy_to_host_async</span><span class="p">()</span>
      <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
        <span class="k">pass</span>
    <span class="k">return</span> <span class="n">tree_map</span><span class="p">(</span><span class="n">_device_get</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></div>



<div class="viewcode-block" id="ShapeDtypeStruct">
<a class="viewcode-back" href="../../../_autosummary/jax.ShapeDtypeStruct.html#jax.ShapeDtypeStruct">[docs]</a>
<span class="k">class</span> <span class="nc">ShapeDtypeStruct</span><span class="p">:</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;A container for the shape, dtype, and other static attributes of an array.</span>

<span class="sd">  ``ShapeDtypeStruct`` is often used in conjunction with :func:`jax.eval_shape`.</span>

<span class="sd">  Args:</span>
<span class="sd">    shape: a sequence of integers representing an array shape</span>
<span class="sd">    dtype: a dtype-like object</span>
<span class="sd">    named_shape: (optional) a dictionary representing a named shape</span>
<span class="sd">    sharding: (optional) a :class:`jax.Sharding` object</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="vm">__slots__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;shape&quot;</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="s2">&quot;named_shape&quot;</span><span class="p">,</span> <span class="s2">&quot;sharding&quot;</span><span class="p">,</span> <span class="s2">&quot;_dll&quot;</span><span class="p">]</span>

<div class="viewcode-block" id="ShapeDtypeStruct.__init__">
<a class="viewcode-back" href="../../../_autosummary/jax.ShapeDtypeStruct.html#jax.ShapeDtypeStruct.__init__">[docs]</a>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">named_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sharding</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;ShapeDtypeStruct: dtype must be specified.&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span> <span class="k">if</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">extended</span><span class="p">)</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">sharding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sharding</span><span class="p">,</span> <span class="p">(</span><span class="n">Sharding</span><span class="p">,</span> <span class="n">Layout</span><span class="p">)):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;sharding should be an instance of `jax.sharding.Sharding` or&quot;</span>
          <span class="sa">f</span><span class="s2">&quot; `jax.experimental.layout.Layout`. Got </span><span class="si">{</span><span class="n">sharding</span><span class="si">}</span><span class="s2"> of type&quot;</span>
          <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">sharding</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">sharding</span><span class="p">,</span> <span class="n">Layout</span><span class="p">)</span> <span class="ow">and</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">sharding</span><span class="o">.</span><span class="n">device_local_layout</span><span class="p">,</span> <span class="n">AutoLayout</span><span class="p">)):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
          <span class="s2">&quot;`DeviceLocalLayout.AUTO` cannot be used in place of a device-local&quot;</span>
          <span class="sa">f</span><span class="s2">&quot; layout in a `ShapeDtypeStruct`. Got </span><span class="si">{</span><span class="n">sharding</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">sharding</span> <span class="o">=</span> <span class="n">sharding</span><span class="o">.</span><span class="n">sharding</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sharding</span><span class="p">,</span> <span class="n">Layout</span><span class="p">)</span> <span class="k">else</span> <span class="n">sharding</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_dll</span> <span class="o">=</span> <span class="n">sharding</span><span class="o">.</span><span class="n">device_local_layout</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sharding</span><span class="p">,</span> <span class="n">Layout</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">named_shape</span> <span class="o">=</span> <span class="p">{}</span> <span class="k">if</span> <span class="n">named_shape</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">dict</span><span class="p">(</span><span class="n">named_shape</span><span class="p">)</span></div>


  <span class="n">size</span> <span class="o">=</span> <span class="nb">property</span><span class="p">(</span><span class="k">lambda</span> <span class="bp">self</span><span class="p">:</span> <span class="n">math</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
  <span class="n">ndim</span> <span class="o">=</span> <span class="nb">property</span><span class="p">(</span><span class="k">lambda</span> <span class="bp">self</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">layout</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">Layout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_dll</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sharding</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">except</span> <span class="ne">IndexError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;len() of unsized object&quot;</span><span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>  <span class="c1"># same as numpy error</span>

  <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">ns</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;, named_shape=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">named_shape</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_shape</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
    <span class="n">sh</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;, sharding=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">sharding</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sharding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
    <span class="n">l</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;, layout=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">layout</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dll</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(shape=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;dtype=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="si">}{</span><span class="n">ns</span><span class="si">}{</span><span class="n">sh</span><span class="si">}{</span><span class="n">l</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

  <span class="fm">__str__</span> <span class="o">=</span> <span class="fm">__repr__</span>

  <span class="k">def</span> <span class="fm">__eq__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">ShapeDtypeStruct</span><span class="p">):</span>
      <span class="k">return</span> <span class="kc">False</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="p">((</span><span class="n">other</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">other</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">other</span><span class="o">.</span><span class="n">named_shape</span><span class="p">,</span> <span class="n">other</span><span class="o">.</span><span class="n">sharding</span><span class="p">,</span> <span class="n">other</span><span class="o">.</span><span class="n">layout</span><span class="p">)</span> <span class="o">==</span>
              <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sharding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">layout</span><span class="p">))</span>

  <span class="k">def</span> <span class="fm">__hash__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># TODO(frostig): avoid the conversion from dict by addressing</span>
    <span class="c1"># https://github.com/google/jax/issues/8182</span>
    <span class="n">named</span> <span class="o">=</span> <span class="nb">frozenset</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">named_shape</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
    <span class="k">return</span> <span class="nb">hash</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">named</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sharding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">layout</span><span class="p">))</span></div>



<span class="n">core</span><span class="o">.</span><span class="n">pytype_aval_mappings</span><span class="p">[</span><span class="n">ShapeDtypeStruct</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">ShapedArray</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">canonicalize_dtype</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">allow_extended_dtype</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                          <span class="n">weak_type</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">named_shape</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">named_shape</span><span class="p">))</span>


<div class="viewcode-block" id="eval_shape">
<a class="viewcode-back" href="../../../_autosummary/jax.eval_shape.html#jax.eval_shape">[docs]</a>
<span class="nd">@api_boundary</span>
<span class="k">def</span> <span class="nf">eval_shape</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Compute the shape/dtype of ``fun`` without any FLOPs.</span>

<span class="sd">  This utility function is useful for performing shape inference. Its</span>
<span class="sd">  input/output behavior is defined by::</span>

<span class="sd">    def eval_shape(fun, *args, **kwargs):</span>
<span class="sd">      out = fun(*args, **kwargs)</span>
<span class="sd">      shape_dtype_struct = lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype)</span>
<span class="sd">      return jax.tree_util.tree_map(shape_dtype_struct, out)</span>

<span class="sd">  But instead of applying ``fun`` directly, which might be expensive, it uses</span>
<span class="sd">  JAX&#39;s abstract interpretation machinery to evaluate the shapes without doing</span>
<span class="sd">  any FLOPs.</span>

<span class="sd">  Using :py:func:`eval_shape` can also catch shape errors, and will raise same</span>
<span class="sd">  shape errors as evaluating ``fun(*args, **kwargs)``.</span>

<span class="sd">  Args:</span>
<span class="sd">    fun: The function whose output shape should be evaluated.</span>
<span class="sd">    *args: a positional argument tuple of arrays, scalars, or (nested) standard</span>
<span class="sd">      Python containers (tuples, lists, dicts, namedtuples, i.e. pytrees) of</span>
<span class="sd">      those types. Since only the ``shape`` and ``dtype`` attributes are</span>
<span class="sd">      accessed, one can use :class:`jax.ShapeDtypeStruct` or another container</span>
<span class="sd">      that duck-types as ndarrays (note however that duck-typed objects cannot</span>
<span class="sd">      be namedtuples because those are treated as standard Python containers).</span>
<span class="sd">    **kwargs: a keyword argument dict of arrays, scalars, or (nested) standard</span>
<span class="sd">      Python containers (pytrees) of those types. As in ``args``, array values</span>
<span class="sd">      need only be duck-typed to have ``shape`` and ``dtype`` attributes.</span>

<span class="sd">  Returns:</span>
<span class="sd">    out: a nested PyTree containing :class:`jax.ShapeDtypeStruct` objects as leaves.</span>

<span class="sd">  For example:</span>

<span class="sd">  &gt;&gt;&gt; import jax</span>
<span class="sd">  &gt;&gt;&gt; import jax.numpy as jnp</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; f = lambda A, x: jnp.tanh(jnp.dot(A, x))</span>
<span class="sd">  &gt;&gt;&gt; A = jax.ShapeDtypeStruct((2000, 3000), jnp.float32)</span>
<span class="sd">  &gt;&gt;&gt; x = jax.ShapeDtypeStruct((3000, 1000), jnp.float32)</span>
<span class="sd">  &gt;&gt;&gt; out = jax.eval_shape(f, A, x)  # no FLOPs performed</span>
<span class="sd">  &gt;&gt;&gt; print(out.shape)</span>
<span class="sd">  (2000, 1000)</span>
<span class="sd">  &gt;&gt;&gt; print(out.dtype)</span>
<span class="sd">  float32</span>

<span class="sd">  All arguments passed via :func:`eval_shape` will be treated as dynamic;</span>
<span class="sd">  static arguments can be included via closure, for example using :func:`functools.partial`:</span>

<span class="sd">  &gt;&gt;&gt; import jax</span>
<span class="sd">  &gt;&gt;&gt; from jax import lax</span>
<span class="sd">  &gt;&gt;&gt; from functools import partial</span>
<span class="sd">  &gt;&gt;&gt; import jax.numpy as jnp</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; x = jax.ShapeDtypeStruct((1, 1, 28, 28), jnp.float32)</span>
<span class="sd">  &gt;&gt;&gt; kernel = jax.ShapeDtypeStruct((32, 1, 3, 3), jnp.float32)</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; conv_same = partial(lax.conv_general_dilated, window_strides=(1, 1), padding=&quot;SAME&quot;)</span>
<span class="sd">  &gt;&gt;&gt; out = jax.eval_shape(conv_same, x, kernel)</span>
<span class="sd">  &gt;&gt;&gt; print(out.shape)</span>
<span class="sd">  (1, 32, 28, 28)</span>
<span class="sd">  &gt;&gt;&gt; print(out.dtype)</span>
<span class="sd">  float32</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">try</span><span class="p">:</span> <span class="nb">hash</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span> <span class="n">fun</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">jit</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span><span class="o">.</span><span class="n">eval_shape</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>



<div class="viewcode-block" id="named_call">
<a class="viewcode-back" href="../../../_autosummary/jax.named_call.html#jax.named_call">[docs]</a>
<span class="k">def</span> <span class="nf">named_call</span><span class="p">(</span>
    <span class="n">fun</span><span class="p">:</span> <span class="n">F</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">F</span><span class="p">:</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Adds a user specified name to a function when staging out JAX computations.</span>

<span class="sd">  When staging out computations for just-in-time compilation to XLA (or other</span>
<span class="sd">  backends such as TensorFlow) JAX runs your Python program but by default does</span>
<span class="sd">  not preserve any of the function names or other metadata associated with it.</span>
<span class="sd">  This can make debugging the staged out (and/or compiled) representation of</span>
<span class="sd">  your program complicated because there is limited context information for each</span>
<span class="sd">  operation being executed.</span>

<span class="sd">  `named_call` tells JAX to stage the given function out as a subcomputation</span>
<span class="sd">  with a specific name. When the staged out program is compiled with XLA these</span>
<span class="sd">  named subcomputations are preserved and show up in debugging utilities like</span>
<span class="sd">  the TensorFlow Profiler in TensorBoard. Names are also preserved when staging</span>
<span class="sd">  out JAX programs to TensorFlow using :func:`experimental.jax2tf.convert`.</span>

<span class="sd">  Args:</span>
<span class="sd">    fun: Function to be wrapped. This can be any Callable.</span>
<span class="sd">    name: Optional. The prefix to use to name all sub computations created</span>
<span class="sd">      within the name scope. Use the fun.__name__ if not specified.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A version of `fun` that is wrapped in a name_scope.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">fun</span><span class="o">.</span><span class="vm">__name__</span>

  <span class="k">return</span> <span class="n">source_info_util</span><span class="o">.</span><span class="n">extend_name_stack</span><span class="p">(</span><span class="n">name</span><span class="p">)(</span><span class="n">fun</span><span class="p">)</span></div>



<div class="viewcode-block" id="named_scope">
<a class="viewcode-back" href="../../../_autosummary/jax.named_scope.html#jax.named_scope">[docs]</a>
<span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">named_scope</span><span class="p">(</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Generator</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;A context manager that adds a user specified name to the JAX name stack.</span>

<span class="sd">  When staging out computations for just-in-time compilation to XLA (or other</span>
<span class="sd">  backends such as TensorFlow) JAX does not, by default, preserve the names</span>
<span class="sd">  (or other source metadata) of Python functions it encounters.</span>
<span class="sd">  This can make debugging the staged out (and/or compiled) representation of</span>
<span class="sd">  your program complicated because there is limited context information for each</span>
<span class="sd">  operation being executed.</span>

<span class="sd">  ``named_scope`` tells JAX to stage the given function with additional</span>
<span class="sd">  annotations on the underlying operations. JAX internally keeps track of these</span>
<span class="sd">  annotations in a name stack. When the staged out program is compiled with XLA</span>
<span class="sd">  these annotations are preserved and show up in debugging utilities like the</span>
<span class="sd">  TensorFlow Profiler in TensorBoard. Names are also preserved when staging out</span>
<span class="sd">  JAX programs to TensorFlow using :func:`experimental.jax2tf.convert`.</span>


<span class="sd">  Args:</span>
<span class="sd">    name: The prefix to use to name all operations created within the name</span>
<span class="sd">      scope.</span>
<span class="sd">  Yields:</span>
<span class="sd">    Yields ``None``, but enters a context in which `name` will be appended to</span>
<span class="sd">    the active name stack.</span>

<span class="sd">  Examples:</span>
<span class="sd">    ``named_scope`` can be used as a context manager inside compiled functions:</span>

<span class="sd">    &gt;&gt;&gt; import jax</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; @jax.jit</span>
<span class="sd">    ... def layer(w, x):</span>
<span class="sd">    ...   with jax.named_scope(&quot;dot_product&quot;):</span>
<span class="sd">    ...     logits = w.dot(x)</span>
<span class="sd">    ...   with jax.named_scope(&quot;activation&quot;):</span>
<span class="sd">    ...     return jax.nn.relu(logits)</span>

<span class="sd">    It can also be used as a decorator:</span>

<span class="sd">    &gt;&gt;&gt; @jax.jit</span>
<span class="sd">    ... @jax.named_scope(&quot;layer&quot;)</span>
<span class="sd">    ... def layer(w, x):</span>
<span class="sd">    ...   logits = w.dot(x)</span>
<span class="sd">    ...   return jax.nn.relu(logits)</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;named_scope name argument must be a string.&quot;</span><span class="p">)</span>
  <span class="k">with</span> <span class="n">source_info_util</span><span class="o">.</span><span class="n">extend_name_stack</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="k">yield</span></div>


<span class="k">def</span> <span class="nf">effects_barrier</span><span class="p">():</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Waits until existing functions have completed any side-effects.&quot;&quot;&quot;</span>
  <span class="n">dispatch</span><span class="o">.</span><span class="n">runtime_tokens</span><span class="o">.</span><span class="n">block_until_ready</span><span class="p">()</span>

<div class="viewcode-block" id="block_until_ready">
<a class="viewcode-back" href="../../../_autosummary/jax.block_until_ready.html#jax.block_until_ready">[docs]</a>
<span class="k">def</span> <span class="nf">block_until_ready</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Tries to call a ``block_until_ready`` method on pytree leaves.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: a pytree, usually with at least some JAX array instances at its leaves.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A pytree with the same structure and values of the input, where the values</span>
<span class="sd">    of all JAX array leaves are ready.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">def</span> <span class="nf">try_to_block</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">block_until_ready</span><span class="p">()</span>
    <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">x</span>

  <span class="k">if</span> <span class="n">xla_extension_version</span> <span class="o">&lt;</span> <span class="mi">246</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tree_map</span><span class="p">(</span><span class="n">try_to_block</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

  <span class="n">arrays</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">leaf</span> <span class="ow">in</span> <span class="n">tree_leaves</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">leaf</span><span class="p">,</span> <span class="n">array</span><span class="o">.</span><span class="n">ArrayImpl</span><span class="p">):</span>
      <span class="n">arrays</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">leaf</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">try_to_block</span><span class="p">(</span><span class="n">leaf</span><span class="p">)</span>

  <span class="k">if</span> <span class="ow">not</span> <span class="n">arrays</span><span class="p">:</span>
    <span class="c1"># `arrays` will be empty if tree_leaves(x) is empty or all leaves are not</span>
    <span class="c1"># jax.Array.</span>
    <span class="k">pass</span>
  <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">arrays</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="c1"># Fast path for single array.</span>
    <span class="n">try_to_block</span><span class="p">(</span><span class="n">arrays</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="c1"># Optimized for multiple arrays.</span>
    <span class="n">xc</span><span class="o">.</span><span class="n">batched_block_until_ready</span><span class="p">(</span><span class="n">arrays</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">x</span></div>



<span class="k">def</span> <span class="nf">clear_backends</span><span class="p">():</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Clear all backend clients so that new backend clients can be created later.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">xb</span><span class="o">.</span><span class="n">_clear_backends</span><span class="p">()</span>
  <span class="n">xb</span><span class="o">.</span><span class="n">local_devices</span><span class="o">.</span><span class="n">cache_clear</span><span class="p">()</span>
  <span class="n">xb</span><span class="o">.</span><span class="n">process_count</span><span class="o">.</span><span class="n">cache_clear</span><span class="p">()</span>
  <span class="n">dispatch</span><span class="o">.</span><span class="n">xla_primitive_callable</span><span class="o">.</span><span class="n">cache_clear</span><span class="p">()</span>
  <span class="n">pjit</span><span class="o">.</span><span class="n">_pjit_lower_cached</span><span class="o">.</span><span class="n">cache_clear</span><span class="p">()</span>
  <span class="n">pjit</span><span class="o">.</span><span class="n">_create_pjit_jaxpr</span><span class="o">.</span><span class="n">cache_clear</span><span class="p">()</span>  <span class="c1"># pytype: disable=attribute-error</span>
  <span class="n">pjit</span><span class="o">.</span><span class="n">_cpp_pjit_cache</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
  <span class="n">xc</span><span class="o">.</span><span class="n">_xla</span><span class="o">.</span><span class="n">PjitFunctionCache</span><span class="o">.</span><span class="n">clear_all</span><span class="p">()</span>

<div class="viewcode-block" id="live_arrays">
<a class="viewcode-back" href="../../../_autosummary/jax.live_arrays.html#jax.live_arrays">[docs]</a>
<span class="k">def</span> <span class="nf">live_arrays</span><span class="p">(</span><span class="n">platform</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Return all live arrays in the backend for `platform`.</span>

<span class="sd">  If platform is None, it is the default backend.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">xb</span><span class="o">.</span><span class="n">get_backend</span><span class="p">(</span><span class="n">platform</span><span class="p">)</span><span class="o">.</span><span class="n">live_arrays</span><span class="p">()</span></div>


<div class="viewcode-block" id="clear_caches">
<a class="viewcode-back" href="../../../_autosummary/jax.clear_caches.html#jax.clear_caches">[docs]</a>
<span class="k">def</span> <span class="nf">clear_caches</span><span class="p">():</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Clear all compilation and staging caches.&quot;&quot;&quot;</span>
  <span class="c1"># Clear all lu.cache and util.weakref_lru_cache instances (used for staging</span>
  <span class="c1"># and Python-dispatch compiled executable caches).</span>
  <span class="n">lu</span><span class="o">.</span><span class="n">clear_all_caches</span><span class="p">()</span>
  <span class="n">util</span><span class="o">.</span><span class="n">clear_all_weakref_lru_caches</span><span class="p">()</span>

  <span class="c1"># Clear all C++ compiled executable caches for pjit</span>
  <span class="n">pjit</span><span class="o">.</span><span class="n">_cpp_pjit_cache</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
  <span class="n">xc</span><span class="o">.</span><span class="n">_xla</span><span class="o">.</span><span class="n">PjitFunctionCache</span><span class="o">.</span><span class="n">clear_all</span><span class="p">()</span>

  <span class="c1"># Clear all C++ compiled executable caches for pmap</span>
  <span class="k">for</span> <span class="n">fun</span> <span class="ow">in</span> <span class="n">_pmap_cache_clears</span><span class="p">:</span>
    <span class="n">fun</span><span class="o">.</span><span class="n">_cache_clear</span><span class="p">()</span>

  <span class="c1"># Clear particular util.cache instances.</span>
  <span class="n">dispatch</span><span class="o">.</span><span class="n">xla_primitive_callable</span><span class="o">.</span><span class="n">cache_clear</span><span class="p">()</span></div>

</pre></div>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The JAX authors
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      Â© Copyright 2024, The JAX Authors. NumPy and SciPy documentation are copyright the respective authors..
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>